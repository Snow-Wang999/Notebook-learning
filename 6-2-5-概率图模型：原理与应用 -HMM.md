# 6-2-1-概率图模型：原理与应用-HMM

## 前言

概率图形模型（PGM）及其在不确定性下智能推理的应用，于20世纪80年代出现在统计和人工智能推理界。人工智能中的**不确定性（UAI）会议**成为这个蓬勃发展的研究领域的首要论坛。在圣何塞的UAI-92，我第一次见到了我们两个研究生恩里克·苏卡尔（Enrique Sucar），他在那里介绍了他在高级视觉推理的关系和时间模型方面的工作。恩里克在过去25年中对我们领域做出了令人印象深刻的研究贡献，**从客观概率的基础工作，到开发高级形式的PGMS（如时间和事件贝叶斯网络），再到PGM的学习**，例如他最近在多维分类贝叶斯链分类器方面的工作。

概率图形模型现在被广泛接受为一种**强大而成熟的不确定性推理技术**。与早期专家系统中采用的一些特殊方法不同，PGM**基于图论和概率论**的强大数学基础。它们可用于广泛的推理任务，包括**预测、监测、诊断、风险评估和决策**。在开源软件和商业软件中有许多用于推理和学习的有效算法。此外，通过成功应用于大量现实问题领域，已经证明了它们的威力和有效性。恩里克·苏卡尔（Enrique Sucar）是PGM作为实用和有用技术的主要贡献者，他的工作涉及广泛的应用领域。这些领域包括医学、康复和护理、机器人和视觉、教育、可靠性分析以及从石油生产到发电厂的工业应用。

第一批借鉴早期贝叶斯网络研究并将其以书的形式写成引人入胜的故事的作者是Judea Pearl在《智能系统中的概率推理》和Rich Neapolitan在《专家系统中的可能性推理》中。恩里克·苏卡尔（Enrique Sucar）的这本专著是继《珍珠》（Pearl）和《那不勒斯》（Neapolitan）之后的一部及时的文献集，它涵盖了比该领域其他近期文本更广泛的PGM：各种vii分类器、隐马尔可夫模型、马尔可夫随机场、贝叶斯网络及其动态、时间和因果变量、关系PGM、决策图和马尔可夫决策过程。它以清晰易懂的方式介绍了这些PGM，以及相关的推理（或推理）和学习方法，使其适合于高级学生以及对使用概率模型感兴趣的其他学科的研究人员或从业者。Enrique充分利用了他在PGM建模方面的丰富实践经验，在从生物信息学到空气污染到物体识别的各种现实应用中展示了PGM的应用，从而大大丰富了文本。我衷心祝贺恩里克的这本书，并将其推荐给潜在的读者。

## 概述

概率图形模型已经成为多个领域中使用的一组强大的技术。本书从工程角度提供了概率图形模型（PGM）的一般介绍。它涵盖了PGM的主要类别的基本原理：**贝叶斯分类器、隐马尔可夫模型、贝叶斯网络、动态和时间贝叶斯网络、马尔可夫随机场、影响图和马尔可夫决策过程**；包括所有技术的表示、推理和学习原则。书中还介绍了每种模型的实际应用。

一些关键特征是：

- PGM的主要类别**在统一框架下**的一本专著中介绍。
- 本书涵盖了所有技术的基本方面：**表示、推理和学习**
- 它说明了不同技术**在实际问题中的应用**，这是学生和从业者的一个重要特点
- 它包括该领域的一些**最新发展**，如多维度贝叶斯分类器、关系图形模型和因果模型
- 每一章都有一套**练习**，包括对研究和编程项目的建议。

- 这不仅需要了解不同的模型和技术，还需要一些实践经验和领域知识。为了帮助不同领域的专业人员深入了解PGM在解决实际问题中的应用，本书包括许多不同类型模型在**广泛领域**中的应用示例，包括：
  - 计算机视觉、生物医学应用、工业应用。信息检索。智能辅导系统。生物信息学。环境应用。机器人。人机交互。信息验证。关爱。

---

### 第二节 概率论

#### 2.1 简介

概率论起源于机会游戏，有着悠久而有趣的历史；它已经发展成为量化不确定性的数学语言。

我们将考虑逻辑或规范方法，并在给定可用证据的情况下，根据某个命题的可信度来定义概率[2]。根据考克斯的工作，Jaynes确立了一些基本的愿望，即这种合理性必须遵循[2]：

- 用实数表示。（Representation by real numbers）
- 与常识的定性对应。（Qualitative correspondence with common sense）
- 一致性（Consistency）

基于这些直觉原理，我们可以导出概率的三个公理：

1. $P(A)$是[0，1]中的连续单调函数。
2. $P(A,B\ |\ C)=P(A\ |\ C)P(B\ |\ A,C)$（乘积规则）。
3. $P(A\ |\ B)+P(¬A\ |\ B)=1$（求和规则）。

其中A、B、C是命题（二进制变量），$P(A)$是命题A的概率。$P（A\ |\ C）$是给定C已知的A的概率，称为条件概率。$P（A,B\ |\ C）$是给定C（逻辑连接）时A与B的概率，$P(¬A \ |\ C)$则是给定C时NOT A（逻辑否定）的概率。这些规则相当于最常用的Kolmogorov公理。从这些公理中，可以导出所有常规的概率论。

#### 2.2 基本规则

两个命题的析取（逻辑和）的概率由和规则给出：$P(A+B\ |\ C)=P(A\ |\ C)+P(B\ |\ C)−P(A，B\ |\ C)$；如果命题A和B在给定C的情况下互斥，我们可以将其简化为：$P(A+B\ |\ C)=P(A\ |\ C)+P(B\ |\  C)$。这可以推广到N个互斥命题：
$$
P(A_1+A_2+...+A_N\ |\ C)=P(A_1\ |\ C)+P(A_2\ |\ C)+...+P(A_N\ |\ C) \tag{2.1}
$$

$$
P(D,H\ |\ B)=P(D\ |\ H,B)P(H\ |\ B)=P(H\ |\ D,B)P(D\ |\ B) \tag{2.2}
$$

$$
P(H\ |\ D,B) = \frac{P(H\ |\ B)P(D\ |\ H,B)}{P(D\ |\ B)} \tag{2.3}
$$


$$
P(A_1,A_2,...,A_N\ |\ B)=P(A_1\ |\ B)P(A_2\ |\ B)...P(A_N\ |\ B) \tag{2.4}
$$

$$
P(A_1,A_2,...,A_N\ |\ B)=P(A_1\ |\ A_2,A_3,...,A_N,B)P(A_2\ |\ A_3,A_4,...,A_N,B)...P(A_N\ |\ B) \tag{2.5}
$$

$$
P(A)=\sum_i{P(A\ |\ B_i)P(B_i)} \tag{2.6}
$$

$$
P(B\ |\ A)=\frac{{P(B)P(A\ |\ B)}}{\sum_i{P(A\ |\ B_i)P(B_i)}} \tag{2.7}
$$



### 第三节 图论

![image-20221229202730094](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202730094.png)

#### 3.1 定义

图形提供了一种表示一组对象之间的二进制关系的紧凑方式。例如，考虑某个地区的一组城市，以及连接这些城市的道路。然后，该地区的地图本质上是一个图形，其中的对象是城市，城市对之间的直接道路是关系。图形通常用图形表示。对象表示为圆或椭圆，关系表示为线或箭头；见图3.1。有两种基本类型的图：无向图和有向图。接下来，我们将有向图和无向图的定义形式化。

给定$V$，一个非空集合，$V$上的二元关系$E⊆V×V$是一组有序对，$(V_j，V_k)$，使得$V_j∈V$和$V$中的$V_k$。

**有向图或有向图是一个有序对，$G=(V,E)$：**

- **$V$是一组顶点或节点，**
- **E是一组弧，表示V上的二元关系**；

见图3.1b。*<u>有向图(directed graph)表示对象之间的反对称关系，例如“父(parent)”关系。</u>*

无向图是一个有序对，$G=(V，E)$，其中V是一组顶点或节点，E是表示对称二元关系的一组边：$(V_j，V_k)∈E→ (V_k，V_j)∈E$；见图3.1a。*<u>无向图表示对象之间的对称关系，例如“兄弟”关系。</u>*

如果节点$j$和$k$之间存在边$E_i(V_j，V_k)$，则$V_j$与$V_k$相邻。<u>*节点的阶数是该节点中关联的边数。*</u>在图3.1a中，上部节点的阶数为2，两个下部节点的阶为1。

与同一对顶点关联的两条边称为平行边；入射在单个顶点上的边是循环；并且不是任何边的端点的顶点是其度为0的孤立顶点。如图3.2所示。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202854426.png" alt="image-20221229202854426" style="zoom:60%;" />
    <br> <!--换行-->
    图3.1图：
    <br>a.无向图(undirected)，
    <br> b.有向图(directed,边有箭头) <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202911657.png" alt="image-20221229202911657" style="zoom:60%;" />
    <br> <!--换行-->
    图3.2 示例：
    <br> a.平行边、
    <br> b.循环、
    <br> c.孤立顶点 <!--标题-->
    </center>
</div>


在有向图中，指向节点的弧的数量是节点的**入度数**，而指向远离节点的边的数量是其**出度数**。在图3.1b中，两个上部节点的入度为0，出度为2；而两个较低节点具有2的入度和0的出度。

给定图 $G=(V，E)$，$G$ 的**子图** $G'=(V'，E')$是这样一个图，即 $V'⊆V$ 和 $E'⊆E$ ，其中$E'$中的每条边都入射到$V'$中的顶点上。例如，如果我们去掉图3.1b中的边的方向（使其成为无向图），那么图3.1a中的图就是图3.1b的子图。

#### 3.2 图的类型

除了有向和无向这两种基本图之外，还有其他类型的图，例如：

- **链图（Chain graph）**：具有有向和无向边的混合图。如下图3.3(a)
- **简单图形（Simple graph）**：不包括循环和平行弧的图形。如下图3.3(b)
- **多重图（Multigraph）**：具有多个组成部分（子图）的图，每个组成部分与其他组成部分没有边，即它们是断开的。如下图3.3(c)
- **完整图（Complete graph）**：每对顶点之间有一条边的图。如下图3.3(d)
- **二分图（Bipartite graph）**：将顶点分为两个子集G1、G2的图，使得所有边将G1中的顶点与G2中的顶点连接起来；也就是说，每个子集中的节点之间没有边。如下图3.3(e)
- **加权图（Weighted graph）**：具有与其边和/或顶点相关联的权重的图。如下图3.3(f)

这些类型的图表示例如图3.3所示。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231114709312.png" alt="image-20221231114709312" style="zoom:100%;" />
    <br> <!--换行-->
    图片3.3 图的类型：
    <br>a.链图(chain graph)、
    <br>b.简单图(simple graph)、
    <br>c.多重图(multigraph)、
    <br>d.完全图(complete graph)、
    <br>e.二分图(bipartite graph)、
    <br>f.加权图(weighted graph) <!--标题-->
    </center>
</div>

#### 3.3轨迹和电路(Trajectories and Circuits)

**轨迹（trajectory）**是边缘序列$E_1、E_2，E_n$使得每条边的最终顶点与序列中下一条边的初始顶点重合（除了最终顶点）；即，对于 $i=1$ 至 $i=n−1$，$E_i(V_j，V_k)$，$E_{i+1}(V_k，V_l)$。一个简单的轨迹不包括两次或多次相同的边；元素轨迹不会多次入射到同一顶点上。不同轨迹的示例如图3.4所示。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231122131928.png" alt="image-20221231122131928" style="zoom:60%;" />
    <br> <!--换行-->
    图3.4 轨迹示例：
    <br>a.是简单但不是基本的轨迹，
    <br>b.是简单而基本的轨迹 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231122153441.png" alt="image-20221231122153441" style="zoom:50%;" />
    <br> <!--换行-->
    图3.5 简单但不基本的回路示例 <!--标题-->
    </center>
</div>


如果图G中的每对不同顶点之间存在轨迹，则图G是连通的。如果图G不连通，则连通的每个部分称为G的一个分量。

**回路（circuit）**是一条轨迹，使得最终顶点与初始顶点重合，即它是一条“闭合轨迹”。以类似于轨迹的方式，我们可以定义简单的基本电路。图3.5显示了电路的示例。

PGM的一个重要类型的图是有向非循环图（DAG-Directed Acyclic Graph）。DAG是一个没有有向电路的有向图（有向电路是序列中所有边都遵循箭头方向的电路）。例如，图3.1b是DAG，图3.3f不是DAG。

图论中的一些经典问题包括轨迹和电路，例如：

- **欧拉轨迹【Euler trajectory】**：找到一条轨迹，该轨迹只包含一次图形中的所有边。
- **欧拉回路【Euler circuit)**：找到一个仅包含一次图中所有边的回路。
- **哈密顿轨迹【Hamiltonian trajectory】**：找到一个仅包含一次图中所有顶点的轨迹。
- **哈密顿回路【Hamiltonian circuit】**：找到一个只包含一次图中所有顶点的回路。
- **旅行推销员问题【Traveling salesman problem】**：在最小成本的加权图中找到哈密顿回路。【1在这种情况下，节点代表具有相关距离或时间的城市和边缘道路，因此解决方案将为旅行推销员提供覆盖所有城市的“最佳”（最小距离或时间）路线。】

这些问题的解决方案超出了本书的范围，感兴趣的读者可以参考[2]。

#### 3.4 图同构（Graph Isomorphism）

如果两个图的顶点和边之间存在一对一的对应关系，则它们是**同构**的，从而保持了关联。给定两个图G1和G2，有三种基本类型的同构：

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231123052594.png" alt="image-20221231123052594" style="zoom:50%;" />
    <br> <!--换行-->
    图3.6 这两个图是同构的 <!--标题-->
    </center>
</div>


1. **图同构（Graph isomorphism）**。图G1和G2是同构的。

2. **子图同构（Subgraph isomorphism）**。图G1同构于G2的子图（反之亦然）。

3. **双子图同构（Double subgraph isomorphism）**。G1的子图同构于G2的子图

图3.6显示了两个同构图的示例。

确定两个图是否同构（类型1）是一个**NP问题**；而子图和双子图同构问题（类型2和3）是**NP完全**的。参见[1]。

#### 3.5 trees

树是一种在计算机科学中非常重要的图，特别是对于PGM。我们将讨论两种类型的树：**无向树和有向树**。**无向树**是一个没有简单电路的连通图；图3.7描述了一个无向树的示例。在无向树中这里有两类顶点或节点：（i）叶或终端节点，具有一级；（ii）内部节点，度数大于1。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231123727684.png" alt="image-20221231123727684" style="zoom:60%;" />
    <br> <!--换行-->
    图3.7 无向树。此树有五个节点、三个叶节点和两个内部节点 <!--标题-->
    </center>
</div>


**树的一些基本属性是：**

- 每对顶点之间有一个简单的轨迹。
- 顶点数$|V|$等于边数$|E|$加一：$|V|=|E|+1$。
- 具有两个或多个顶点的树至少有两个叶节点。

**有向树**是一个连通的有向图，使得每对节点之间只有一个有向轨迹（也称为单连通有向图）。

有两种类型的有向树：

- （i）根树（或简单的树），

- （ii）多根树。

**根树**有一个度数为零的节点（根节点），其余的节点度数为一。一个**多根树**可能有一个以上的零度节点（根），而某些节点（零或更多）的零度大于一（称为多父节点）。如果我们去掉多根树中边的方向，它就会转化为无向树。我们可以把树看作是多根树的特例。图3.8显示了根树和多根树的示例。

<u>**有向树**</u>的一些相关术语如下。

- **根（Root）**：度数等于零的节点。
- **叶（Leaf）**：外度数等于零的节点。
- **内部节点（Internal node）**：外度数大于零的节点。
- **父级/子级（Parent/Child）**：如果存在从a到B的有向弧，则a是B的父级，B是a的子级。
- **兄弟级（Brothers）**：如果两个或多个节点具有相同的父级则为兄弟级。
- **上升/下降（Ascendants /Descendants）**：如果有一条从a到B的有向轨迹，a是B的上升，B是a的后代。
- **根为A的子树（Subtree with root A）**：以A为根的子树。
- **A的子树（Subtree of A）**：以A的子树为根的子树。
- **K元树（K-ary Tree）**：每个内部节点最多有K个子节点的树。如果每个内部节点都有K个子节点，则它是一个规则树。
- **二叉树（Binary Tree）**：每个内部节点最多有两个子节点的树。

```html
<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="图片路径放这里" 
         alt="无法显示图片时显示的文字" 
         style="zoom:这里写图片的缩放百分比"/>
    <br> <!--换行-->
    这里是图片的标题 <!--标题-->
    </center>
</div>
```

<div>
    <center>
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231124315351.png" 
         alt="image-20221231124315351"
         style="zoom:60%"
         />
    <br>
    图3.8 a.有根树（A rooted tree.）。
     <br>   b.多根树（A polytree）
    </center>
</div>

<div>
    <center>
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231124738338.png" alt="image-20221231124738338" style="zoom:60%;" />
    <br>
    图3.9说明术语的有向树示例
    </center>
</div>


例如，在图3.9中的树中；

- （i） A是根节点，

- （ii）C、D、F、G是叶节点，

- （iii）B、E是内部节点，

- （iv）A是B的父节点，B是A的子节点，

- （v）B和C是兄弟节点，

- （vi）A是F的祖先，F是A的后代，

- （vii）子树B、D、E、F、G是根B的子树，

- （viii）子树E、F和G是B的子树。图3.9中的树是非正则（nonregular）二叉树。

#### 3.6 Cliques（团）

完整图是一个图，$G_c$，其中每对节点是相邻的；也就是说，每对节点之间都有一条边。图3.3d是完整图表的示例。一个完备集，$W_c$是G的一个子集，它诱导了$G$的一个完备子图。它是$G$的顶点的子集，因此该子图中的每对节点都是相邻的。例如，在图3.3d中，图中三个节点的每个子集都是一个完整的集合。

**团（Cliques）**C是图G的子集，使得它是一个最大的完备集；也就是说，G中没有其他包含C的完备集。图3.3d中三个节点的子集不是集团，因为它们不是最大的；它们包含在完整的图表中。

图3.10中的图表有五个分支，一个分支有四个节点，一个有三个节点，三个分支有两个节点。注意，图中的每个节点都是至少一个集团的一部分；因此，图的一组群总是覆盖V。

以下各节介绍了图论的一些更高级的概念，因为概率图形模型的一些推理算法使用了这些概念。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101181620922.png" alt="image-20230101181620922" style="zoom:80%;" />
    <br> <!--换行-->
    图3.10 团：图中突出显示了五个团 <!--标题-->
    </center>
</div>


#### 3.7完美顺序

图中节点的排序包括为每个顶点分配一个整数。给定具有$n$个顶点的图$G=(V，E)$，则$\alpha=[V_1,V_2,...,V_n]$ 是图的排序；根据此排序，如果$i＜j$，则$V_i$在$V_j$之前。

如果根据此排序在$V_i$之前的每个顶点$V_i$的所有相邻顶点完全连接，则图$G＝(V，E)$的排序$α$是完美排序。也就是说，对于每个$i$，$Adj(V_i)\bigcap\{V_1，V_2，…，V_{i−1}\}$是$G$的完整子图。图3.11描述了一个完美排序的示例。

考虑一组派系$C_1,C_2,...,C_p$。以与节点排序类似的方式，我们可以定义群的排序，$\beta=[C_1,C_2,...,C_p]$。如果每个集团$C_i$的所有公共节点都包含在一个集团C j中，则集团的一个序$β$具有运行交集性质；$C_j$是$C_i$的父级。换句话说，对于每个团$i>1$都存在一个团$j<i$，使得$C_i\bigcap\{C_1,C_2,...,C_{i−1}\}⊆C_j$。一个团(clique)可能有不止一个家长(parent)。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101182339243.png" alt="image-20230101182339243" style="zoom:50%;" />
    <br> <!--换行-->
    图3.11 图中节点和群的排序示例。
    <br>在这种情况下，节点具有完美的排序，并且群的排序满足运行交集属性 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101182455895.png" alt="image-20230101182455895" style="zoom:50%;" />
    <br> <!--换行-->
    图3.12 未三角化的图形示例。
    <br>电路1、2、5、4、1没有和弦 <!--标题-->
    </center>
</div>


图3.11中的团 $C_1$、$C_2$ 和 $C_3$ 具有完美的顺序。在本例中，$C_1$ 是 $C_2$ 的父级， $C_1$ 和 $C_2$ 是 $C_3$ 的父级。

如果 $G$ 中长度大于3的每个简单回路都有一个和弦（chord），则图G被三角化。**弦（chord）**是连接回路中两个顶点的边，不是回路的一部分。例如，在图3.11中，顶点1、2、4、3、1形成的电路具有连接节点2和3的弦。图3.11中的图形是三角形的。图3.12描述了未三角化的图形示例。虽然从视觉上看，该图可能是三角形的，但有一个回路1、2、5、4、1没有任何和弦。

要实现顶点的完美排序，并具有满足运行交集属性的簇的排序，一个条件是对图进行三角化。

在下一节中，我们将介绍以下算法：

- （i）对图的节点进行排序，以实现完美排序；

- （ii）对团进行计数，以确保给定完美排序的运行交集属性；

- 以及（iii）如果没有，则对图进行三角化。

#### 3.8排序和三角剖分算法（Ordering and Triangulation Algorithms）

##### 3.8.1最大基数搜索（ Maximum Cardinality Search）

假设一个图是三角形的，下面的算法（称为最大基数搜索）保证了一个完美的排序。给定具有$n$个顶点的无向图 $G=(V,E)$：

![image-20230101214846043](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101214846043.png)

---

**算法3.1 最大基数搜索算法**

---

从 $V$ 中选择任意顶点并将其赋值为1。

**while** 不是G中的所有顶点都已编号。**do:**

- 从所有未标记的顶点中，选择相邻标记顶点数较高的顶点，并为其指定下一个编号。任意断开连接。

**end while**

----

给定顶点的完美顺序，可以很容易地对群进行编号，以使顺序满足运行交集属性。为此，各团按相反顺序编号。给定 $p$ 个团，具有最高数量节点的团被分配为 $p$；包括下一个最高编号节点的团被分配 $p−1$ ；这种方法可以用图3.11中的示例来说明。编号最高的节点是5，因此包含它的团是$C_3$。下一个最高节点是4，因此包含它的团是$C_2$。剩下的团是$C_1$。现在，我们将看到如何“填充”图形以使其三角化。

##### 3.8.2图形填充

图的填充包括向原始图G添加弧，以获得新的图$G_t$，从而$G_t$被三角化。给定具有n个节点的无向图$G=(V，E)$，以下算法将该图三角化：

![image-20230101215528247](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101215528247.png)

---

**算法3.2 图形填充算法**

---

使用最大基数搜索对顶点$V$排序：$V_1,V_2,V_n$。

**for** $i=n$ **to** $i=1$ **do**

- 对于节点$V_i$，选择其所有相邻节点$V_j$，使$j>i$。调用这组节点$A_i$。
- 如果$k＞i$且$V_k\notin A_i$，则添加从$V_i$到$V_k$的弧。

**end for**

---

例如，考虑图3.12中未三角化的图形。如果我们应用前面的算法，我们首先对节点进行排序，生成图中所示的标记。接下来，我们以相反的顺序处理节点，并获得每个节点的集合A：

- A5：∅

- A4：5 
- A3：4，5 
- A2：3，5。圆弧从2添加到4。
- A1：2、3、4。圆弧从1添加到5。

生成的图形有两个额外的弧2−4和1−5，我们可以验证它是三角形的。

填充算法保证生成的图形是三角形的，但一般来说，在添加最小数量的附加弧方面，它不是最优的。

#### 3.9 附加阅读

有几本图论书籍更详细地涵盖了本章中介绍的大多数概念，包括[2-4]。那不勒斯( Neapolitan)[5]，第3章，涵盖了贝叶斯网络所需的主要图论背景，包括更先进的概念。[1]中描述了一些从算法角度出发的图论技术，包括图同构。

#### 3.10 Exercises

1. 十八世纪，柯尼斯堡市（位于普鲁士，目前是俄罗斯的一部分）被七座桥连接成四部分。据说，居民们试图在整个城市找到一条线路，以便他们只穿过一次每座桥。Euler将问题转化为一个图（在本章开头进行了说明），并建立了一个连通图中恰好通过每条边一次的电路的条件：图中的所有顶点都必须具有偶数次。确定Könisberg的居民是否能够找到欧拉回路。

2. 证明了欧拉建立的条件：图G具有欧拉回路当且仅当G中的所有顶点都具有偶次。

3. 图具有欧拉轨迹的条件是什么？

4. 给定图3.10中的图形，确定其是否具有（a）欧拉回路（b）欧拉轨迹（c）哈密顿回路（d）哈密顿轨迹。

5. 给定图3.10中的图表，是否进行三角测量？如果未进行三角化，请通过应用图形填充算法进行三角化。

6. 证明图中奇数阶顶点的数目是偶数。

7. 给定图3.13中的图，将其转换为无向图，并应用最大基数搜索算法对节点进行排序。

   <div> <!--块级封装-->
       <center> <!--将图片和文字局中-->
       <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101193114967.png" alt="image-20230101193114967" style="zoom:60%;" />
       <br> <!--换行-->
       图3.13 几个练习中使用的图表 <!--标题-->
       </center>
   </div>

8. 对上一个问题的图形进行三角剖分。

9. 给定在前一个问题中获得的三角化图：（a）找到其簇，（b）根据节点排序对簇进行排序，并验证它们是否满足运行交集属性，（c）显示生成的簇树。

10. \*\*\*开发一个程序，用于在给定无向图的情况下生成一个群树。为此，考虑（a）根据最大基数搜索对节点进行排序，（b）使用图填充算法对图进行三角化，（c）找到生成的图的簇并对其进行编号。考虑到前面算法的实现，考虑一个足够的数据结构来表示图形。

11. \*\*\*将一些启发式方法结合到先前练习的程序中，以选择节点排序，从而使图中最大团的大小最小化（这对于贝叶斯网络的连接树推断算法很重要）。

#### References

1. Aho, A.V., Hopcroft, J.E., Ullman, J.D.: The Design and Analysis of Computer Algorithms【计算机算法的设计与分析】.
   Addison-Wesley, Boston (1974)
2. Golumbic, M.C.: Algorithmic Graph Theory and Perfect Graphs【算法图论和完美图】. Elsevier, The Netherlands
   (1994)
3. Gould, R.: Graph Theory【图论】. Benjamin/Cummings, Menlo Park (1988)
4. Gross, J.L., Yellen, J.: Graph Theory and its Applications【图论及其应用】. CRC Press, Boca Raton (2005)
5. Neapolitan, R.: Probabilistic Reasoning in Expert Systems: Theory and Algorithms【专家系统中的概率推理：理论和算法】. Wiley, New
   York (1990)



### 第五节 隐马尔可夫模型（Hidden Markov Models）

```html
<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="图片路径放这里" 
         alt="无法显示图片时显示的文字" 
         style="zoom:这里写图片的缩放百分比"/>
    <br> <!--换行-->
    这里是图片的标题 <!--标题-->
    </center>
</div>
```

![img](https://pic3.zhimg.com/v2-7758cdbd6f4ea67d59f67acb35cfc4fa_r.jpg)

#### 5.1 引言

马尔可夫链是表示动态过程的另一类PGM，特别是过程状态如何随时间变化。例如，假设我们正在模拟特定地点的天气如何随时间变化。在一个非常简化的模型中，我们假设天气在一天中是恒定的，并且可以有三种可能的状态：晴朗、多云、下雨。此外，我们假设某一天的天气只取决于前一天。因此，我们可以把这个简单的天气模型看作一个马尔可夫链，其中每天都有一个状态变量，有三个可能的值；这些变量以一条链的形式连接在一起，从一天到下一天有一条有向弧（见图5.1）。这意味着所谓的马尔可夫性质，即第二天的天气状态$S_{t+1}$，与给定当前天气$S_t$的所有前几天无关，即$P(S_{t+1}\ |\ S_t，S_{t−1}，…)=P(S_{t+1} \ |\ S_t)$。因此，在马尔可夫链中，所需的主要参数是给定前一状态的概率。

前一个模型假设我们可以每天精确地测量天气，即状态是可观测的。然而，这并不一定是真的。在许多应用程序中，我们无法直接观察过程的状态，因此我们有一个所谓的隐马尔可夫模型，其中状态是隐藏的。在这种情况下，除了给定当前状态的下一个状态的概率之外，还有另一个参数对状态的不确定性进行建模，表示为给定状态的观测概率$P(O_t \ |\ S_t)$。这种类型的模型比简单的马尔可夫链更强大，有许多应用，例如在语音和手势识别中。

在简要介绍了马尔可夫链之后，在接下来的章节中，我们将详细讨论隐马尔可夫模型，包括如何解决这类模型的计算问题。然后，我们讨论了标准HMM的几个扩展，并以两个应用示例结束本章。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101220744434.png" alt="image-20230101220744434" style="zoom:100%;" />
    <br> <!--换行-->
    图5.1 该图说明了一个马尔可夫链，其中每个节点表示某个时间点的状态 <!--标题-->
    </center>
</div>
#### 5.2 马尔可夫链（Markov Chains）

![image-20230102175644620](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102175644620.png)

如果小明第一天选A，再选A的概率是40%，选B的概率是60%，而若第一天选B，再选B的概率是50%，选A的概率是50%。它只跟前一天的状态相关。马尔可夫链有可能有稳态分布。初始状态*转移矩阵的n次方会得到稳态分布概率。

![image-20230102175856874](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102175856874.png)

![image-20230102182651536](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102182651536.png)



马尔可夫链（MC）是具有离散数量的状态q1、q2、…的状态机，q n，并且状态之间的转换是不确定的，即存在从状态qi转换到另一状态q j:P（St=q j|St−1=qi）的概率。时间也是离散的，使得链对于每个时间步t可以处于某个状态qi。它满足马尔可夫性质，即下一个状态的概率仅取决于当前状态。

形式上，马尔可夫链定义为

**状态集（Set of states）**：$Q = \{q_1, q_2, . . . , q_n \}$

**先验概率向量（Vector of prior probabilities）**：$Π = \{π_1, π_2, . . . , π_n \}$， 当$π_i = P(S_0 = q_i )$

**转移概率矩阵（Matrix of transition probabilities）**：

$A = \{a_{ij} \}, i = [1..n], j = [1..n]$，当 $a_{ij} =
P(S_t = q_j | S_{t−1} = q_i )$

$a_{ij}$  是给定 $S_{t−1} = q_i$ 时，$S_t = q_j$的概率

其中 $n$ 是状态数，$S_0$ 是初始状态。以紧凑的方式，MC表示为$λ = \{A, Π \}$.

（一阶）马尔可夫链满足以下性质：

1. 概率公理：$∑_{i}π_{i}=1$和$∑_{j}a_{i j}=1$

2. 马尔可夫性质：$P(S_t=q_j | S_{t−1}=q_i，S_{t−2}=q_k，…)=P(S_t=q_j \ | \ S_{t−1}=q_i)$

例如，考虑前面的简单天气模型，它有三种状态：$q_1$=晴朗，$q_2$=多云，$q_3$=下雨。在这种情况下，为了指定MC，我们需要一个具有三个先验概率的向量（见表5.1）和一个3×3的转移概率矩阵（见表5.2）。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101222759151.png" 
         alt="image-20230101222759151" 
         style="zoom:80%;" />
    <br> <!--换行-->
    表5.1 天气示例的先验概率 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101223012717.png" 
         alt="image-20230101223012717" 
         style="zoom:80%;" />
    <br> <!--换行-->
    表5.2 天气示例的转移概率 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101223153128.png" 
         alt="image-20230101223153128" 
         style="zoom:80%;" />
    <br> <!--换行-->
    图5.2 该图说明了天气示例的状态转换图 <!--标题-->
    </center>
</div>

其中每个节点是一个状态，并且弧表示状态之间的可能转变。如果图中没有出现状态$q_i$和$q_j$之间的弧，则意味着相应的跃迁概率为零。天气示例的状态图示例如图5.2所示。1【不要将状态图与图形模型图混淆，其中节点表示每个状态一个随机变量的特定值，弧在状态之间转换，其中节点代表随机变量，弧表示概率依赖关系。】

给定马尔可夫链模型，我们可以**问三个基本问题：**

- 某一状态序列的概率是多少？
- 链条在一段时间内保持某种状态的概率是多少？
- 链条保持在某一状态的预期时间是多久？

接下来，我们将看到如何回答这些问题，并将使用天气示例来说明这些问题。

给定模型的状态序列的概率基本上是状态序列的转移概率的乘积：
$$
P(q_i,q_j,q_k,...)=a_{0i} a_{ij} a_{jk} \tag{5.1}
$$
其中$a_{0i}$是序列中到初始状态的转变，它可以是它的先验概率（$π_i$）或从先前状态的转移（如果这是已知的）。

例如，在天气模型中，我们可能想知道以下状态序列的概率：Q=晴、晴、雨、雨、晴、多云、晴。假设晴天是MC中的初始状态，则：
$$
P(Q)=π_1a_{11}a_{13}a_{33}a_{31}a_{12}a_{21}=(0.2)(0.8)(0.1)(0.4)(0.3)(0.1)(0.2)=3.84×10^{-5}
$$
将d个时间步长保持在某一状态的概率$q_i$相当于序列在该状态下$d−1$个时间步长，然后转变到不同状态的概率。即，
$$
P(d_i ) = a^{d−1}_{ii}(1 − a_{ii} ) \tag{5.2}
$$
考虑到天气模型，三天多云的概率是多少？其计算方法如下：
$$
P(d_2=3)=a^2_{22}(1-a_{22})=0.6^2(1-0.6)=0.144 
$$
某一状态下状态序列的平均持续时间是该状态下级数的期望值，即$E(D)=∑_i d_i P(d_i)$。代入方程（5.2），我们得到：
$$
E(d_i)=∑_i d_i a^{d−1}_{ii}(1 − a_{ii} ) \tag{5.3}
$$
其可以紧凑形式写成：
$$
E(d_i ) = 1/(1 − a_{ii} ) \tag{5.4}
$$
例如，天气持续多云的预计天数是多少？使用前面的公式：
$$
E(d_2 ) = 1/(1 − a_{22} )=1/(1−0.6)=2.5
$$

##### 5.2.1 参数估算

另一个重要问题是如何确定模型的参数，这就是所谓的参数估计。对于MC，可以简单地通过计算**序列处于特定状态 $i$ 的次数**以及**从状态 $i$ 到状态 $j$ 的转换次数**来估计参数。假设有$N$个观测序列。$γ_{0i}$ 是状态 $i$ 是序列中初始状态的次数， $γ_i$ 是我们观察状态 $i$ 的次数，而$γ_{ij}$是我们观察到从状态 $i$ 到状态 $j$ 的转变的次数。参数可以用以下方程估计。

初始概率：
$$
π_i = γ_{0i} /N \tag{5.5}
$$
转换概率：
$$
a_{ij} = γ_{ij} /γ_{i} \tag{5.6}
$$
注意，对于序列中最后一个观察到的状态，我们不会观察到下一个状态，因此计数中不考虑所有序列的最后一个状态。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101230622955.png" 
         alt="image-20230101230622955" 
         style="zoom:100%;" />
    <br> <!--换行-->
    表5.3 天气示例的计算先验概率 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101230649311.png" 
         alt="image-20230101230649311" 
         style="zoom:100%;" />
    <br> <!--换行-->
    表5.4 天气示例的计算转换概率 <!--标题-->
    </center>
</div>

例如，对于天气示例，我们有以下四个观测序列（$q_1$=晴朗，$q_2$=多云，$q_3$=下雨）：
$$
q_2, q_2, q_3, q_3, q_3, q_3, q_1\\
q_1, q_3, q_2, q_3, q_3, q_3, q_3\\
q_3, q_3, q_2, q_2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
q_2, q_1, q_2, q_2, q_1, q_3, q_1\\
$$
给定这四个序列，可以如表5.3和5.4所示估计相应的参数。

##### 5.2.2收敛(Convergence)

另外一个有趣的问题是：如果一个序列从一个状态转换到另一个状态的次数很多，M，极限中的概率是多少（$M→ ∞$）每一个状态，$q_i$？

给定初始概率向量 $∏$ 和转移矩阵 $A$，$M$ 次迭代后每个状态的概率$P=\{p_1,p_2,…,p_n\}$为：
$$
P = π A^M \tag{5.7}
$$
当 $M→ ∞$ ? 该解由Perron Frobenius定理给出，该定理表示当满足以下两个条件时：

1. 不可约性（Irreducibility）：从每一个状态 $i$ ，都有一个概率$a_{ij}＞0$过渡到任何状态 $j$ 。

2. 非周期性（Aperiodicity）：链不形成循环（链在过渡到这些状态之一后仍保持的状态子集）。

然后作为 $M→ ∞$ , 链收敛到不变分布$P$，使得$P×A＝P$，其中$A$是转移概率矩阵。收敛速度由矩阵$A$的第二特征值决定。

例如，考虑具有三个状态的MC和以下转移概率矩阵：
$$
A= \left[
\begin{matrix}
0.9 & 0.075 & 0.025 \\
0.15 & 0.8 & 0.05 \\
0.25 & 0.25 & 0.5\\
\end{matrix}
\right]
$$
可以看出，在这种情况下，稳态概率收敛到$P＝\{0.625，0.3125，0.0625\}$。

应用程序部分介绍了马尔可夫链的这种收敛特性在网页排名中的有趣应用。接下来我们讨论隐马尔可夫模型。

#### 5.3隐马尔可夫模型( Hidden Markov Models)

举例说明：早餐不可测，但是选择的饮料可测，转移矩阵已知，早餐-饮料概率矩阵已知

![image-20230102183527413](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102183527413.png)

![image-20230102184744745](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102184744745.png)![image-20230102184842403](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102184842403.png)![image-20230102184938391](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102184938391.png)

![image-20230102185022314](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102185022314.png)

viterbi Algorithm（删除概率低的路径）

![image-20230102184311241](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102184311241.png)

隐马尔可夫模型（HMM）是**状态不可直接观察**的马尔可夫链。例如，如果我们考虑天气示例，则无法直接测量天气；实际上，天气是基于一系列传感器（温度、压力、风速等）来估计的。因此，在这种情况下，与许多其他现象一样，状态是无法直接观察到的，HMM提供了一种更合适、更强大的建模工具。关于HMM的另一种思考方式是，它是一个双重随机过程：（i）一个我们无法直接观察到的隐藏随机过程，（ii）第二个随机过程，在给定第一个过程的情况下产生观察序列。

例如，假设我们有两个不公平或“有偏见”的硬币，M1和M2。M1具有较高的头部概率，而M2具有较高的尾部概率。有人依次翻转这两个硬币，然而，我们不知道是哪一个。我们只能观察结果：
$$
H, T, T, H, T, H, H, H, T, H, T, H, T, T, T, H, T, H, H, . . .
$$
假设掷硬币的人选择序列中的第一个硬币（先前概率），而给定先前的一个硬币（转移概率），下一个硬币将以相等的概率被掷。除了状态的先验概率和转移概率（如MC），在HMM中，我们需要指定观察概率，在这种情况下，给定每个硬币（状态）的H或T的概率。让我们假设M1的头部概率为80%，M2的尾部概率为80%。然后，我们为这个简单的示例指定了所有必需的参数，汇总在表5.5中。

硬币示例的状态图如图5.3所示，具有两个状态变量和两个可能的观察值，这取决于状态。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101233054038.png" 
         alt="image-20230101233054038" 
         style="zoom:100%;" />
    <br> <!--换行-->
    表5.5不公平硬币示例的先验概率（∏）、转移概率（A）和观测概率（B） <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101233314510.png" 
         alt="image-20230101233314510" 
         style="zoom:100%;" />
    <br> <!--换行-->
    图5.3 HMM硬币示例的状态图。图中显示了两个状态q1和q2以及两个观测值H和T，圆弧表示跃迁和观测概率 <!--标题-->
    </center>
</div>

形式上，隐马尔可夫模型定义为

**状态集（Set of states）**：$Q = \{q_1, q_2, . . . , q_n \}$

**观察集（Set of observations）**：$O = \{o_1, o_2, . . . , o_m \}$

**先验概率向量（Vector of prior probabilities）**：$Π = \{π_1, π_2, . . . , π_n \}$， 当$π_i = P(S_0 = q_i )$

**转移概率矩阵（Matrix of transition probabilities）**：

$A = \{a_{ij} \}, i = [1..n], j = [1..n]$，当 $a_{ij} =
P(S_t = q_j | S_{t−1} = q_i )$

**观测概率矩阵（Matrix of observation probabilities）**：$B = \{b_{ij} \}, i = [1..n], j = [1..m]$，当 $b_{ik} =
P(O_t = o_k | S_{t} = q_i )$

其中$n$是状态数，$m$是观察数；$S_0$是初始状态。紧凑地，HMM表示为$λ=\{A，B，∏\}$。

（一阶）HMM满足以下性质：

马尔可夫性质：$P(S_t=q_j | S_{t−1}=q_i，S_{t−2}=q_k，…)=P(S_t=q_j\  |\  S_{t−1}=q_i)$

平稳过程：$P(S_{t−1}＝q_j \  |\ S_{t−2}＝q_i)＝P(S_{t}=q_j\  |\  S_{t−1}＝q_i)$和$P(O_{t−1}＝o_k \  |\  S_{t−1}=q_j)＝P(O_{t}＝o_k \  |\  S_{t}=q_j), ∀(t)$

观察的独立性：$P(O_{t}＝o_k \  |\  S_{t}=q_i,S_{t-1}=q_j,…)=P(O_t=o_k \  |\  S_t=q_i)$

与MC的情况一样，马尔可夫性质意味着当前状态的概率仅取决于先前状态，并且与历史的其余部分无关。第二个属性表示跃迁和观测概率不会随时间变化，即过程是平稳的。第三个属性指定观察值仅取决于当前状态。对基本HMM的扩展放宽了某些假设；其中一些将在下一节和后续章节中讨论。

根据前面的属性，HMM的图形模型如图5.4所示，其中包括两系列随机变量，即时间$t$，$S_t$时的状态和时间$t$，$O_t$时的观测值

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101234300752.png" 
         alt="image-20230101234300752" 
         style="zoom:100%;" />
    <br> <!--换行-->
    图5.4 表示隐藏马尔可夫模型的图形模型 <!--标题-->
    </center>
</div>

给定某个域的HMM表示，大多数应用程序中有三个基本问题感兴趣[7]：

1. 评估：给定一个模型，估计一系列观测的概率。
2. 最优序列：给定模型和特定观测序列，估计产生观测的最可能状态序列。
3. 参数学习：给定一系列观察结果，调整模型的参数。

接下来将描述解决这些问题的算法，假设标准HMM具有有限数量的状态和观测值。

举例：nlp中的隐马尔可夫模型

![image-20230102185808612](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102185808612.png)

利用语言处理工具“tokenization”进行分词，结果是“小明”，“爱”，“游戏”。

不同词性间的转移概率矩阵

![image-20230102190042131](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102190042131.png)

初始状态下，首字词的词性概率可以使用马尔可夫链的稳态分布结果表示

![image-20230102190411990](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102190411990.png)

各个词性中，对应词语的出现概率

![image-20230102190348930](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102190348930.png)

![image-20230102190447684](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102190447684.png)

![image-20230102190517940](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102190517940.png)

初始词性稳态分布概率【绿色】\*词性间转移矩阵概率【黄色】\*词性词语条件概率，然后比较各种可能组合后，取最优值作为最终结果。也可以使用viterbi算法进行优化计算。【词袋、词嵌入、词向量等NLP基础概念】

##### 5.3.1评估

评估包括在给定模型λ的情况下，确定观测序列$O = \{o_1, o_2, o_3, . . .  \}$的概率，即估计$P(O\ |\ λ)$。我们提出了两种方法。首先，我们提出了直接方法，这是一种天真的算法，激发了对更高效算法的需求，然后对其进行了描述。

###### 5.3.1.1直接法

观测序列$O = \{o_1, o_2, o_3, . . . ,o_T \}$可以由不同的状态序列$Q_i$生成，因为HMM的状态是未知的。因此，为了计算观测序列的概率，我们可以对某个状态序列进行估计，然后将所有可能状态序列的概率相加：
$$
P(O\  |\  λ) = ∑_i {P(O, Q_i \ |\  λ)} \tag{5.8}
$$
为了获得$P(O, Q_i \ |\  λ)$，我们简单地将初始状态的概率$q_1$乘以状态序列的转移概率$q_1,q_2$。以及观测序列的观测概率$o_1.o_2.…$：
$$
P(O, Q_i \ |\  λ)=π_1b_{1}(o_1)a_{12}b_{2}(o_2)...a_{(T-1)T}b_{T}(o_T) \tag{5.9}
$$
因此，O的概率由所有可能的状态序列Q的总和给出：
$$
P(O \ |\  λ)=\sum_Q{π_1b_{1}(o_1)a_{12}b_{2}(o_2)...a_{(T-1)T}b_{T}}(o_T) \tag{5.10}
$$
对于具有$N$个状态且观测长度为$T$的模型，存在$N^T$个可能的状态序列。求和中的每个项都需要$2T$运算。因此，评估需要$2T×N^T$数量级的操作。

例如，如果我们考虑具有五个状态（$N=5$）和长度为$T=100$的观测序列（这是HMM应用的常见参数）的模型，则所需操作的数量为$10^{72}$。需要更有效的方法！

###### 5.3.1.2 迭代法（Iterative Method）

迭代方法的基本思想，也称为正向，是估计每个时间步的状态/观测的概率。也就是说，计算直到时间 $t$（开始形式 $t=1$）的部分观测序列的概率，并基于该部分结果，计算时间 $t+1$ 的概率，依此类推。

首先，我们定义一个名为forward的辅助变量：
$$
\alpha_t(i)=P(o_1,o_2,...,o_t,S_t=q_i\ |\  \lambda) \tag{5.11}
$$
也就是说，直到时间t的部分观测序列在时间 $t$ 处于状态 $q_i$ 的概率。

迭代算法由三个主要部分组成：初始化、归纳和终止。在初始化阶段，获得初始时间所有状态的 $α$ 变量。诱导阶段包括根据$α_t(i)$计算$α_{t+1}(i)$；这从$t＝2$重复到$t＝t$。最后，通过将终止阶段的所有$α^T$相加，得到$P(O\ |\ λ)$。该过程如算法5.1所示。

![image-20230102141156594](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102141156594.png)

---

算法5.1正向算法

---

要求：$HMM$，$λ$；观测序列，$O$；状态数，$N$；观测次数，$T$

**for** $i=1$ **to** $N$ **do**:

​		$α_1(i)=P(O_1，S_1=q_i)=π_{i}b_{i}(O_1)$（初始化【Initialization】）

**end for**

**for** $t=2$ **to** $T$ **do**:

​	**for ** $j=1$ **to** $N$ **do**:

​			$α_t(j)=[∑_iα_{T−1}(i)a_{ij}]b_j(O_t)$（诱导【Induction】）

​	**end for**

**end for**

$P(O)=∑_iα_T(i)$（终止【Termination】）

**return** $P(O)$

---

现在让我们分析迭代方法的时间复杂性。每一次迭代都需要$N$次乘法和$N$次加法（约），因此对于T次迭代，运算次数为$N^2×T$。因此，时间复杂度从直接方法的T中的指数减少到迭代方法的$T$和$N$中的二次，显著降低了复杂度；注意，在大多数应用中 $T>>N$。

回到$N＝5$和$T＝100$的示例，现在操作的数量大约为 $2500$，这可以用标准个人计算机在几毫秒内执行。

刚刚描述的迭代程序是求解HMM的其他两个问题的基础。下面将描述这些。

##### 5.3.2 状态估计（State Estimation）

找到观测序列$O = \{o_1, o_2, o_3, . . .  \}$的最可能状态序列可以用两种方式解释：（i）在每个时间步骤t获得最可能状态$S_t$，（ii）获得最可能的状态序列$s_0,s_1,...,s_T$。注意，对于 $t=1$，每个时间步长的最可能状态的串联。T不一定与最可能的状态序列相同。2【这是最有可能解释或MPE问题的一个特例，将在第7章讨论。】首先，我们解决在一定时间T内找到最可能或最佳状态的问题，然后是找到最佳序列的问题。

我们首先需要定义一些附加的辅助变量。向后变量类似于向前变量，但在这种情况下，我们从序列的末尾开始，即，
$$
\beta_t(i)=P(o_{t+1},o_{t+2},...,o_{T},S_t=q_i \ |\ \lambda) \tag{5.12}
$$
也就是说，从 $t+1$ 到 $t$ 的部分观测序列在时间 $t$ 处于状态 $q_i$ 的概率。以与 $α$ 类似的方式，它可以迭代计算，但现在向后计算：
$$
\beta_t(i)=\sum_{j}{\beta_{t+1}(j)a_{ij}b_j(o_t)} \tag{5.13}
$$
$T$的$β$变量定义为$β_T(j)=1$。

因此，我们也可以使用$β$而不是$α$来解决前一节的评估问题，从观察序列的末尾开始，通过时间向后迭代。或者我们可以组合两个变量，向前和向后迭代，在某个中间时间t相遇；即，
$$
P(O,s_t=q_i \ | \ \lambda)=\alpha_t(i)\beta_t(i) \tag{5.14}
$$
然后：
$$
P(O\ |\ λ)=\sum_{i}{\alpha_t(i)\beta_t(i)} \tag{5.15}
$$
现在我们定义了一个附加变量$γ$，即给定观测序列，处于某个状态$q_i$的条件概率：
$$
\gamma_t(i)=P(s_t=q_i \ |\ O, \lambda)=P(s_t=q_i,O\ | \ \lambda)/P(O) \tag{5.16}
$$
其可以用$α$和$β$表示为：
$$
\gamma_t(i)=\alpha_t(i)\beta_t(i)/\sum_{i}{\alpha_t(i)\beta_t(i)} \tag{5.17}
$$
这个变量 $γ$ 提供了第一个子问题的答案，即时间 $t$ 时的最可能状态（MPS）；我们只需要找到它在哪个状态下具有最大值。即，
$$
MPS(t)=ArgMax_{i}\gamma_{t}(i) \tag{5.18}
$$
现在让我们解决第二个子问题，即给定观测序列 $O$ 的最可能状态序列 $Q$ ，这样我们希望最大化 $P(Q\ |\ O，λ)$。根据贝叶斯规则：$P(Q\ |\ O，λ)=P(Q，O\ |\ λ)/P(O)$。假设$P(O)$不依赖于$Q$，这相当于最大化$P(Q，O\ |\ λ)$。

用于获得最佳状态序列的方法被称为维特比算法，其以与前向算法类似的方式迭代地解决问题。在我们看到算法之前，我们需要定义一个附加变量 $δ$ 。该变量给出了直到时间 $t$ 的状态和观测的子序列的概率的最大值，在时间 $t$ 处于状态 $q_i$ ；即：
$$
\delta_{t}(i)=MAX[P(s_1,s_2,...,s_t=q_i,o_1,o_2,...,o_t\ |\ \lambda)] \tag{5.19}
$$
其也可以以迭代方式获得：
$$
\delta_{t+1}(i)=[MAX\delta_t(i)a_{ij}]b_j(o_{t+1}) \tag{5.20}
$$
维特比算法需要四个阶段：初始化、递归、终止和回溯。它需要一个额外的变量$ψ_t(i)$，为每个状态 $i$ 在每个时间步长 $t$ 存储给出最大概率的前一状态。这用于在终止阶段之后通过回溯来重建序列。算法5.2描述了完整的过程。

使用维特比算法，我们可以获得最可能的状态序列，即使这些状态对于HMM是隐藏的。

![image-20230102145822690](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102145822690.png)

---

算法5.2 维特比算法(The viterbi Algorithm)

---

要求：$HMM$，$λ$；观测序列，$O$；状态数，$N$；观测次数，$T$

**for** $i=1$ **to** $N$ **do**:

​	（初始化），

​	$δ_1(i)=π_{i}b_{i}(O_1)$

​	$ψ1(i)=0$，

**end for**

**for** $t=2$ **to** $T$ **do**，

​	**for** $j=1$ **to** $N$ **do**，

​		（递归），

​		$δ_t(j)=MAX_i[δ_{t−1}(i)a_{ij}]b_j(O_t)$

​		$ψ_t(i)=ARGMAX_i[δ_{t-1}(i)a_{ij}]$，

​	**end for**

**end for**

（终止），

$P^*=MAX_i[\delta_{T}(i)]$

$q^*_T=ARGMAX_i[δ_T(i)]$，

**for** $t=T$ **to** $2$ **do**:

​	（回溯）

​	$q^*_{t−1}=ψ_t(q^*_t)$

**end for**

---



##### 5.3.3 Learning

最后，我们将看到如何通过Baum-Welch算法从数据中学习HMM。首先，我们应该注意，这种方法假设模型的结构是已知的：状态和观测值的数量是预先定义的；因此，它只估计参数。通常，观察结果是由应用程序域给出的，但隐藏状态的数量不那么容易定义。有时，可以基于领域知识定义隐藏状态的数量；在其他时候，它是通过试验和错误来完成的：用不同数量的状态（2，3，…）测试模型的性能，并选择给出最佳结果的数量。应该注意，在这种选择中存在权衡，因为更多的状态往往会产生更好的结果，但也意味着额外的计算复杂性。

Baum-Welch算法确定HMM的参数，$λ=\{A,B,∏\}$，给定多个观测序列，$O = {O_1, O_2, . . . ,O_K }$。好吧。为此，它最大化了给定观测值的模型概率：$P（O|λ）$。对于具有$N$个状态和$M$个观测值的HMM，我们需要分别估计$∏$、$A$和$B$的$N+N^2+N×M$参数。

我们需要再定义一个辅助变量$ξ$，即给定观测序列$O$，从时间$t$的状态$i$到时间$t+1$的状态$j$的转换概率：
$$
\xi_t(i,j)=P(s_t=q_i,s_{t+1}=q_j \ |\ O,\lambda)=P(s_t=q_i,s_{t+1}=q_j,O \ |\ \lambda)/P(O) \tag{5.21}
$$
就$\alpha$和$\beta$而言：
$$
\xi_{t}(i,j)=\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)/P(O) \tag{5.22}
$$
用$α$和$β$表示$P(O)$：
$$
\xi_{t}(i,j)=\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)/\sum_i{\sum_j{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}} \tag{5.23}
$$
$γ$也可以用$ξ$表示：$γ_t(i)=∑_jξ_t(i,j)$

通过对所有时间步长$∑_tγ_t(i)$加上$γ_t(i)$，我们得到了链处于状态 $i$ 的次数的估计，并且通过在 $t$ 上累积$ξ_t(i,j)$，$∑_tξ_t(i,j)$，我们估计了从状态 $i$ 到状态 $j$ 的转变次数。

基于前面的定义，算法5.3总结了HMM参数估计的Baum-Welch程序。

![image-20230102154843559](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102154843559.png)

---

算法5.3 Baum-Welch算法

---

1. 估计在时间t处于状态$i$的次数的先验概率
   $$
   \pi_i=\gamma_1(i)
   $$

2. 估计状态$i$中的次数之间从状态$i$到$j$的转换次数的转换概率
   $$
   a_{ij}=\sum_t\xi_t(i,j)/\sum_t{\gamma_t(i)}
   $$

3. 估计处于状态$j$的次数和处于状态$j$中的次数之间的观察概率$k$。
   $$
   b_{jk}=\sum_{t,O=k}\gamma_t(j)/\sum{\gamma_t(j)}
   $$
   

---

注意，$γ$和$ξ$变量的计算是根据$α$和$β$进行的，这需要$HMM$、$∏$、$A$和$B$的参数。因此，我们遇到了一个“鸡和蛋”的问题，我们需要Baum-Welch算法的模型参数，该算法估计模型参数！这个问题的解决方案基于EM（期望最大化）原则。

其想法是从模型的一些初始参数（E步）开始，$λ={A，B，∏}$，可以随机初始化或基于一些领域知识。然后，通过Baum-Welch算法，重新估计这些参数（M步）。重复该循环直到收敛；即直到模型从一个步骤到下一个步骤的参数之间的差异低于某个阈值。

EM算法提供了所谓的最大似然估计器，它不能保证最优解，这取决于初始条件。然而，这种估计在实践中往往工作得很好。3.

##### 5.3.4 Extensions

已经提出了对标准HMM的若干扩展，以处理若干应用中的特定问题[2]。接下来，我们简要描述其中的一些扩展，其图形模型如图5.5所示。

参数HMM（PHMM）表示涉及模型变化的领域。在PHMM中，观察变量被调节为状态变量和一个或多个解释这种变化的参数，见图5.5b。参数值是已知的，在训练时是恒定的。在测试中，最大化PHMM的值的可能性通过定制的期望最大化算法来恢复。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102160354622.png" 
         alt="image-20230102160354622" 
         style="zoom:100%;" />
    <br> <!--换行-->
    图5.5 基本HMM和几个扩展的图形模型表示。
    <br>a.基本模型
    <br>b.参数HMM
    <br>c.耦合HMM
    <br>d.输入输出HMM
    <br>e.并行HMM
    <br>f.分级HMM
    <br>g.混合状态动态贝叶斯网络
    <br>h.隐半马尔可夫模型 <!--标题-->
    </center>
</div>

耦合HMM（CHMM）通过在状态变量之间引入条件依赖关系来连接HMM，见图5.5c。这些模型适用于表示并行发生的子过程之间的影响。

输入-输出HMM（IOHMM）考虑影响马尔可夫链状态的额外输入参数，以及可选的观察变量。这些类型的模型如图5.5d所示。输入变量与观测值相对应。IOHMM的输出信号是正在执行的模型类（例如，语音识别中的音素或手势识别中的特定运动）。一个IOHMM可以描述一组完整的类。

通过假设HMM之间的相互独立性，并行HMM（PaHMM）需要比CHMM更少的HMM用于复合过程，见图5.5e。其思想是为两个（或多个）独立的并行过程（例如，手势识别中每只手的可能运动）构建独立的HMM，并通过乘以它们各自的似然度来组合它们。具有最可能联合似然性的PaHMM定义了所需类别。

分层隐马尔可夫模型（HHMM）将HMM排列成不同抽象级别的层；图5.5f。在两层HHMM中，下层是一组表示子模型序列的HMM。上层是控制这些子模型动力学的马尔可夫链。分层允许我们简单地通过改变上层来重用基本HMM。

混合状态动态贝叶斯网络（MSDBNs）4将离散和连续状态空间组合成两层结构。MSDBN由上层的HMM和下层的线性动态系统（LDS）组成。LDS用于模拟实值状态之间的转换。HMM的输出值驱动线性系统。MSDBN的图形表示如图5.5g所示。在MSDBN中，HMM可以描述离散的高级概念，如语法，而LDS描述连续状态空间中的输入信号。

隐半马尔可夫模型（HSMM）通过定义每个状态的显式持续时间来利用属于过程的时间知识，见图5.5h。HSMM适合于在建模大型观测序列时避免状态概率的指数衰减。



#### 5.4 应用

在本节中，我们将说明马尔可夫链和HMM在两个领域中的应用。首先，我们描述了利用PageRank算法使用马尔可夫链对网页进行排序。然后，我们介绍了HMM在手势识别中的应用。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102160734396.png" 
         alt="image-20230102160734396" 
         style="zoom:100%;" />
    <br> <!--换行-->
    图5.6 具有3个页面的WWW的一个小示例 <!--标题-->
    </center>
</div>

##### 5.4.1 PageRank

我们可以将万维网（WWW）视为一个非常大的马尔可夫链，这样每个网页都是一个状态，网页之间的超链接对应于状态转换。假设有N个网页。一个特定的网页wi有m个传出超链接。如果有人在网页wi，她可以选择此网页中的任何超链接转到另一个网页。一个合理的假设是，可以以相等的概率选择每个传出链路；因此，从wi到其具有超链接的任何网页的转换概率wj为A i j=1/m。对于没有传出链接的其他网页，转换概率为零。这样，根据WWW的结构，我们可以获得相应马尔可夫链的转移概率矩阵A。具有三个网页的小示例的状态图如图5.6所示。

给定WWW的转移概率矩阵，我们可以根据Perron Frobenius定理（见第5.2节）获得每个状态（网页）的收敛概率。某个网页的收敛概率可以被认为与浏览WWW的人访问该网页的概率相等。直观地说，具有更多进入链接的网页，与具有更多进入连接的网页相比，将具有更高的被访问概率。

基于之前的想法，L.Page等人开发了PageRank算法，这是我们在谷歌搜索网页时如何排序的基础[6]。由搜索算法检索的网页根据其收敛概率呈现给用户。其想法是，更相关（重要）的网页往往具有更高的收敛概率。

##### 5.4.2 手势识别

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102161436788.png" 
         alt="image-20230102161436788" 
         style="zoom:80%;" />
    <br> <!--换行-->
    图5.7 描述一个人用右手做停止手势的视频序列。显示了几个关键帧 <!--标题-->
    </center>
</div>

手势对人与人之间的交流至关重要，因此对人与计算机的交互也很重要。例如，我们可以使用手势命令服务机器人。我们将重点关注动态手势，即人的手/手臂的动作。例如，图5.7描绘了一个人执行停止手势的一些帧。

对于识别手势，一个强大的选择是隐藏的马尔可夫模型[1，9]。HMM适合于对顺序过程进行建模，并且对动态手势执行中常见的时间变化具有鲁棒性。在我们将HMM应用于建模和识别手势之前，需要对视频序列中的图像进行处理，并从中提取一组特征；这些将构成HMM的观测结果。

图像处理包括检测图像中的人，检测他们的手，然后在图像序列中跟踪手。从图像序列中，从每个图像中提取手的位置（XYZ）。此外，可以检测身体的一些其他区域，例如头部和躯干，这些区域用于获得如下所述的姿势特征。

描述姿势的备选方案可以分为：（a）运动特征，（b）姿势特征，和（c）姿势运动特征。运动特征描述人的手在笛卡尔空间XYZ中的运动。姿势特征表示手相对于身体其他部位（如头部或躯干）的位置。这些运动和姿势特征通常被编码为有限数量的码字，这些码字为HMM提供观察结果。例如，如果我们考虑三个值来描述每个运动坐标和两个二进制姿势特征（例如，手在头部上方等），则会有m=3×3×3×2×2=108个可能的观察结果。这些是针对手势的视频序列中的每一帧（或每n帧）获得的。

为了识别N个不同的手势，我们需要训练N个HMM，每个手势一个HMM。需要定义的第一个参数是每个模型的隐藏状态数。如前所述，这可以基于领域知识来设置，或者通过实验评估不同数量的状态来通过交叉验证来获得。在动态手势的情况下，我们可以将状态视为表示手的运动的不同阶段。例如，停止手势可以被认为有三个阶段：向上和向前移动手臂、伸出手和向下移动手臂；这意味着三种隐藏状态。实验发现，使用三到五个状态通常会产生良好的结果。

一旦定义了每个手势模型的状态数（每个模型的状态可能不同），就可以使用Baum-Welch程序获得参数。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230102161217997.png" 
         alt="image-20230102161217997" 
         style="zoom:100%;" />
    <br> <!--换行-->
    图5.8 使用HMM的手势识别。从视频序列中提取的特征作为观察结果被馈送到每个HMM，每个手势类一个，并且获得每个模型的概率。概率最高的模型定义了识别的手势 <!--标题-->
    </center>
</div>

为此，每个手势类需要一组M个训练序列，样本越多越好。因此，获得N个HMM，每个手势类型一个。

为了识别，从视频序列中提取特征。这是N个HMM模型λi的观测值O，每个手势类型一个。使用Forward算法获得给定观测序列的每个模型的概率P（O|λi）。选择概率最高的模型λ*k作为识别手势。图5.8说明了识别过程，考虑了五类手势。



#### 5.5 附加阅读

[4]中提供了马尔可夫链的一般介绍。Rabiner[7]介绍了HMM及其在语音识别中的应用；[8] 提供了语音识别的更一般概述。[2]对手势识别环境中HMM的几种扩展进行了综述。参考文献[5]分析了搜索引擎和PageRank算法。[1，9]描述了HMM在手势识别中的应用。[3]中提供了HMM的开放软件。

#### 5.6 练习

1. 对于马尔可夫链天气模型：（i）确定状态序列的概率：多云、下雨、晴天、晴天、晴朗、多云、下雨。（ii）连续四天下雨的概率是多少？（iii）预计持续降雨的天数是多少？
2. 对于图5.6的小网页示例，确定：（i）是否满足收敛条件，如果满足，（ii）向用户呈现三个网页的顺序。
3. 以不公平的硬币为例。给定表5.5中的参数，使用（i）直接方法，（ii）正向算法，获得以下观测序列的概率：HHTT。
4. 对于上一个问题，两种方法中的每种方法的操作数是多少？
5. 对于问题3，使用维特比算法获得最可能的状态序列。
6. 标准HMM中的三个基本假设是什么？用数学表达。
7. 假设有两个HMM表示两个音素：ph1和ph2。每个模型都有两个状态和两个观测值，参数如下：模型1：∏=[0.5，0.5]，A=[0.5、0.5|0.5、0.5]，B=[0.8、0.2|0.2、0.8]模型2：∏=[0.5，0.5]，A=[0.5、0.5、0.5]，B=[0.2、0.8|0.8、0.2]给定以下观测序列：“o1、o1、o2、o2”，哪一个是最可能的音素？
8. 我们想开发一个头部手势识别系统，我们有一个视觉系统，可以检测头部的以下运动：（1）向上，（2）向下，（3）向左，（4）向右，（5）稳定。视觉系统每秒为每种类型的运动提供一个数字（1–5），这是对年龄识别系统的输入（观察）。手势识别系统应识别四类头部手势：（a）肯定动作，（b）否定动作，（c）右转，（d）左转。（i） 指定适合此识别问题的模型，包括结构和所需参数。（ii）指出哪些算法适合于学习模型参数和识别。
9. \*\*\*制定一个解决前一个问题的程序。
10. \*\*\*HMM的一个开放问题是为每个模型建立最佳状态数。开发一种搜索策略，以确定头部姿势识别系统的每个模型的最佳状态数，从而最大化识别率。考虑将数据集（每类数据的示例）分为三组：（a）用于估计模型参数的训练，（b）用于比较不同模型的验证，（c）用于测试最终模型的测试。

#### References

1. Aviles, H., Sucar, L.E., Mendoza C.E.: Visual recognition of similar gestures【类似手势的视觉识别】. In: 18th International Conference on Pattern Recognition, pp. 1100–1103 (2006)
2. Aviles, H., Sucar, L.E., Mendoza, C.E., Pineda, L.A.: A Comparison of dynamic naive Bayesian classifiers and hidden Markov models for gesture recognition【用于手势识别的动态朴素贝叶斯分类器和隐马尔可夫模型的比较】. J. Appl. Res. Technol. 9(1), 81–102 (2011)
3. Kanungo, T.: UMDHMM: Hidden Markov Model Toolkit【UMDHMM：隐马尔可夫模型工具包】. In: Kornai, A. (ed.) Extended Finite State Models of Language. Cambridge University Press (1999). http://www.kanungo.com/software/software.html
4. Kemeny, J.K., Snell, L.: Finite Markov Chains【有限马尔可夫链】. Van Nostrand, Princeton (1965)
5. Langville, N., Carl, D., Meyer, C.D.: Google’s PageRank and Beyond: The Science of Search Engine Rankings【谷歌的PageRank及其后：搜索引擎排名科学】. Princeton University Press, Princeton (2012)
6. Page, L., Brin, S., Motwani, R., Winograd, T.: The PageRank Citation Ranking: Bringing Order to the Web【PageRank引文排名：为网络带来秩序】. Stanford Digital Libraries Working Paper (1998)
7. Rabiner, L.E.: A tutorial on hidden Markov models and selected applications in speech recognition【关于隐马尔可夫模型和语音识别中选定应用的教程】. In: Waibel, A., Lee, K. (eds.) Readings in Speech Recognition, pp. 267–296. Morgan Kaufmann, San Francisco (1990)
8. Rabiner, L., Juang, B.H.: Fundamentals on Speech Recognition【语音识别基础】. Prentice-Hall Signal Processing Series, New Jersey (1993)
9. Wilson, A., Bobick, A.: Using hidden Markov models to model and recognize gesture under variation【使用隐马尔可夫模型来模拟和识别变化下的手势】. Int. J. Pattern Recognit. Artif. Intell., Spec. Issue Hidden Markov Models Comput. Vis. 15(1), 123–160 (2000)

[语音识别(Speech Recognition)概述](https://zhuanlan.zhihu.com/p/380589078)

[hmmlearn 0.2.8.post18+g29072a4 documentation Tutorial](https://hmmlearn.readthedocs.io/en/latest/tutorial.html)

https://github.com/hmmlearn/hmmlearn

[hmmlearn-github-base](https://github.com/hmmlearn/hmmlearn/blob/main/lib/hmmlearn/base.py)

[hmmlearn-GMMHMM](https://hmmlearn.readthedocs.io/en/latest/api.html#gmmhmm)

[How to use the hmmlearn.hmm.GMMHMM function in [hmmlearn](https://snyk.io/advisor/python/hmmlearn)](https://snyk.io/advisor/python/hmmlearn/functions/hmmlearn.hmm.GMMHMM)

[Hidden Markov Model (HMM) Tutorial](http://practicalcryptography.com/miscellaneous/machine-learning/hidden-markov-model-hmm-tutorial/)

[Introduction to Hidden Markov Model](http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-hidden-markov-model/)

[语音识别(二)：HMM&GMM](https://zhuanlan.zhihu.com/p/501923694)

[HMM代码参数](https://blog.csdn.net/m0_67084346/article/details/128145717)

[Markdown 常用数学符号和公式](https://blog.csdn.net/qq_38253837/article/details/109923758)

[最大似然估计和最大后验概率的不同](https://zhuanlan.zhihu.com/p/512661336)

[最大似然估计和最大后验概率估计的区别](https://www.jianshu.com/p/640b35f328e4)

[【语音识别】EM算法和GMM模型](https://blog.csdn.net/nianmaoren2400/article/details/125802222)

[上下文相关的GMM-HMM声学模型续：参数共享](https://www.bbsmax.com/A/kPzOV7Qedx/)

[三音素](https://zhuanlan.zhihu.com/p/335209312)

[kaldi三音素GMM学习笔记](https://zhuanlan.zhihu.com/p/408251495)

[语音识别补充(一)（音素，三音素）](https://blog.csdn.net/baidu_31437863/article/details/81410890)

[#透彻理解# GMM+HMM 语音识别模型 [训练+识别] 过程1](https://zhuanlan.zhihu.com/p/386166391)

[#透彻理解# GMM+HMM 语音识别模型 [训练+识别] 过程2](https://blog.csdn.net/lch551218/article/details/118344589)

[ValueError: startprob_ must sum to 1.0 (got nan) ](https://github.com/hmmlearn/hmmlearn/issues/186)

[[语音识别] HMM理论理解+实战](https://cloud.tencent.com/developer/article/1604927)

[【SLP·Python】基于 DTW GMM HMM 三种方法实现的语音分类识别系统](https://blog.csdn.net/Sherry_ling/article/details/118713802)

[[Sherry-XLL](https://github.com/Sherry-XLL)/**[Digital-Recognition-DTW_HMM_GMM](https://github.com/Sherry-XLL/Digital-Recognition-DTW_HMM_GMM)**](https://github.com/Sherry-XLL/Digital-Recognition-DTW_HMM_GMM/blob/main/gmm.py)

[初识HMM及hmmlearn实现](https://zhuanlan.zhihu.com/p/166552799)

[隐马尔可夫模型：hmmlearn库的使用](https://blog.csdn.net/doswynkfsw/article/details/124356671)
