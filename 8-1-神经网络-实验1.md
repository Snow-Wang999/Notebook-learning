# 6-5-神经网络-视听情感识别-实验一

## 6个实验

##### Григорьев Дмитрий Сергеевич

课程链接：

https://us04web.zoom.us/j/2818939341?pwd=cWJITmVWQUVRWThiMS9qVGR3QkVHZz09#success

trygx@tpu.ru

в среду будет консультация, это последний день когда я принимаю задолженности в этом семестре.
https://github.com/IlyaKalinovskiy/NeuralNetworks
консультация среда 18.25

## 课程大作业

Применить GAN к генерации изображений
курсовой проект сдаете Илье Андреевичу, лектору

kua21@tpu.ru

https://us05web.zoom.us/j/4498886372?pwd=cXVDc1lzMms0dnQzeW5ya0ZjNUV1QT09#success

## 大作业题目

1.使用SNA和LSTM网络的组合来理解文本。

2.应用GAN进行绘画。

3.使用生成模型创建假面孔。

4.将GAN应用于音频生成。

5.使用GAN/VAE模拟人类语言。

6.应用粒子群算法（PSO）优化神经网络的结构和耦合权重。

## 实验：

文件地址：D:\00研二上\神经网络\实验\NeuralNetworks-main\NeuralNetworks-main

## 视听情感识别

[情感识别导论](https://habr.com/ru/company/speechpro/blog/418151/)

情感识别是人工智能领域的热门话题。这些技术最有趣的应用领域包括驾驶员状态识别、市场研究、智能城市视频分析系统、人机交互、在线课程监控、可穿戴设备等。

今年，千年发展目标将这一主题作为其暑期机器学习学校（ [летнюю школу по машинному обучению](https://mlschool.speechpro.ru/).）的主题。在这篇文章中，我将尝试简要地介绍一个人的情绪状态识别问题，并介绍解决这一问题的方法。

![image-20230116161835869](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116161835869.png)

### 什么是情感？ 

情感是一种特殊的心理过程，它表达了一个人对周围世界和自己关系的体验。根据俄罗斯生理学家P.K.Anokhin提出的一种理论，体验情感的能力是在进化过程中发展出来的，作为生物更好地适应生存条件的手段。这种情绪被证明有助于生存，并使生物能够快速而经济地应对外部影响。 

情感在人的生活和人际交往中起着巨大的作用。它们可以用不同的方式表达：**面部表情、姿势、运动反应、声音和植物反应（心率、血压、呼吸频率）**。人的脸最有表现力。 

每个人表达的情感都有点不同。美国心理学家保罗·埃克曼（Paul Ekman）在研究上世纪70年代巴布亚新几内亚孤立部落的非言语行为时发现，愤怒、恐惧、悲伤、厌恶、蔑视、惊讶和快乐是普遍的，无论文化如何，都可以被人理解。 

人们可以表达各种各样的情感。人们认为，它们可以被描述为基本情感的结合（例如，怀旧是悲伤和快乐之间的中间体）。

但这种分类方法并不总是方便的，因为它不允许量化情绪的力量。因此，除了离散的情感模型外，还开发了一些**连续的情感模型**。在模型J。罗素有一个二维的基础，其中每种情绪都以**符号（valence）**【正面和负面情绪】和**强度（arousal）**【激烈程度】为特征。由于其简单性，罗素模型最近在自动面部表情分类的背景下越来越流行。

![image-20230116162106664](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116162106664.png)

所以，我们发现，如果你不试图掩盖情绪激动，那么你目前的状态可以通过面部表情来评估。此外，利用深度学习领域的最新进展，甚至可以根据保罗·埃克曼（Paul Eckman）的作品《对我撒谎》（Lie to Me）构建一个测谎仪。然而，这项任务远非如此简单。神经生物学家丽莎·费尔德曼·巴雷特（Lisa Feldman Barrett）的研究（[研究](http://people.ict.usc.edu/~gratch/CSCI534/Readings/Barrett-context.pdf)）表明，在识别情绪时，一个人会积极地使用上下文信息：**声音、动作、情境。**看看下面的照片，确实如此。只使用面部区域，无法进行正确的预测。因此，为了解决这一问题，必须使用额外的模式和信号随时间变化的信息。

![image-20230116162240180](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116162240180.png)

在这里，我们将考虑仅分析**音频和视频**两种模式的方法，因为这些信号可以通过非接触方式获得。为了解决这个问题，你首先需要获得数据。这是我所知道的最大的公共情感数据库列表。这些数据库中的图像和视频都是手动标记的，有些使用Amazon Mechanical Turk。

| 标题                                                         | 数据      | 标记                      | 年份 |
| ------------------------------------------------------------ | --------- | ------------------------- | ---- |
| [OMG-Emotion challenge](https://www2.informatik.uni-hamburg.de/wtm/OMG-EmotionChallenge/) | 音频/视频 | 7个类别，valence/arousal  | 2018 |
| [EmotiW challenge](https://sites.google.com/view/emotiw2018) | 音频/视频 | 6 个类别                  | 2018 |
| [AffectNet](http://mohammadmahoor.com/affectnet/)            | 图像      | 7 个类别, valence/arousal | 2017 |
| [AFEW-VA](https://ibug.doc.ic.ac.uk/resources/afew-va-database/) | 视频      | valence/arousal           | 2017 |
| [EmotioNet challenge](http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/) | 图像      | 16 个类别                 | 2017 |
| [EmoReact](https://www.behnaznojavan.com/emoreact)           | 音频/视频 | 17 个类别                 | 2016 |

### 情感分类问题的经典方法 

识别面部表情情感的最简单方法是使用PDM、CML、AAM、DPM或CNN（ [PDM](http://www.menpo.org/menpofit/pdm.html), [CML](https://github.com/TadasBaltrusaitis/OpenFace), [AAM](http://www.menpo.org/menpofit/aam.html), [DPM](http://cmp.felk.cvut.cz/~uricamic/clandmark/index.php?page=info) или [CNN](https://github.com/1adrianb/2D-and-3D-face-alignment).）算法对关键点进行分类。通常在眉毛、眼睛、嘴唇、鼻子和下颚上标记5到68个点，以便部分捕捉面部表情。标准化点坐标可以直接提交到分类器（例如SVM或Random Forest）中，并获得基本解决方案。当然，个人的地位必须保持平衡。

![image-20230116162726380](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116162726380.png)

简单地使用坐标而不使用可视化组件会导致大量有用信息的丢失，因此计算不同的描述符来改进这些点的系统：LBP、HOG、SIFT、LATCH（[LBP](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_local_binary_pattern.html), [HOG](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html), [SIFT](https://ianlondon.github.io/blog/how-to-sift-opencv/), [LATCH](https://gilscvblog.com/2015/11/07/performance-evaluation-of-binary-descriptor-introducing-the-latch-descriptor/) ）等。在用PCA匹配描述符和缩小维度后，可以使用获得的特征向量来对情绪进行分类。

![image-20230116164604569](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116164604569.png)

[引用文章](http://www.mdpi.com/1424-8220/18/2/401/pdf)

然而，这种方法已经被认为是过时的，因为深度卷积网络是分析视觉数据的最佳选择。

### 深度学习（Deep Learning）

要构建一个神经网络分类器，只需使用**ImageNet预训练的基本架构**，并重新训练最后几层。这可以为各种数据的分类提供一个很好的基本解决方案，但考虑到该问题的特殊性，用于大规模面部识别（ [распознавания лиц](https://arxiv.org/abs/1711.04598).）任务的神经网络将更为合适。 

因此，建立一个情绪分类器的个人图像是相当容易的，但我们发现，即时图像并不完全反映真实的情绪，人在这种情况下经历。因此，必须分析人员的序列，以提高系统的准确性。有两种方法可以做到这一点。**第一种方法**是将CNN对每个帧进行分类的高级特征发送到递归网络（例如LSTM）以捕获时间分量。

![image-20230116164734716](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116164734716.png)

[引用文章](https://dl.acm.org/citation.cfm?id=3143012)

**第二种方法**是直接向3D-CNN输入从视频中提取的帧序列。类似的CNN使用三自由度卷积，将四维输入转换为三维特征图。

![image-20230116164830919](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116164830919.png)

[引用文章](http://www.diva-portal.org/smash/get/diva2:1174434/FULLTEXT01.pdf)

事实上，在一般情况下，这两种方法可以结合起来，建造这样一个怪物。

![image-20230116164937011](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116164937011.png)

[引用文章](https://arxiv.org/pdf/1705.07871.pdf)

### 情感的言语分类

基于视觉数据，可以非常准确地预测情绪信号，但在确定**强度**时，最好使用**语音信号**（[речевые сигналы](https://arxiv.org/pdf/1704.08619.pdf)）。分析音频有点困难，因为语音长度和口述人的声音有很大的变化。它通常不使用原始声波，而是使用各种特征集（[признаков](http://cslt.riit.tsinghua.edu.cn/~fzheng/PAPERS/2018/1805E_ACII-Asia_Emotion-Recog-Channenge_ZXT.pdf)），如F0、MFCC、LPC、I向量等。OpenSmile( [OpenSMILE](https://audeering.com/technology/opensmile/))是一个开放的开源库，它包含了一系列用于分析语音和音乐信号的算法。提取后，特征可以提交到SVM或LSTM进行分类。

然而，卷积神经网络最近开始渗透到声音分析领域，取代了传统的方法。为了应用它们，**声音在线性或MEL刻度上以光谱仪的形式表示**，然后将获得的光谱仪与普通的二维图像一起操作。在这种情况下，任意大小的光谱仪在时间轴上的问题可以通过统计Pulling([статистического пулинга](https://arxiv.org/pdf/1803.10963.pdf))或通过将递归网络集成到体系结构中来优雅地解决。

![image-20230116165056622](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116165056622.png)

[引用文章](https://arxiv.org/ftp/arxiv/papers/1707/1707.09917.pdf)

### 视听情感识别

因此，我们考虑了几种分析音频和视频模式的方法，剩下的是最后一个阶段——将分类器组合起来，得出最终的解决方案。最简单的方法是直接合并它们的估计数。在这种情况下，只需取最大值或平均值。一个更复杂的选择是在每个模式的Embadding级别合并。SVM通常用于此目的，但这并不总是正确的，因为Embadding可能有不同的规范。因此，开发了更先进的算法，如Multiple Kernel Learning( [Multiple Kernel Learning](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7837868))和ModDrop([ModDrop](https://arxiv.org/pdf/1501.00102.pdf))。

当然，值得一提的是一类所谓的端到端（ [end-to-end](https://arxiv.org/pdf/1704.08619.pdf)）解决方案，可以直接从多个传感器的原始数据中学习，而无需任何预处理。

总体而言，自动识别情绪的任务还远未实现。根据去年的“野生情感识别”比赛，最佳解决方案（[最佳解决方案](https://drive.google.com/file/d/1-mVVbabm8ePTMJKwO0itdMXB3j5vEw7h/view)）的准确率约为60%。我希望本文中提供的信息足以帮助我们建立自己的情感识别系统。

标签：情感识别机器学习Deep Learning 

枢纽：公司博客语音技术中心（MDG）算法机器学习

## 实验准备

### 设置您的环境

以下命令创建虚拟 Python 环境并安装必要的软件包：

Terminal(Pycharm快捷键alt+F12)

```
conda create --name py38 python=3.8 anaconda#  创建指定Python版本的虚拟环境。
conda activate py38# 激活进入创建的虚拟环境
# 下载librosa，opencv-python,tqdm
pip install  -i https://pypi.tuna.tsinghua.edu.cn/simple librosa 
pip install  -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python 
pip install tqdm
```

补充：

```
conda create --name py38 python=3.8 anaconda #  创建指定Python版本的虚拟环境。
## 上面命令意思是创建名称为py38,解释器为python3.8版本的虚拟环境，Conda会自动取下载对应版本的Python解释器，且下载的解释器为当前版本的最高版本。
## 查看所有已创建的虚拟环境使用conda info -e，也可以使用conda env list。

source activate py38 # 激活进入创建的虚拟环境
pip install librosa opencv-python tqdm
# 下载librosa，opencv-python,tqdm
pip install opencv-python
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python
```

*安装* *PyTorch* 的说明  *PyTorch* : https**://**pytorch.org/

![image-20230117123821720](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230117123821720.png)

[利用OpenCV-Python实现视频拆帧（组帧），也可以用于组合实现视频格式的转换](https://blog.csdn.net/qq_50838982/article/details/126075584)

### 虚拟环境

[python interpreter配置conda_在pycharm中配置Anaconda以及pip源配置详解](https://blog.csdn.net/weixin_39900286/article/details/110973422)

[LInux下部署基于Conda的Python虚拟环境](https://blog.csdn.net/m0_46570759/article/details/123627542)

[PyCharm专业版最简单的试用续期教程，永久适用](https://zhuanlan.zhihu.com/p/567219075)

[手把手教你在Pycharm中新建虚拟环境并使用(超详细!)](https://www.jb51.net/article/252828.htm)

conda创建虚拟环境时报错CondaSSLError是因为翻墙了。

虚拟环境的创建是基于版本控制。不同的项目基于不同的第三方库以及指定版本的编译器。如果没有虚拟环境，则兼容性的复杂程度将会成几何形成长。而虚拟环境就是创建了一个专属空间，当前项目的所有依赖，都从这个**专属空间**里应用，即专属的第三方库，专属的指定版本。

推荐使用pipenv来作为第三方库及版本控制工具。

powershell不显示虚拟环境名称。

```
conda init powershell
```

[在 Windows powershell 无法使用 conda 虚拟环境的问题](https://blog.csdn.net/a277265432/article/details/125848017)

查看已有虚拟环境名称

```
conda info --envs
```

Please update conda by running

```
conda update -n base -c defaults conda
```

```
conda upgrade pip
```

### 第三方库

[windows下pip install的warning(timeout)问题的两种解决方案](https://blog.csdn.net/weixin_46203060/article/details/112856827)

换源

下面是一部分可用源（来源于方法一参考博客）
阿里云 http://mirrors.aliyun.com/pypi/simple/
豆瓣(douban) http://pypi.douban.com/simple/
清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/
中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/

[pip换源](https://zhuanlan.zhihu.com/p/551940762)

### 1.临时换源：

```text
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name
```

### 2.永久还原：

```text
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
```

#### 方法一，pip命令补充源地址

可以直接执行

```
pip install k -i http://pypi.douban.com/simple --trusted-host pypi.douban.com（k替换为模块名称即可，采用的源为豆瓣）或

pip install k -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com（用其他源安装依旧可行，此源为阿里云）
```


这里测试了参考网站提供的中科大和清华大学的源，无论–turst-host后的内容如何修改，均无法成功执行pip install命令。

参考
https://blog.csdn.net/lsf_007/article/details/87931823?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1.control

### 数据

提供了三组数据来运行作业：

| 名字 | RAVDESS                                                      | AFEW-VA         | OMG-Emotion challenge                                        |
| :--- | ------------------------------------------------------------ | --------------- | ------------------------------------------------------------ |
| 数据 | 音频/视频                                                    | 视频            | 音频/视频                                                    |
| 标记 | 8 个类别:  <br />1 – neutral (исключен)  <br />2 - calm  <br />3 - happy  <br />4 - sad  <br />5 - angry  <br />6 - fearful  <br />7 - disgust<br />8 - surprised | valence/arousal | valence/arousal  <br />7 个类别:  <br />0 - anger  <br />1 - disgust  <br />2 - fear  <br />3 - happy  <br />4 - neutral  <br />5 - sad  <br />6 - surprise |
| 年   | 2018                                                         | 2017            | 2018                                                         |

对于每个数据库，都有用于训练和验证的文件列表。 学生进行的项目的最终评估将在OMG-Emotion挑战基地的测试子集上进行。

### 数据格式说明

在所有作业中，数据都是使用 AVDBParser 类加载的。

它具有以下输入变量和选项：

| 输入变量                     | 解释                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| dataset_root                 | 包含基目录的根目录的路径                                     |
| file_list                    | 下载文件列表的路径                                           |
| max_num_clips （默认值：0）  | 要上传的最大剪辑数。（0 – 从所有文件夹下载文件）。 *在调试阶段使用不同的值来加快加载速度。* |
| max_num_samples（默认值：0） | 每个剪辑要上传的最大帧数                                     |
| ungroup  （默认： 假）       | 如果 为 true，则帧将不会按输出列表中的剪辑分组               |
| load_image（默认值： 假）    | 如果为 true，则除了元数据之外，还将加载图像本身。            |
| nOrmalize （默认值： 假）    | 如果为 true，则价/唤醒值将被归一化                           |

该类返回类型为 DataSample 或 DataGroup 的项的列表，具体取决于取消分组标志。

数据样本结构

| 数据结构     | 解释                                            |
| ------------ | ----------------------------------------------- |
| img_rel_path | 图像路径                                        |
| wav_rel_path | 音频文件路径                                    |
| labels       | 类标签（如果有）                                |
| text_ labels | 情绪名称 （如有）                               |
| valence      | 情绪符号（valence）【正面和负面情绪】（如果有） |
| arousal      | 情绪强度（arousal）【情绪的激烈程度】（如果有） |
| landmarks    | [x， y] 格式的像素坐标列表                      |
| image        | BGR 格式图像的像素强度数组                      |

数据组结构包含与 DataSample 类似的字段，但 data_samples 字段除外，该字段包含给定剪辑的所有帧的列表 （DataSample）。 在这种情况下，在所有帧上平均化情绪符号（valence）和情绪强度（arousal），并将标签（labels）作为中位数。

**任务** **1.** **多维数据可视化**

在开始分析多维数据之前，可视化以评估其内部结构的复杂性非常有用，因为它可以很容易地线性分离。 为了在2D或3D空间中显示多维数据，经常使用经典的PCA，PLS或LDA算法，还有一类非线性降维算法  ，  例如t-SNE（t分布随机邻居嵌入）基于最小化库尔巴克-莱布勒距离。

在此任务中，建议**使用pytorch DL框架实现   t-SNE算法**。 作为可视化数据，使用**68个面部特征点**，为每帧计算。 点的位置如下图所示。

![image-20230116202129846](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116202129846.png)

有必要发明基于地标的特征，以增加数据中的类间距离。请注意，这些数据是视频帧序列，在为单个剪辑构建特征向量时可以考虑这些数据。

比较T-SNE和PCA算法。
T-SNE算法描述：https://habr.com/post/267041/
任务项目：Data_Visualization
需要更改标记为todo的行中的tsne.py、wrapper.py和dataviz.py文件。

RAVDESS 数据库的厌恶和惊讶类的投影示例。

![image-20230116202356533](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230116202356533.png)

## t-sne

在《R上的深度学习》一文中，我曾多次提到T-SNE，这是一种神秘的非线性维数衰减和多维变量可视化技术（例如这里），我对此很感兴趣，决定弄清楚细节。T-SNE是T-distributed stochastic neighbor embedding。俄罗斯版本的“引入邻居”在某种程度上听起来很荒谬，所以我将继续使用英文首字母缩略语。

T-SNE算法也被称为多重特征学习技术，由荷兰研究人员Lawrence van der Maaten（现为Facebook AI Research工作）和神经网络魔术师Jeffrey Hinton于2008年发表（本文末尾引用1）。经典的SNE是由Hinton和Rowice在2002年提出的（参考文献2）。2008年的一篇文章描述了一些“技巧”，简化了全球最低值的搜索，并提高了可视化质量。其中之一是用低维数据的学生分布代替正态分布。此外，还成功地实现了算法（本文引用了MATLAB），然后将其移植到其他流行的环境中。

### 一点数学

让我们从经典的SNE开始，并制定一个问题。我们有一组数据，其中点由空间维数明显大于3的多维变量描述。需要获得二维或三维空间中存在的新变量，以最大限度地保持原始数据的结构和模式。

多维欧几里德距离：[【机器学习实战】计算两个矩阵的成对距离（pair-wise distances）](https://www.cnblogs.com/wuliytTaotao/p/12024380.html)

SNE首先将点之间的**多维欧几里德距离**转换为反映点相似性的**条件概率**。

<img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230117165733141.png" alt="image-20230117165733141" style="zoom:30%;" />

![image-20230117165821903](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230117165821903.png)

让我们尝试将这些概念与基础理论联系起来。在数学上，我们将**正态分布**的方程写为如下形式：
$$
P(x)={1\over \sigma \sqrt{2\pi}}{e^{-(x-\mu)^2/(2\sigma^2)}}
$$
这在数学上是这样的（公式1）：
$$
P_{j|i}={{\exp(-{||x_i-x_j||}^2/2\sigma_i^2)}\over {\sum_{k \neq i}{\exp(-{||x_i-x_k||}^2/2\sigma_i^2)}}} \tag{1}
$$
这个公式显示了当高斯分布在$x_i$周围时，给定的偏差为$σ$时，点$x_j$与点$x_i$的接近程度。偏差$σ$对每个点都不同。它的选择是为了使密度较大的区域中的点具有较小的方差。为此，使用**Perplexia评估**：
$$
Perp(P_i)=2^{H(P_i)}
$$
其中 $H(P_i)$ 是混合（或位形）香农熵（信息熵是香农提出的）（公式2）：
$$
H(P_i)=-\sum_j{p_{j|i}\log_2(p_{j|i})} \tag{2}
$$

> 香农熵：随机变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大。
>
> 混合（或位形）熵，CE, configurational entropy，在统计力学中，位形熵是一种与构成粒子的位置有关的熵。

> 熵这一名称并不是香农首先提出的。最先提出熵这一名称的是物理学家，他提出的熵称其为热熵，它是热力学系统的一个状态函数，热熵是物理系统无序性的量度，热熵越大，表明物理系统可能的微观状态数也就越多，从微观上看，系统就越变化多端，越没有秩序。

在这种情况下，perplexia可以解释为**点$x_i$的有效邻域数的平滑估计**。它指定为方法参数。作者建议使用5到50之间的值。使用二进制搜索算法( [алгоритма бинарного поиска](https://en.wikipedia.org/wiki/Binary_search_algorithm))为每个$X_i$和$X_j$对定义$σ$。

接下来，我们考虑**降维**到低维空间的情形。首先，我们创建一个`n_samples x ` `n_components的`矩阵(在这种情况下为9×1)并用随机值(即位置)填充。

<img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230117184554399.png" alt="image-20230117184554399"  />

对于二维或三维“同事”对$X_i$和$X_j$，为了清晰起见，我们称它们为$Y_i$和$Y_j$，使用相同的公式1估计条件概率并不困难。建议将标准差设置为$1/√2$：
$$
q_{j|i}={{\exp(-{||y_i-y_j||}^2)}\over {\sum_{k \neq i}{\exp(-{||y_i-y_k||}^2)}}} 
$$
请注意，就像以前一样，我们采用正态分布方程式，将所有内容放在前面，使用其他点代替均值，然后通过除以所有其他点的似然之和来解决尺度问题(这里忽略了标准差)。

**如果我们能使降维后特征空间中的点的概率分布近似于原始特征空间中的点的概率分布，则可以得到定义良好的聚类**。为此，我们使用了称为Kullback-Leiber散度。

![image-20230117185239969](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230117185239969.png)

如果映射点$Y_i$和$Y_j$正确地模拟了高维$X_i$和$X_j$的原始点之间的相似性，则相应的条件概率$p(j\ |\ i)$和$q(j\ |\ i)$是等价的。Kulback-Leibler距离([расстояние Кульбака-Лейблера](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence))或散度被用作$q(j\ |\ i)$反映$p(j\ |\ i)$的明显质量估计。

> Kullback-Leiber散度
>
> KL散度是一个概率分布与另一个概率分布之间差异的度量。
>
> ![image-20230117185353434](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230117185353434.png)
>
> KL散度的值越小，两个分布之间的距离越近。 KL散度为0表示所讨论的两个分布是相同的。

SNE通过梯度下降最小化所有显示点的距离和，即使用梯度下降法将所有数据点上Kullback-Leiber散度的总和最小化。这种方法的损失函数将由公式3确定：
$$
Cost = \sum_i{KL(P_i\ ||\ Q_i)}=\sum_i{\sum_j{p_{j|i}\log{p_{j|i}\over q_{j|i}}}} \tag{3}
$$
![image-20230117185621173](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230117185621173.png)

这个梯度看起来非常简单：
$$
\frac{\part Cost}{\part y_i}=2\sum_j{(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})(y_i-y_j)}
$$
作者为优化过程提供了以下物理类比：所有映射点都由弹簧连接。连接点 $i$ 和 $j$ 的弹簧的刚性取决于**多维空间**中两个点的相似性和**映射空间**中两个点的相似性之间的**差异**。在这个类比中，梯度是作用于映射空间中一个点的结果力。如果一个系统“放手”，过了一段时间，它就会达到平衡，这就是它想要的分布。从算法上讲，建议考虑以下几点来寻找平衡：
$$
Y^{(t)}=Y^{(t-1)}+\eta{\frac{\part Cost}{\part Y}}+{\alpha(t)}(Y^{(t-1)}-Y^{(t-2)})
$$
其中 $η$ 是决定学习速度（步长）的参数，$α$ 是惯性系数。使用经典的SNE可以得到很好的结果，但它可能与优化损失函数和拥挤问题有关。如果T-SNE不完全解决这些问题，它将大大缓解问题。T-SNE损失函数有两个主要区别。首先，T-SNE在多维空间中具有对称相似性，并且具有**更简单的梯度变体**。其次，**T-分布（Student）代替了高斯分布**，T-分布的“重”尾巴简化了优化，解决了拥挤问题。

作为最小化条件概率$p_{i|j}$和$q_{i|j}$之间的Kulback-Leibler发散和的替代方案，建议**最小化多维空间中的联合概率 $P$ 和映射空间中的联合概率 $Q$ 之间的单发散**：
$$
Cost = \sum_i{KL(P\ ||\ Q)}=\sum_i{\sum_j{p_{ij}\log{p_{ij}\over q_{ij}}}} 
$$
其中$p_{ii}=0,q_{ii}=0$，$p_{ij}=p_{ji}$，$q_{ij}=q_{ji}$表示任何$i$和$j$，$p_{ij}$定义为：
$$
p_{ij}={{p_{j|i}+p_{i|j}}\over 2}
$$
其中$n$是数据集中的点数。对称SNE的梯度比经典SNE的梯度容易得多：
$$
\frac{\part Cost}{\part y_i}=4\sum_j{(p_{ij}-q_{ij})(y_i-y_j)}
$$
拥挤问题是映射空间中对应于多维空间中两个平均距离点的两点之间的距离必须远远大于高斯分布所允许的距离。学生的尾巴解决了问题。T-SNE使用单自由度T-分布。在这种情况下，映射空间的共同概率由公式4定义：
$$
q_{ij}={{(1+{||y_i-y_j||}^2)}^{-1}\over {\sum_{k \neq l}{(1+{||y_k-y_l||}^2)}^{-1}}} \tag{4}
$$
相应的梯度是表达式5：
$$
\frac{\part Cost}{\part y_i}=4\sum_j{(p_{ij}-q_{ij})(y_i-y_j){(1+{||y_i-y_j||}^2)}^{-1}} \tag{5}
$$
回到物理类比，由公式5定义的结果力将基本上收缩空间的映射点到多维空间的附近点，并将其排斥到远程点。

### 算法 

简化的T-SNE算法可以用以下伪码表示：

> - 数据：数据集 $X=\{x_1,x_2,…,x_n\}$，
> - 损失函数参数：PERP perplexia，
> - 优化参数：迭代次数 $t$，学习速度 $η$，矩 $α(t)$。
> - 结果：数据表示 $Y(t)=\{y_1,y_2,…,y_n\}$（2d或3D）。
>
> begin
> 	计算PERP perplexia的 p(j|i) 的成对相似性（使用公式1）
> 	设置 $p_{ij}={{p_{j|i}+p_{i|j}}\over 2}$
> 	初始化 $Y(0)=\{y_1,y_2,…,y_n\}$ 的正态分布点（$mean=0,sd=1e-4$）
> 	for t = 1 to T do
>     	计算 $q_{ij}$ 映射空间中点的相似性（公式4）
>     	计算梯度 $\frac{\part Cost}{\part Y}$（公式5）
>     	设置 $Y^{(t)}=Y^{(t-1)}+\eta{\frac{\part Cost}{\part Y}}+{\alpha(t)}(Y^{(t-1)}-Y^{(t-2)})$
> 	end
> end

为了提高效果，建议使用两种技巧。第一个作者称之为**“早期压缩”**。它的任务是在优化开始时使映射空间中的点尽可能靠近彼此。当映射点之间的距离很小时，将一个集群移动到另一个集群要容易得多。因此，探索优化空间并“瞄准”全球最低值要容易得多。早期压缩是通过在损失函数中添加一个**L2惩罚**来创建的，该惩罚与映射点从原点到原点距离的平方之和成比例（在源代码中找不到）。

第二个不太明显的把戏是**“早期过度放大”**（Early Exaggeration）。它是在优化开始时将所有$P_{ij}$乘以一个整数，例如4。这意味着，对于较大的$P_{ij}$，可以获得较大的$Q_{ij}$。这将允许原始数据中的群集在映射空间中获得密集且分散的群集。

### 代码

k个样本

```python
import numpy as np
matrix = np.random.randint(0,10,(7,10))
print(matrix)
print(matrix.shape[0])#样本数k
```

```
[[4 8 3 6 0 1 6 4 2 5]
 [3 7 0 3 7 2 4 0 7 7]
 [1 2 9 2 3 5 6 3 7 0]
 [7 8 4 7 6 5 1 1 3 5]
 [0 7 3 0 6 4 6 1 2 0]
 [0 4 7 4 2 3 7 3 6 9]
 [1 2 4 6 9 4 5 5 9 0]]
7
```

```python
from sklearn.metrics.pairwise import pairwise_distances
distances2 = pairwise_distances(matrix, metric="euclidean", squared=True)
print(distances2)
```

返回k*k的矩阵

```
[[  0. 119. 173.  98. 132.  94. 212.]
 [119.   0. 198.  89. 111. 117. 141.]
 [173. 198.   0. 201. 105. 101.  87.]
 [ 98.  89. 201.   0. 152. 168. 176.]
 [132. 111. 105. 152.   0. 160. 138.]
 [ 94. 117. 101. 168. 160.   0. 166.]
 [212. 141.  87. 176. 138. 166.   0.]]
```

```python
from sklearn import manifold
pij = manifold._t_sne._joint_probabilities(distances2, 30, False)
print(pij)
```

```
[0.02380952 0.02380952 0.02380952 0.02380952 0.02380952 0.02380952
 0.02380952 0.02380952 0.02380952 0.02380952 0.02380952 0.02380952
 0.02380952 0.02380952 0.02380952 0.02380952 0.02380952 0.02380952
 0.02380952 0.02380952 0.02380952]
```

k个样本，矩阵的行数。

返回一维数据的元素数量：
$$
n=1+2+3+...+(k-1)
$$

## 参考文献：

1. [数据](https://cloud.mail.ru/public/Qu6i/vSMNyyooY)

2. 现有代码

   1. [topicsne](https://github.com/cemoody/topicsne/commit/5318073d12f43b3ec7cf8072e344c420cbc0edb6)
   2. https://github.com/cemoody/topicsne

3. 各种新兴算法代码库

   [HelloGitHub](https://github.com/521xueweihan/HelloGitHub)

4. 实验1算法t-sne的原理

   1. 百度搜索t-SNE
   2. [Препарируем t-SNE](https://habr.com/ru/post/267041/)
   3. [t-SNE的原理及Python实现](https://vimsky.com/article/4400.html)
   4. [mxl1990](https://github.com/mxl1990)/**[tsne-pytorch](https://github.com/mxl1990/tsne-pytorch)**

5. 项目中的函数

   1. [【python】scipy中pdist和squareform](https://blog.csdn.net/qq_20135597/article/details/94212816)

      1. pdist(X, “sqeuclidean”)
      2. [scipy.spatial.distance.squareform的使用](https://blog.csdn.net/qq_41000421/article/details/84581218)
      3. [scipy.spatial.distance.squareform](https://blog.csdn.net/gaoxiaobai666666/article/details/89093975)

   2. [sklearn 下的流行学习（Manifold Learning）—— sklearn.manifold](https://blog.csdn.net/lanchunhui/article/details/52936028)

      ```python
      from sklearn.manifold import TSNE
      ```

      _joint_probabilities输出一维数据

   3. [nn.Embedding参数说明](https://blog.csdn.net/zl1085372438/article/details/109236969)

      1. torch.nn.modules.module.Module
      2. nn.Embedding(n_points, n_dim) 离散点
      3. [pytorch-EMBEDDING](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

   4. [计算两个矩阵的成对距离（pair-wise distance）](https://www.cnblogs.com/wuliytTaotao/p/12024380.html)

      1. [torch.pairwise_distance(): 计算特征图之间的像素级欧氏距离](https://blog.csdn.net/qq_36560894/article/details/112199266)
      2. pairwise_distance

   5. [torch.nn.Module类](https://zhuanlan.zhihu.com/p/388838076)

6. 任务1：实现映射点和Kullback-Leibler距离相似矩阵的计算

