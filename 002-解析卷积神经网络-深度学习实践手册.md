# 002-解析卷积神经网络-深度学习实践手册

腾讯微云打开《解析卷积神经网络-深度学习实践手册》

魏秀参

一、 绪论 11

0.1 引言  12

0.2 什么是深度学习？ 13 

0.3 深度学习的前世今生  14

二、 基础理论篇 19

1. 卷积神经网络基础知识 21

   1.1 发展历程  22

   1.2 基本结构  24

   1.3 前馈运算  26

   1.4 反馈运算  27

   1.5 小结   29

2. 卷积神经网络基本部件 31

   2.1 “端到端”思想  31

   2.2 网络符号定义  33

   2.3 卷积层  34

   ​	2.3.1 什么是卷积？  34

   ​	2.3.2 卷积操作的作用  36

   2.4 汇合层  37

   2.4.1 什么是汇合？38

   2.4.2 汇合操作的作用 39

   2.5 激活函数 40

   2.6 全连接层 42

   2.7 目标函数 43

   2.8 小结 43

3. 卷积神经网络经典结构 44

   3.1 网络结构中的重要概念 44 

   3.1.1 感受野 44  

   3.1.2 分布式表示 46  

   3.1.3 深度特征的层次性 48  

   3.2 经典网络案例分析 49  

   3.2.1 Alex-Net 网络模型 49  

   3.2.2 VGG-Nets 网络模型  53

   3.2.3 Network-In-Network 53

   3.2.4 残差网络模型 54 

   3.3 小结  59

4. 卷积神经网络的压缩 64

   4.1 低秩近似  66

   4.2 剪枝与稀疏约束 67  

   4.3 参数量化  71

   4.4 二值网络  75

   4.5 知识蒸馏  77

   4.6 紧凑的网络结构 79  

   4.7 小结  81

5. 数据扩充 84

   5.1 简单的数据扩充方式 84

   5.2 特殊的数据扩充方式 86

   5.2.1 fancy PCA 86

   5.2.2 监督式数据扩充 86

   5.3 小结 87

6. 数据预处理 89

7. 网络参数初始化 91

   7.1 全零初始化 92

   7.2 随机初始化 92

   7.3 其他初始化方法 95

   7.4 小结 96

8. 激活函数 97

   8.1 sigmoid 型函数 98

   8.2 tanh（x）型函数 99

   8.3 修正线性单元（ReLU）99

   8.4 Leaky ReLU 100

   8.5 参数化 ReLU 100

   8.6 随机化 ReLU 101

   8.7 指数化线性单元（ELU） 103

   8.8 小结 103

9. 目标函数 106

   9.1 分类任务的目标函数 106

   9.1.1 交叉熵损失函数 106

   9.1.2 合页损失函数 107

   9.1.3 坡道损失函数 107

   9.1.4 大间隔交叉熵损失函数 109

   9.1.5 中心损失函数 110

   9.2 回归函数的目标函数 112

   9.2.1 $l_1$ 损失函数 113

   9.2.2 $l_2$ 损失函数 113

   9.2.3 Tukey's biweight 损失函数 113

   9.3 其他任务的目标函数 114

   9.4 小结 116

10. 网络正则化 117

    10.1 $l_2$ 正则化 118

    10.2 $l_1$ 正则化 119

    10.3 最大范数约束 119

    10.4 随机失活 119

    10.5 验证集的使用 120

    10.6 小结 122

11. 超参数设定和网络训练 124

    11.1 网络超参数设定 124

    11.1.1 输入数据像素大小 124

    11.1.2 卷积层参数的设定 125

    11.1.3 汇合层参数的设定 126

    11.2 训练技巧 126

    11.2.1 训练数据随机打乱 126

    11.2.2 学习率的设定 127

    11.2.3 批规范化操作 128

    11.2.4 网络模型优化算法选择 130

    11.2.5 微调神经网络 134

    11.3 小结 135

12. 不平衡样本的处理 137

    12.1 数据层面处理方法 138

    12.1.1 数据重采样 138

    12.1.2 类别平衡采样 139

    12.2 算法层面处理方法 140

    12.2.1 代价敏感方法 140

    12.2.2 代价敏感法中权重的指定方式 141 

    12.3 小结 143

13. 模型集成方法

    13.1 数据层面的集成方法 145

    13.1.1 测试阶段数据扩充 145

    13.1.2 “简易集成”法 145

    13.2 模型层面的集成方法 146

    13.2.1 单模型集成 146

    13.2.2 多模型集成 148

    13.3 小结 150

14. 深度学习开源工具简介 152

    14.1 常用框架对比 152

    14.2 常用框架的各自特点 155

    14.2.1 caffe 155

    14.2.2 deeplearning4j 155

    14.2.3 Keras 156

    14.2.4 MXnet 156

    14.2.5 MatConvNet 157 

    14.2.6 TensorFlow 157

    14.2.7 Theano 157

    14.2.8 Torch 158

15. 附录

    ![image-20221114233130433](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221114233130433.png)



1.1 发展历程

加拿大神经科学家David H. Hubel 和 Torsten Wiesel 于1959年提出猫的初级视皮层中单个神经元的“感受野”（receptive field）概念，紧接着于1962年发现了猫的视觉中枢里存在感受野、双目视觉和其他功能结构，标志着神经网络结构首次在大脑视觉系统中被发现。 

[^感受野的来源]: Lei Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In advances in Neural Information Processing Systems, pages 2654-2662, 2014.

1980年前后，日本科学家福岛邦彦在Hubel 和Wiesel 工作的基础上，模拟生物视觉系统并提出了一种层级化的多层人工神经网络，即“神经认知”（neurocognition），以处理手写字符识别和其他模式识别任务。

[^神经认知]: Kunihiko Fukushima. Neocognition: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernelics, 36:193-202, 1980

神经认知模型在后来也被认为是现今卷积神经网络的前身。在福岛邦彦的神经认知模型中，两种最重要的组成单元是“S型细胞”（S-cells）和“C型细胞”（C-cells），两类细胞交替堆叠在一起构成了神经认知网络。其中，S型细胞用于抽取局部特征（local features），C型细胞则用于抽象和容错，如图1.2所示，不难发现这与现今卷积神经网络中的卷积层（convolution layer）和汇合层（pooling layer）可一一对应。







