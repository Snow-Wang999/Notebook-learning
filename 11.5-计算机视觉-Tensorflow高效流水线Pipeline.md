# 卷积分类器-The Convolutional Classifier

在本课程中，您将： 

- 用Keras与现代深度学习网络创建图像分类器（image classifier）
- 使用可重用的块设计您自己的自定义卷积网络（custom convnet） 
- 了解视觉特征提取（feature extraction）背后的基本思想
- 掌握迁移学习（transfer learning）的艺术来提升你的模型
- 利用数据扩充（data augmentation）来扩展您的数据集

## 介绍

本课程将向您介绍计算机视觉的基本思想。我们的目标是了解**神经网络如何“理解”自然图像**，足以解决人类视觉系统可以解决的同类问题。

最擅长这项任务的神经网络称为卷积神经网络（**convolutional neural networks**）（有时我们说 **convnet**  或 **CNN**。）卷积是一种**数学运算**，它赋予卷积网络的层独特的结构。在以后的课程中，您将了解为什么这种结构在解决计算机视觉问题方面如此有效。

我们将把这些想法应用到**图像分类**问题上：给定一张图片，我们可以训练计算机告诉我们它是什么图片吗？您可能已经看到可以从照片中识别植物种类的应用程序。那是一个图像分类器！在本课程中，您将学习如何构建与专业应用程序中使用的图像分类器一样强大的图像分类器。

虽然我们的重点将放在图像分类上，但您将在本课程中学到的内容与各种计算机视觉问题相关。最后，您将准备好继续使用更高级的应用程序，例如生成对抗网络（generative adversarial networks-GAN）和图像分割（image segmentation）。

## 卷积分类器-The Convolutional Classifier

用于图像分类的卷积网络由两部分组成：

- 卷积基——a convolutional base 
- 密集头——a dense head

![image-20221030144207276](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030144207276.png)

卷积基（a convolutional base）用于**从图像中提取特征**。它主要由执行卷积操作的层组成，但通常也包括其他类型的层。 

密集头（a dense head）用于确定**图像的类别**。它主要由dense层组成，但可能包括其他层，如 dropout。 

我们所说的视觉特征是什么意思？特征可以是线条、颜色、纹理、形状、图案——或一些复杂的组合。 

整个过程是这样的：

![image-20221030144432516](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030144432516.png)

实际提取的特征看起来有点不同，但它给出了这个想法。

## 训练分类器-Training the Classifier

网络在训练期间的目标是学习两件事： 

- 从图像（基础-base）中提取哪些特征【which features to extract from an image (base),】
- 哪个类具有什么特征（头部-head）【which class goes with what features (head)】

如今，很少从头开始训练卷积网络。更常见的是，我们重用预训练模型的基础（**reuse the base of a pretrained model**. ）。然后，我们将一个未经训练的头部连接（**attach an untrained head**）到预训练的基础上。换句话说，我们重用网络中已经学会做的部分 1. 提取特征，并附加一些新的层来学习 2. 分类。

![image-20221030144838780](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030144838780.png)

因为头部通常只包含几个密集（dense）层，所以可以从相对较少的数据中创建非常准确的分类器。

重用预训练模型是一种称为迁移学习的技术。它非常有效，以至于现在几乎每个图像分类器都会使用它。

## Example - Train a Convnet Classifier

在整个课程中，我们将创建试图解决以下问题的分类器：

**这是汽车的图片还是卡车的图片？**

我们的数据集是大约 10,000 张各种汽车的图片，大约一半是汽车，一半是卡车。

### 第一步 - 数据加载

```python
# Imports
import os, warnings
import matplotlib.pyplot as plt
from matplotlib import gridspec
#gridspec.Gridspec(5，5)创建网格子图5*5

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory
#从自己硬盘加载自己的图像数据集

# Reproducability
def set_seed(seed=31415):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
set_seed(31415)

# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('image', cmap='magma')
warnings.filterwarnings("ignore") # to clean up output cells


# Load training and validation sets
ds_train_ = image_dataset_from_directory(
    '../input/car-or-truck/train',
    labels='inferred',
    label_mode='binary',
    image_size=[128, 128],
    interpolation='nearest',
    batch_size=64,
    shuffle=True,
)
ds_valid_ = image_dataset_from_directory(
    '../input/car-or-truck/valid',
    labels='inferred',
    label_mode='binary',
    image_size=[128, 128],
    interpolation='nearest',
    batch_size=64,
    shuffle=False,
)

# Data Pipeline
# 数据类型转换为浮点数
def convert_to_float(image, label):
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return image, label

AUTOTUNE = tf.data.experimental.AUTOTUNE
ds_train = (
    ds_train_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
ds_valid = (
    ds_valid_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
```

```python
import matplotlib.pyplot as plt
```

***

#### TF-高效流水线优化pipeline

```python
AUTOTUNE = tf.data.experimental.AUTOTUNE
ds_train = (
    ds_train_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
```

##### 1. 介绍-Tensorflow高效流水线Pipeline

[**Tensorflow高效流水线Pipeline**](https://www.cnblogs.com/huangyc/p/10340766.html)

GPU和TPU可以显著缩短执行单个训练步所需的时间。实现最高性能需要高效的输入流水线，以在当前时间步完成之前为下一步提供数据。

tf.data API可以帮助我们构建灵活高效的输入流水线。

##### 2. Pipeline Structure输入流水线结构

我们可以将典型的 TensorFlow 训练输入流水线视为 ETL 流程： 

- Extract：从永久性存储（可以是 HDD 或 SSD 等本地存储或 GCS 或 HDFS 等远程存储）**读取数据**。

- Transform：使用CPU核心解析数据并对其执行**预处理操作**，例如图像解压缩、数据增强转换（例如随机裁剪、翻转和颜色失真）、重排和批处理、数据类型转换。 

- Load：将转换后的数据**加载到执行机器学习模型的加速器设备**（例如，GPU 或 TPU）上。

这种模式可高效利用 CPU，同时预留加速器来完成对模型进行训练的繁重工作。此外，将输入流水线视为 ETL 流程可提供便于应用性能优化的结构。

###### 2.1 最佳Pipeline步骤

- 使用 <u>prefetch 转换可将提供方和使用方的工作重叠</u>。我们特别建议将 `prefetch(n)`（其中 n 是单步训练使用的元素数/批次数）添加到输入流水线的末尾，以便将在 CPU 上执行的转换与在加速器上执行的训练重叠。 
- <u>通过设置 `num_parallel_calls` 参数并行处理 map 转换</u>。建议您将其值设为可用 CPU 核心的数量。 
- 如果您使用 batch 转换将预处理元素组合到一个批次中，<u>建议您使用 map_and_batch 混合转换；特别是在您使用的批次较大时</u>。 
- 如果您要处理<u>远程存储的数据并/或需要反序列化</u>，建议您使用 parallel_interleave 转换来重叠从不同文件读取（和反序列化）数据的操作。 
- 向量化传递给 map 转换的低开销用户定义函数，以分摊与调度和执行相应函数相关的开销。 
- 如果内存可以容纳您的数据，请使<u>用 cache 转换在第一个周期中将数据缓存在内存中</u>，以便后续周期可以避免与读取、解析和转换该数据相关的开销。 
- 如果<u>预处理操作会增加数据大小</u>，建议您首先应用 interleave、prefetch 和 shuffle（如果可以）以减少内存使用量。
-  建议您在<u>应用 repeat 转换之前先应用 shuffle 转换</u>，最好使用 shuffle_and_repeat 混合转换。

##### 3. 优化性能

由于新型计算设备（例如 GPU 和 TPU）可以不断提高神经网络的训练速度，因此，CPU 处理很容易成为瓶颈。tf.data API 为用户提供构建块来设计可高效利用 CPU 的输入流水线，并优化 ETL 流程的每个步骤。

###### 自动调整数据集性能

```
tf.data.experimental.AutotuneOptions()
```

[**tf.data模块--二 优化pipeline性能**](https://zhuanlan.zhihu.com/p/163656225)

###### 3.1 prefetch预取数据

`dataset.prefetch()`的作用是会在第n个epoch的training的同时预先fetch第n+1个epoch的data,这个操作的实现是在background开辟一个新的线程，将数据读取在cache中，这也大大的缩减了总的训练的时间。

```python
benchmark(
    ArtificialDataset()
    .prefetch(tf.data.experimental.AUTOTUNE)
)
```

###### 3.2 map并行处理数据转换（Parallelizing Data Transformation）

准备批次数据时，可能需要预处理输入元素。

为此，tf.data API 提供了 tf.data.Dataset.map 转换，以将用户定义的函数（例如，正在运行的示例的 parse_fn）应用于输入数据集的每个元素。由于输入元素彼此独立，因此可以跨多个 CPU 核心并行执行预处理。为实现这一点，map 转换提供了 num_parallel_calls 参数来指定并行处理级别。

并行后，由于数据预处理的时间缩短，整体的时间也减少了。如何为 num_parallel_calls 参数选择最佳值取决于硬件、训练数据的特征（例如其大小和形状）、映射函数的成本以及同时在 CPU 上进行的其他处理；一个简单的启发法是设为可用 CPU 核心的数量。例如，如果执行以上示例的机器有 4 个核心，则设置 num_parallel_calls=4 会更高效。另一方面，将 num_parallel_calls 设置为远大于可用 CPU 数量的值可能会导致调度效率低下，进而减慢速度。

###### 3.3 并行处理远程数据提取（**Parallel data extraction**）

在实际设置中，输入数据可能会***远程存储***（例如，GCS 或 HDFS），这是因为输入数据<u>不适合本地存储</u>，或因为训练是<u>分布式训练</u>，因此在每台机器上复制输入数据没有意义。非常适合在本地读取数据的数据集流水线在远程读取数据时可能会遇到 I/O 瓶颈，这是因为*本地存储和远程存储之间存在以下差异*： 

- **首字节时间**：与本地存储相比，从远程存储读取文件的首字节所用时间可能要多出几个数量级。 

- **读取吞吐量**：虽然远程存储通常可提供较大的聚合带宽，但读取单个文件可能只能利用此带宽的一小部分。 

此外，将原始字节读入内存中后，可能还需要对数据进行反序列化或解密（例如，protobuf），这会带来额外的开销。无论数据是在本地还是远程存储，都存在这种开销，但如果未有效预取数据，则在远程存储的情况下可能更糟。 

为了降低各种数据提取开销的影响，tf.data API 提供了 tf.contrib.data.parallel_interleave 转换。使用此转换可以并行执行其他数据集（例如数据文件读取器）并交错这些数据集的内容。可以通过 cycle_length 参数指定要重叠的数据集的数量。

###### 3.4 缓存（cache）

据官网的介绍cache操作可以将数据集缓存在本地或者内存中，可以节省在每一个新的epoch的时间

###### 3.5 建议减小内存的消耗

类似于interleave, prefetch,shuffle都会有内部存储的消耗，map的顺序会影响内存的消耗，通常我们会选择内存消耗较小的顺序，除非特定的顺序能带来较大的性能的提升。

缓存部分的操作

```text
dataset.map(time_consuming_mapping).cache().map(memory_consuming_mapping)
```

##### 4. 训练时间开销图

###### 1) 普通训练时间开销

![image-20221030162810127](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030162810127.png)

![image-20221030165847925](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030165847925.png)

###### 2) prefetch训练时间开销

![image-20221030162956704](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030162956704.png)

![image-20221030165859089](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030165859089.png)

3）map并行处理数据转换

![image-20221030180307444](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030180307444.png)

![image-20221030180332165](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030180332165.png)

例如，下图说明了将 num_parallel_calls=2 设置为 map 转换的效果：

![image-20221030170018851](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030170018851.png)

4）并行处理远程数据提取-parallel_interleave

![image-20221030170425815](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030170425815.png)![image-20221030170530671](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030170530671.png)

![image-20221030170207377](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030170207377.png)

5）cache缓存

![image-20221030180414835](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221030180414835.png)


