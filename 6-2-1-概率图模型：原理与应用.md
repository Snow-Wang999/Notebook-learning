# 6-2-1-概率图模型：原理与应用

## 前言

概率图形模型（PGM）及其在不确定性下智能推理的应用，于20世纪80年代出现在统计和人工智能推理界。人工智能中的**不确定性（UAI）会议**成为这个蓬勃发展的研究领域的首要论坛。在圣何塞的UAI-92，我第一次见到了我们两个研究生恩里克·苏卡尔（Enrique Sucar），他在那里介绍了他在高级视觉推理的关系和时间模型方面的工作。恩里克在过去25年中对我们领域做出了令人印象深刻的研究贡献，**从客观概率的基础工作，到开发高级形式的PGMS（如时间和事件贝叶斯网络），再到PGM的学习**，例如他最近在多维分类贝叶斯链分类器方面的工作。

概率图形模型现在被广泛接受为一种**强大而成熟的不确定性推理技术**。与早期专家系统中采用的一些特殊方法不同，PGM**基于图论和概率论**的强大数学基础。它们可用于广泛的推理任务，包括**预测、监测、诊断、风险评估和决策**。在开源软件和商业软件中有许多用于推理和学习的有效算法。此外，通过成功应用于大量现实问题领域，已经证明了它们的威力和有效性。恩里克·苏卡尔（Enrique Sucar）是PGM作为实用和有用技术的主要贡献者，他的工作涉及广泛的应用领域。这些领域包括医学、康复和护理、机器人和视觉、教育、可靠性分析以及从石油生产到发电厂的工业应用。

第一批借鉴早期贝叶斯网络研究并将其以书的形式写成引人入胜的故事的作者是Judea Pearl在《智能系统中的概率推理》和Rich Neapolitan在《专家系统中的可能性推理》中。恩里克·苏卡尔（Enrique Sucar）的这本专著是继《珍珠》（Pearl）和《那不勒斯》（Neapolitan）之后的一部及时的文献集，它涵盖了比该领域其他近期文本更广泛的PGM：各种vii分类器、隐马尔可夫模型、马尔可夫随机场、贝叶斯网络及其动态、时间和因果变量、关系PGM、决策图和马尔可夫决策过程。它以清晰易懂的方式介绍了这些PGM，以及相关的推理（或推理）和学习方法，使其适合于高级学生以及对使用概率模型感兴趣的其他学科的研究人员或从业者。Enrique充分利用了他在PGM建模方面的丰富实践经验，在从生物信息学到空气污染到物体识别的各种现实应用中展示了PGM的应用，从而大大丰富了文本。我衷心祝贺恩里克的这本书，并将其推荐给潜在的读者。

## 概述

概率图形模型已经成为多个领域中使用的一组强大的技术。本书从工程角度提供了概率图形模型（PGM）的一般介绍。它涵盖了PGM的主要类别的基本原理：**贝叶斯分类器、隐马尔可夫模型、贝叶斯网络、动态和时间贝叶斯网络、马尔可夫随机场、影响图和马尔可夫决策过程**；包括所有技术的表示、推理和学习原则。书中还介绍了每种模型的实际应用。

一些关键特征是：

- PGM的主要类别**在统一框架下**的一本专著中介绍。
- 本书涵盖了所有技术的基本方面：**表示、推理和学习**
- 它说明了不同技术**在实际问题中的应用**，这是学生和从业者的一个重要特点
- 它包括该领域的一些**最新发展**，如多维度贝叶斯分类器、关系图形模型和因果模型
- 每一章都有一套**练习**，包括对研究和编程项目的建议。

激励将概率图形模型应用于现实问题是本书的目标之一。这不仅需要了解不同的模型和技术，还需要一些实践经验和领域知识。为了帮助不同领域的专业人员深入了解PGM在解决实际问题中的应用，本书包括许多不同类型模型在**广泛领域**中的应用示例，包括：

- 计算机视觉。
- 生物医学应用
- 工业应用。
- 信息检索。
- 智能辅导系统。
- 生物信息学。
- 环境应用。
- 机器人。
- 人机交互。
- 信息验证。
- 关爱。



## 第一章 基础

本书的第一章包括概率图形模型的一般介绍，并提供了本书其余部分所需的理论基础：**概率理论和图论。**

### 第一节 介绍

![image-20221229131825328](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229131825328.png)

#### 1.1 不确定性

为了实现他们的目标，智能体，无论是自然的还是人工的，都必须在许多可能性中选择一个行动方案。也就是说，他们必须**根据从环境中获得的信息、以前的知识和目标做出决定**。在许多情况下，信息和知识是不完整或不可靠的，他们的决策结果是不确定的，也就是说，他们必须**在不确定的情况下做出决策**。例如：紧急情况下的医生必须及时采取行动，即使她对患者的状态信息有限；自动驾驶汽车如果检测到前方可能有障碍物，必须在不确定障碍物的距离、大小和速度的情况下决定是否转弯或停车；或者金融机构需要根据其对不同备选方案的预期回报和客户要求的模糊预测来选择最佳投资。

人工智能的目标之一是**开发能够在不确定性条件下推理和决策的系统**。不确定性下的推理给早期智能系统带来了挑战，因为传统的范式不太适合管理不确定性。

##### 1.1.1 不确定性的影响

早期的人工智能系统基于经典逻辑，其中知识可以表示为一组逻辑子句或规则。这些系统具有两个重要的特性，即**模块性和单调性**，这有助于简化知识获取和推理。

如果每一条知识都可以独立使用以得出结论，那么系统就是模块化的。也就是说，如果任何逻辑子句或规则的前提为真，那么我们可以断言其结论，而无需考虑知识库中的其他元素。例如，如果我们有规则， $∀X，stroke(X)→ impairedarm(X)$ 如果我们知道玛丽中风了，我们就知道她的手臂受损了。

如果一个系统的知识总是单调增加，那么它就是单调的：也就是说，即使系统知道新的事实，任何推断出的事实或结论也会保持不变。例如，如果有一个规则，例如 $∀X，bird(X)→ flies(X)$，那么如果Tweety是一只鸟，我们可以断言她会飞。

然而，如果我们有不确定性，这两个性质一般都不成立。在医疗系统中，患者的诊断通常存在不确定性，因此，如果患者中风，其手臂可能不会受损；这取决于受中风影响的大脑部位。同样，并不是所有的鸟都会飞，所以如果我们后来了解到Tweety是一只企鹅，我们就需要收回她会飞的结论。

这两个特性的丧失使得在不确定性下必须推理的系统变得更加复杂。原则上，系统在得出结论时必须考虑所有可用的知识和事实，并且在获取新数据时必须能够改变其结论。

#### 1.2 简史

从人工智能的角度来看，我们可以考虑不确定性管理技术发展的以下阶段：

- 开始【Beginnings】（1950年代和1960年代）-人工智能（AI）研究人员专注于**解决定理证明、国际象棋等游戏和“区块世界”规划领域等不涉及不确定性的问题**，使得没有必要开发管理不确定性的技术。最初，符号范式主导着人工智能。

- 特设技术【Ad hoc techniques】（1970年代）——为医学和采矿等实际应用开发**专家系统，需要开发不确定性管理方法**。为特定的专家系统开发了新的即席技术，如MYCIN的确定因素[15]和浏览者的伪概率[3]。后来发现，这些技术有一组隐含的假设，限制了其适用性[5]。在这一时期，还提出了管理专家系统中不确定性的替代理论，包括**模糊逻辑**[17]和Dempster-Shafer理论[14]。

- 概率的复兴【Resurgence of probability】（20世纪80年代）-概率理论被用于一些最初的专家系统，但后来被丢弃，因为它以天真的方式应用意味着高计算复杂性（见第1.3节）。新的发展，特别是**贝叶斯网络**[11]，使得以有效的方式构建复杂的概率系统成为可能，开始了人工智能中不确定性管理的新时代。

- 多样化的形式主义【Diverse formalisms】（20世纪90年代）**-贝叶斯网络继续发展**，并随着高效推理和学习算法的发展而得到巩固。同时，**模糊和非单调逻辑**等其他技术被认为是不确定性下推理的替代方案。

- 概率图形模型【Probabilistic graphical models】（2000年代）-基于概率和图形表示的几种技术被整合为在不确定性下**表示、推理和决策的强大方法**，包括贝叶斯网络、马尔可夫网络、影响图和马尔可夫决策过程等。

#### 1.3 基本概率模型

概率论为管理不确定性提供了一个稳固的基础，因此在不确定性下使用概率论进行推理是很自然的。然而，如果我们以天真的方式将概率应用于复杂问题，我们很快就会被计算复杂性吓退。

在本节中，我们将展示如何使用基于平面表示的天真概率方法来建模问题；然后我们如何使用这种表示来回答一些概率查询。这将有助于理解基本方法的局限性，激励概率图形模型的开发。1【本节和以下章节假设读者熟悉概率论的一些基本概念；第2章对这些和其他概念进行了回顾。】

许多问题可以表示为一组变量X1、X2，使得我们知道其中一些变量的值，而其他变量是未知的。例如，在医学诊断中，变量可能代表某些症状和相关疾病；通常我们知道症状，我们想找到最可能的疾病。另一个例子是，金融机构开发了一个系统来帮助确定给予某个客户的信贷金额。在这种情况下，相关变量是客户的属性，即年龄、收入、以前的信用等。；以及表示要给予的信贷金额的变量。例如，根据客户属性，我们希望确定安全给予客户的最大信用额度。一般来说，有几种类型的问题可以通过这种方式建模，例如诊断、分类和感知问题等。

在概率框架下，我们可以考虑问题的每个属性都是一个随机变量，因此它可以从一组值中获取某个值2【随机变量稍后正式定义。】；例如，$X=\{x_1，x_2，…，x_m\}$可能代表医学领域中的m种可能的疾病。随机变量的每个值都将具有与上下文相关联的特定概率；在X的情况下，它可能是特定人群中每种疾病的概率（所谓的疾病流行率），即。；$P(X=x_1),P(X=x_2),...,$简称$P(x_1),P(x_2),...$

如果我们考虑两个随机变量$X$，$Y$，那么我们可以计算$X$取某一值和$Y$取某一个值的概率，即$P(X=X_i \ | \ Y=Y_j)$或$P(X_i，Y_j)$；这称为X和Y的联合概率。该思想可以推广到n个随机变量，其中联合概率表示为$P(X_1，X_2，…，X_n)$。我们可以将$P(X_1，X_2，…，X_n)$看作一个函数，它将概率值分配给变量$X_1,X_2,X_n$个

因此，我们可以将域表示为：

1. 一组随机变量$X_1,X_2,X_n$。

2. 与这些变量$P(X_1,X_2,…,X_n)$相关的联合概率分布【所谓联合概率，就是既满足 $X_1$ 条件，又满足 $X_2$ 条件的概率，......，还满足$X_n$条件的概率，**n个条件的满足是站在同一起跑线的**。】

给定此表示，我们可以回答关于域中某些变量的一些查询，例如：

**边际概率【Marginal probabilities】**：其中一个变量取某个值的概率。【单一就从某一变量的类别（假设$X_1$是颜色）上说，$P(X_1=红色) = 9/16$ ，$P(X_1=黑色) = 7/16$，这就是边缘分布。】这可以通过对联合概率分布的所有其他变量求和来获得。换句话说，
$$
P(X_i)=∑_{∀X\neq X_i} {P(X_1,X_2,...,X_n)}
$$
这就是所谓的边缘化。边际化可以通过对剩余变量求和来推广，以获得变量子集的边际联合概率。

**条件概率【Conditional probabilities】**：根据定义，给定$X_j$为$P(X_i \ |\ X_j)=P(X_i,X_j)/P(X_j)$，$P(X_j)\neq 0$，的条件概率。$P(X_i，X_j)$和$P(X_j)$可以通过边缘化获得，从中我们可以获得条件概率。

**全外展或MPE【Total Abduction or MPE】**：假设变量的子集$(E)$已知，绑架包括在给定证据$maxP(J\ |\ E)$的情况下找到使其条件概率最大化的其余变量$(J)$的值。即$Arg Max_J[P(X_1，X_2，…，X_n)/P(E)]$。

**部分外展或MAP【Partial abduction or MAP】**：在这种情况下，有三个变量子集：证据$E$，我们希望最大化的查询变量$J$，以及其他变量$K$，这样我们希望使$P(J \ | \ E)$最大化。这是通过在$K$上求积和在$J$上最大化获得的，即 $ArgMax_J[∑_{X∈K} P(X_1,X_2,...,X_n)/P(E)]$ 。

此外，如果我们有感兴趣领域的数据，我们可以从这些数据中获得一个模型，即估计相关变量的联合概率分布。

接下来，我们用一个简单的例子说明基本方法。

##### 1.3.1 示例

| Outlook  | Temperature | Humidity | Windy | Play |
| -------- | ----------- | -------- | ----- | ---- |
| Sunny    | High        | High     | False | No   |
| Sunny    | High        | High     | True  | No   |
| Overcast | High        | High     | False | Yes  |
| Rain     | Medium      | High     | False | Yes  |
| Rain     | Low         | Normal   | False | Yes  |
| Rain     | Low         | Normal   | True  | No   |
| Overcast | Low         | Normal   | True  | Yes  |
| Sunny    | Medium      | High     | False | No   |
| Sunny    | Low         | Normal   | False | Yes  |
| Rain     | Medium      | Normal   | False | Yes  |
| Sunny    | Medium      | Normal   | True  | Yes  |
| Overcast | Medium      | High     | True  | Yes  |
| Overcast | High        | Normal   | False | Yes  |
| Rain     | Medium      | High     | True  | No   |

表1.1高尔夫示例的样本数据集

我们将使用传统的高尔夫示例来说明基本方法。在这个问题中，我们有五个变量：前景、温度、湿度、有风、玩耍。表1.1显示了高尔夫示例的一些数据；所有变量都是离散的，因此它们可以从有限的值集合中取一个值，例如Outlook可以是晴天、阴天或雨天。现在，我们将说明如何计算本例前面提到的不同概率查询。

首先，我们将仅使用两个变量（Outlook和Temperature）简化示例。根据表1.1中的数据，我们可以获得表1.2所示的展望和温度的联合概率。表中的每个条目对应于联合概率$P(Outlook,Temperature)$，例如，$P(Outlook=S，Temperature=H)=0.143$。

让我们首先获得两个变量的边际概率。如果我们对每一行求和（边缘化温度），那么我们获得了向外看的边缘概率，$P(Outlook)=[0.357，0.286，0.357]$；如果我们对每一列求和，我们可以得到温度的边际概率，$P(Temperature)=[0.286，0.428，0.286]$。从这些分布中，我们可以得出最可能的温度是$M$，展望的最可能值是$S$和$R$。

表1.2展望和温度的联合概率分布

![image-20221229182420699](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229182420699.png)

现在，我们可以计算给定温度下展望的条件概率，反之亦然。例如：
$$
P(Temp.\  |\  Outlook = R) = P(Temp. ,\  Outlook = R)/P(Outlook = R)
= [\ 0,\  0.6,\  0.4\ ]
$$

$$
P(Outlook\  |\  Temp. = L) = P(Outlook, \  Temp. = L)/P(Temp. = L)
= [\ 0.25,\  0.25,\  0.5\ ]
$$

考虑到这些分布，假设天气为雨，最有可能的温度为中等，而假设温度为低，最有可能性的天气为雨。

最后，天气和温度的最可能组合是{雨，中等}，在这种情况下，可以直接从联合概率表中获得。

尽管可以为这个小示例计算不同的概率查询，但对于具有许多变量的复杂问题，这种方法变得不切实际，因为表的大小以及边际概率和条件概率的直接计算随着模型中变量的数量呈指数增长。

这种天真方法的另一个缺点是，为了从数据中获得联合概率的良好估计，如果模型中有许多变量，我们将需要一个非常大的数据库。经验法则是，实例（记录）的数量至少是变量可能组合值数量的10倍，因此如果我们考虑50个二进制变量，则至少需要$10 × 2^{50}$个实例！

最后，联合概率表对人类来说并没有太多的问题；因此，这种方法也有认知局限性。

基本方法中出现的问题是概率图形模型开发的一些动机。

#### 1.4 概率图形模型

概率图形模型（PGM）提供了一种基于概率理论以计算效率方式**管理不确定性的框架**。基本思想是只考虑那些对某个问题有效的**独立关系**，并将其包含在概率模型中，以减少内存需求和计算时间方面的复杂性。表示一组变量之间的依赖关系和独立关系的一种自然方法是**使用图**，这样，直接依赖的变量是连接的，并且独立关系在这个依赖图中是隐含的。

概率图形模型是联合概率分布的紧凑表示，我们可以从中获得边际概率和条件概率。与平面表示相比，它有几个优点：

- 它通常更紧凑（空间）。
- 它通常效率更高（时间）。
- 更容易理解和沟通。
- 学习表单数据或基于专家知识构建更容易。

概率图形模型由两个方面规定：

- （i）定义模型结构的图G（V，E）；

- 和（ii）定义参数的一组局部函数f（Yi），其中Yi是X的子集。联合概率由局部函数的乘积获得：

$$
P(X_1,X_2,...,X_N)=K\prod_{i=1}^{M}{f(Y_i)}\tag{1.1}
$$

其中K是归一化常数（它使概率总和为1）。

这种图形和一组局部函数（称为势函数）的表示是PGM中推理和学习的基础：

推断：在给定任何其他子集Y的情况下，获得变量Z的任何子集的边际概率或条件概率。

学习：给定X的一组数据值（可能不完整），估计模型的结构（图）和参数（局部函数）。

我们可以根据三个维度对概率图形模型进行分类：

1. 定向或无定向(Directed or undirected)
2. 静态或动态(Static or dynamic)
3. 概率或决策(Probabilistic or decisional)

第一个维度与用于表示依赖关系的图形类型有关。无向图表示对称关系，而有向图表示方向重要的关系。给定一组具有相应条件独立关系的随机变量，不可能用一种类型的图来表示所有关系[11]；因此，这两种类型的模型都是有用的。

第二个维度定义模型是在某个时间点（静态）还是在不同时间点（动态）表示一组变量。概率模型只包括随机变量，而决策模型也包括决策和效用变量。

表1.3总结了PGM的最常见类别及其根据先前维度的类型。

所有这些类型的模型将在以下章节中详细介绍，以及一些考虑更具表现力的模型（关系概率图形模型）或表示因果关系（因果贝叶斯网络）的扩展。

![image-20221229192959192](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229192959192.png)

#### 1.5 表示、推理和学习

每类概率图形模型有三个主要方面：表示、推理和学习。

表示是每个模型的基本属性，它定义了哪些实体构成了它，以及这些实体是如何关联的。例如，所有的PGM都可以表示为定义模型结构的图形和描述其参数的局部函数。然而，图形的类型和局部函数因不同类型的模型而异。

推理包括基于模型和一些证据回答不同的概率查询。例如，在已知模型中其他变量的情况下，获得变量或变量集的后验概率分布。挑战是如何有效地做到这一点。

要构建这些模型，基本上有两种选择：在领域专家的帮助下“手动”构建模型，或者从数据中导出模型。近年来的重点是基于机器学习技术来诱导模型，因为在专家的帮助下进行这项工作既困难又昂贵。特别是，获取模型的参数通常基于数据，因为人类往往是概率的糟糕估计者。

从应用的角度来看，这些技术的一个重要特性是它们倾向于将推理和学习技术从模型中分离出来。也就是说，与其他人工智能表示（如逻辑和生产规则）一样，推理机制是通用的，可以应用于不同的模型。因此，为每类PGM中的概率推断开发的技术可以直接应用于各种应用中的不同模型。

![image-20221229193306206](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229193306206.png)

图1.1不同类别的PGM遵循的一般范式的示意图，其中学习和推理引擎（通用）与知识库（取决于特定应用）之间存在明显的分离

这一想法如图1.1所示。基于数据或专家知识或两者的组合，使用学习引擎构建知识库（在本例中为概率图形模型）。一旦我们有了模型，我们就可以使用它通过推理机进行概率推理；根据观察结果和模型，推理机得出结果。学习和推理引擎对于一类PGM是通用的，因此它们可以应用于不同领域的建模和推理。

对于本书中介绍的每种类型的PGM，我们将首先描述其表示，然后介绍一些最常见的推理和学习技术。

#### 1.6 应用

大多数现实世界中的问题都意味着要处理不确定性，并且在解决这些问题时通常需要考虑大量的因素或变量。概率图形模型构成了解决具有不确定性的复杂问题的理想框架，因此它们被广泛应用于以下领域：

- 医疗诊断和决策。
- 移动机器人定位、导航和规划。
- 涡轮机和发电厂等复杂工业设备的诊断。
- 自适应界面和智能导师的用户建模。
- 语音识别和自然语言处理。
- 污染建模和预测。
- 复杂过程的可靠性分析。
- 模拟病毒的进化。
- 计算机视觉中的物体识别。
- 信息检索。
- 能源市场。

不同类型的PGM更适合于不同的应用，这将在下面的章节中显示，当我们为每类PGM提供应用示例时。

#### 1.7 本书概述

这本书分为四部分。

第一部分为理解以下章节中介绍的模型和技术提供了数学基础。第2章回顾了概率和信息理论中的一些基本概念，这些概念对于理解概率图形模型很重要。第3章概述了图论，重点介绍了概率图形模型中表示和推理的某些重要方面；除其他外，还包括小团体(cliques,)、三角图(triangulated graphs)和完美排序。

第二部分介绍了**不同类型的概率模型**，这些模型只有随机变量，而不考虑模型中的决策或效用。这是最大的部分，包括以下类型的PGM：

- 贝叶斯分类器(Bayesian classifiers)
- 马尔可夫链和隐马尔可夫模型(Markov chains and hidden Markov models)
- 马尔可夫随机场(Markov random fields)
- 贝叶斯网络(Bayesian networks)
- 动态贝叶斯网络和时间网络(Dynamic Bayesian networks and temporal networks)

一章专门介绍每种类型的模型（贝叶斯网络除外，它分为两章），包括表示、推理和学习；以及实际应用实例。

第三部分介绍了那些考虑决策和效用的模型，因此侧重于帮助决策者在不确定性条件下采取最佳行动。本部分包括两章。第一章介绍了当有一个或几个决策时的建模技术，包括**决策树和影响图**。第二章讨论顺序决策问题，特别是**马尔可夫决策过程**。

第四部分考虑了可以被认为是传统概率图形模型的扩展的**替代范例**。它包括两章。第一章致力于**关系概率模型**，通过将一阶逻辑的表达能力与概率模型的不确定性推理能力相结合，提高了标准PGM的表达能力。第二章介绍了超越概率依赖的因果图形模型，以表达**因果关系**。

#### 1.8 附加阅读

在本书中，我们介绍了概率图形模型的广泛视角。很少有其他书有类似的报道。一个是Koller和Friedman[7]提出的，他们在不同的结构下提出了模型，但较少强调应用。另一个是Lauritzen[8]，它更注重统计。贝叶斯编程[1]提供了一种基于编程范式实现图形模型的替代方法。

有几本书更深入地涵盖了一种或几种类型的模型，例如：贝叶斯网络[2，10，11]、决策图[6]、马尔可夫随机场[9]、马尔可夫决策过程[13]、关系概率模型[4]和因果模型[12，16]。

#### 工具书类

1. Bessiere，P.，Mazer，E.，Ahuactzin，J.M.，Mekhnacha，K.：贝叶斯编程。CRC出版社，Boca Raton（2014）
2. 达尔文，A.：贝叶斯网络的建模和推理。剑桥大学出版社，纽约（2009）
3. Duda，R.O.，Hart，P.A.，Nilsson，N.L.：基于规则推理系统的主观贝叶斯方法。在：《国家计算机会议论文集》，第45卷，第1075–1082页（1976）
4. Getoor，L.，Taskar，B.：统计关系学习导论。麻省理工学院出版社，剑桥（2007）
5. Heckerman，D.：MYCIN确定性因素的概率解释。《第一届人工智能不确定性会议论文集》（UAI），第9–20页（1985）
6. Jensen，F.V.：贝叶斯网络和决策图。Springer，纽约（2001）
7. Koller，D.，Friedman，N.：概率图形模型：原理和技术。麻省理工学院出版社，剑桥（2009）
8. Lauritzen，S.L.：图形模型。牛津大学出版社，牛津（1996）
9. Li，S.Z.：图像分析中的马尔可夫随机场建模。施普林格，伦敦（2009）
10. Neapolitan，R.E.：专家系统中的概率推理。纽约威利（1990）
11. Pearl，J.：《智能系统中的概率推理：合理推理网络》。Morgan Kaufmann，旧金山（1988）
12. 《因果关系：模型、推理和推论》。剑桥大学出版社，纽约（2009）
13. Puterman，M.L.：马尔可夫决策过程：离散随机动态规划。Wiley，纽约（1994）
14. Shafer，G.：证据的数学理论。普林斯顿大学出版社，普林斯顿（1976）
15. Shortliffe，E.H.，Buchanan，B.G.：医学中不精确推理的模型。数学Biosci公司。23, 351–379 (1975) 
16.  Spirtes，P.，Glymour，C.，Scheines，R.：因果关系、预测和搜索。麻省理工学院出版社，纽约（2000）
17. Zadeh，L.A.：模糊逻辑中的知识表示。IEEE Trans。知道。数据工程1（1），89–100（1989）

---

1. Bessiere, P., Mazer, E., Ahuactzin, J.M., Mekhnacha, K.: Bayesian Programming. CRC Press, Boca Raton (2014)
2. Darwiche, A.: Modeling and Reasoning with Bayesian Networks. Cambridge University Press, New York (2009)
3. Duda, R.O., Hart, P.A., Nilsson, N.L.: Subjective Bayesian methods for rule-based inference systems. In: Proceeding of the National Computer Conference, vol. 45, pp. 1075–1082 (1976)
4. Getoor, L., Taskar, B.: Introduction to Statistical Relational Learning. MIT Press, Cambridge (2007)
5. Heckerman, D.: Probabilistic interpretations for MYCIN’s certainty factors. In: Proceedings of the First Conference on Uncertainty in Artificial Intelligence (UAI), pp. 9–20 (1985)
6. Jensen, F.V.: Bayesian Networks and Decision Graphs. Springer, New York (2001)
7. Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Techniques. MIT
   Press, Cambridge (2009)
8. Lauritzen, S.L.: Graphical Models. Oxford University Press, Oxford (1996)
9. Li, S.Z.: Markov Random Field Modeling in Image Analysis. Springer, London (2009)
10. Neapolitan, R.E.: Probabilistic Reasoning in Expert Systems. Wiley, New York (1990)
11. Pearl, J.: Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
    Morgan Kaufmann, San Francisco (1988)
12. Pearl, J.: Causality: Models, Reasoning and Inference. Cambridge University Press, New York
    (2009)
13. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Programming.
    Wiley, New York (1994)
14. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press, Princeton (1976)
15. Shortliffe, E.H., Buchanan, B.G.: A model of inexact reasoning in medicine. Math. Biosci. 23,
    351–379 (1975)
16. Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction, and Search. MIT Press, New York (2000)
17. Zadeh, L.A.: Knowledge Representation in Fuzzy Logic. IEEE Trans. Knowl. Data Eng. 1(1),
    89–100 (1989)

---

### 第二节 概率论

#### 2.1 简介

概率论起源于机会游戏，有着悠久而有趣的历史；它已经发展成为量化不确定性的数学语言。

考虑某个实验，比如掷骰子；这个实验可以有不同的结果，我们称每个结果为结果或元素。在模具示例中，可能的结果或元素如下：｛1，2，3，4，5，6｝。实验的所有可能结果的集合称为样本空间，Ω。事件是Ω的一组元素或子集。继续使用骰子示例，一个事件可能是骰子显示偶数，即{2，4，6}。

在我们用数学定义概率之前，值得讨论概率的含义或解释。已经提出了几种概率的定义或解释，从拉普拉斯的经典定义开始，包括极限频率、主观、逻辑和倾向解释[1]：

- 经典（classical）：概率与等概率事件有关；如果某个实验有N个可能的结果，则每个结果的概率为1/N。

- 逻辑（logical）：概率是理性信念的量度；也就是说，根据可用的证据，一个理性的人会对一个事件有一定的信念，这将定义它的概率。

- 主观（subjective）：概率是衡量个人对某一事件的信任程度；这可以用一个投注因素来衡量，一个人在某一事件上的概率与该人愿意在该事件上投注多少有关。

- 频率（frequency）：当实验的重复次数趋于无穷大时，概率是对给定实验事件发生次数的度量。

- 倾向性（propensity）：概率是在可重复条件下事件发生次数的量度，即使实验只发生一次。

这些解释可以分为概率和统计中的两种主要方法：

- 客观（经典、频率、倾向）：概率存在于现实世界中，可以测量。

- 认识论（逻辑的，主观的）：概率与人类的知识边缘有关，它们是信念的量度。

这两种方法遵循下面定义的相同数学公理；然而，在应用概率的方式上存在差异，特别是在统计推断中。这些差异让位于主要的两个统计学派：频率学派和贝叶斯学派。在人工智能领域，特别是在专家系统中，首选的方法往往是认识论或主观方法；然而，也使用了客观方法[4]。

我们将考虑逻辑或规范方法，并在给定可用证据的情况下，根据某个命题的可信度来定义概率[2]。根据考克斯的工作，Jaynes确立了一些基本的愿望，即这种合理性必须遵循[2]：

- 用实数表示。（Representation by real numbers）
- 与常识的定性对应。（Qualitative correspondence with common sense）
- 一致性（Consistency）

基于这些直觉原理，我们可以导出概率的三个公理：

1. $P(A)$是[0，1]中的连续单调函数。
2. $P(A,B\ |\ C)=P(A\ |\ C)P(B\ |\ A,C)$（乘积规则）。
3. $P(A\ |\ B)+P(¬A\ |\ B)=1$（求和规则）。

其中A、B、C是命题（二进制变量），$P(A)$是命题A的概率。$P（A\ |\ C）$是给定C已知的A的概率，称为条件概率。$P（A,B\ |\ C）$是给定C（逻辑连接）时A与B的概率，$P(¬A \ |\ C)$则是给定C时NOT A（逻辑否定）的概率。这些规则相当于最常用的Kolmogorov公理。从这些公理中，可以导出所有常规的概率论。

#### 2.2 基本规则

两个命题的析取（逻辑和）的概率由和规则给出：$P(A+B\ |\ C)=P(A\ |\ C)+P(B\ |\ C)−P(A，B\ |\ C)$；如果命题A和B在给定C的情况下互斥，我们可以将其简化为：$P(A+B\ |\ C)=P(A\ |\ C)+P(B\ |\  C)$。这可以推广到N个互斥命题：

![image-20221229202129371](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202129371.png)

![image-20221229202204671](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202204671.png)

![image-20221229202212635](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202212635.png)

![image-20221229202220855](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202220855.png)

![image-20221229202229960](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202229960.png)

![image-20221229202241683](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202241683.png)

![image-20221229202248608](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202248608.png)

### 第三节 图论

![image-20221229202730094](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202730094.png)

#### 3.1 定义

图形提供了一种表示一组对象之间的二进制关系的紧凑方式。例如，考虑某个地区的一组城市，以及连接这些城市的道路。然后，该地区的地图本质上是一个图形，其中的对象是城市，城市对之间的直接道路是关系。图形通常用图形表示。对象表示为圆或椭圆，关系表示为线或箭头；见图3.1。有两种基本类型的图：无向图和有向图。接下来，我们将有向图和无向图的定义形式化。

给定$V$，一个非空集合，$V$上的二元关系$E⊆V×V$是一组有序对，$(V_j，V_k)$，使得$V_j∈V$和$V$中的$V_k$。

**有向图或有向图是一个有序对，$G=(V,E)$：**

- **$V$是一组顶点或节点，**
- **E是一组弧，表示V上的二元关系**；

见图3.1b。*<u>有向图(directed graph)表示对象之间的反对称关系，例如“父(parent)”关系。</u>*

无向图是一个有序对，$G=(V，E)$，其中V是一组顶点或节点，E是表示对称二元关系的一组边：$(V_j，V_k)∈E→ (V_k，V_j)∈E$；见图3.1a。*<u>无向图表示对象之间的对称关系，例如“兄弟”关系。</u>*

如果节点$j$和$k$之间存在边$E_i(V_j，V_k)$，则$V_j$与$V_k$相邻。<u>*节点的阶数是该节点中关联的边数。*</u>在图3.1a中，上部节点的阶数为2，两个下部节点的阶为1。

与同一对顶点关联的两条边称为平行边；入射在单个顶点上的边是循环；并且不是任何边的端点的顶点是其度为0的孤立顶点。如图3.2所示。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202854426.png" alt="image-20221229202854426" style="zoom:100%;" />
    <br> <!--换行-->
    图3.1图：
    <br>a.无向图(undirected)，
    <br> b.有向图(directed,边有箭头) <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221229202911657.png" alt="image-20221229202911657" style="zoom:100%;" />
    <br> <!--换行-->
    图3.2 示例：
    <br> a.平行边、
    <br> b.循环、
    <br> c.孤立顶点 <!--标题-->
    </center>
</div>

在有向图中，指向节点的弧的数量是节点的**入度数**，而指向远离节点的边的数量是其**出度数**。在图3.1b中，两个上部节点的入度为0，出度为2；而两个较低节点具有2的入度和0的出度。

给定图 $G=(V，E)$，$G$ 的**子图** $G'=(V'，E')$是这样一个图，即 $V'⊆V$ 和 $E'⊆E$ ，其中$E'$中的每条边都入射到$V'$中的顶点上。例如，如果我们去掉图3.1b中的边的方向（使其成为无向图），那么图3.1a中的图就是图3.1b的子图。

#### 3.2 图的类型

除了有向和无向这两种基本图之外，还有其他类型的图，例如：

- **链图（Chain graph）**：具有有向和无向边的混合图。如下图3.3(a)
- **简单图形（Simple graph）**：不包括循环和平行弧的图形。如下图3.3(b)
- **多重图（Multigraph）**：具有多个组成部分（子图）的图，每个组成部分与其他组成部分没有边，即它们是断开的。如下图3.3(c)
- **完整图（Complete graph）**：每对顶点之间有一条边的图。如下图3.3(d)
- **二分图（Bipartite graph）**：将顶点分为两个子集G1、G2的图，使得所有边将G1中的顶点与G2中的顶点连接起来；也就是说，每个子集中的节点之间没有边。如下图3.3(e)
- **加权图（Weighted graph）**：具有与其边和/或顶点相关联的权重的图。如下图3.3(f)

这些类型的图表示例如图3.3所示。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231114709312.png" alt="image-20221231114709312" style="zoom:100%;" />
    <br> <!--换行-->
    图片3.3 图的类型：
    <br>a.链图(chain graph)、
    <br>b.简单图(simple graph)、
    <br>c.多重图(multigraph)、
    <br>d.完全图(complete graph)、
    <br>e.二分图(bipartite graph)、
    <br>f.加权图(weighted graph) <!--标题-->
    </center>
</div>

#### 3.3轨迹和电路(Trajectories and Circuits)

**轨迹（trajectory）**是边缘序列$E_1、E_2，E_n$使得每条边的最终顶点与序列中下一条边的初始顶点重合（除了最终顶点）；即，对于 $i=1$ 至 $i=n−1$，$E_i(V_j，V_k)$，$E_{i+1}(V_k，V_l)$。一个简单的轨迹不包括两次或多次相同的边；元素轨迹不会多次入射到同一顶点上。不同轨迹的示例如图3.4所示。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231122131928.png" alt="image-20221231122131928" style="zoom:100%;" />
    <br> <!--换行-->
    图3.4 轨迹示例：
    <br>a.是简单但不是基本的轨迹，
    <br>b.是简单而基本的轨迹 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231122153441.png" alt="image-20221231122153441" style="zoom:100%;" />
    <br> <!--换行-->
    图3.5 简单但不基本的回路示例 <!--标题-->
    </center>
</div>

如果图G中的每对不同顶点之间存在轨迹，则图G是连通的。如果图G不连通，则连通的每个部分称为G的一个分量。

**回路（circuit）**是一条轨迹，使得最终顶点与初始顶点重合，即它是一条“闭合轨迹”。以类似于轨迹的方式，我们可以定义简单的基本电路。图3.5显示了电路的示例。

PGM的一个重要类型的图是有向非循环图（DAG-Directed Acyclic Graph）。DAG是一个没有有向电路的有向图（有向电路是序列中所有边都遵循箭头方向的电路）。例如，图3.1b是DAG，图3.3f不是DAG。

图论中的一些经典问题包括轨迹和电路，例如：

- **欧拉轨迹【Euler trajectory】**：找到一条轨迹，该轨迹只包含一次图形中的所有边。
- **欧拉回路【Euler circuit)**：找到一个仅包含一次图中所有边的回路。
- **哈密顿轨迹【Hamiltonian trajectory】**：找到一个仅包含一次图中所有顶点的轨迹。
- **哈密顿回路【Hamiltonian circuit】**：找到一个只包含一次图中所有顶点的回路。
- **旅行推销员问题【Traveling salesman problem】**：在最小成本的加权图中找到哈密顿回路。【1在这种情况下，节点代表具有相关距离或时间的城市和边缘道路，因此解决方案将为旅行推销员提供覆盖所有城市的“最佳”（最小距离或时间）路线。】

这些问题的解决方案超出了本书的范围，感兴趣的读者可以参考[2]。

#### 3.4 图同构（Graph Isomorphism）

如果两个图的顶点和边之间存在一对一的对应关系，则它们是**同构**的，从而保持了关联。给定两个图G1和G2，有三种基本类型的同构：

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231123052594.png" alt="image-20221231123052594" style="zoom:100%;" />
    <br> <!--换行-->
    图3.6 这两个图是同构的 <!--标题-->
    </center>
</div>

1. **图同构（Graph isomorphism）**。图G1和G2是同构的。

2. **子图同构（Subgraph isomorphism）**。图G1同构于G2的子图（反之亦然）。

3. **双子图同构（Double subgraph isomorphism）**。G1的子图同构于G2的子图

图3.6显示了两个同构图的示例。

确定两个图是否同构（类型1）是一个**NP问题**；而子图和双子图同构问题（类型2和3）是**NP完全**的。参见[1]。

#### 3.5 trees

树是一种在计算机科学中非常重要的图，特别是对于PGM。我们将讨论两种类型的树：**无向树和有向树**。**无向树**是一个没有简单电路的连通图；图3.7描述了一个无向树的示例。在无向树中这里有两类顶点或节点：（i）叶或终端节点，具有一级；（ii）内部节点，度数大于1。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231123727684.png" alt="image-20221231123727684" style="zoom:100%;" />
    <br> <!--换行-->
    图3.7 无向树。此树有五个节点、三个叶节点和两个内部节点 <!--标题-->
    </center>
</div>

**树的一些基本属性是：**

- 每对顶点之间有一个简单的轨迹。
- 顶点数$|V|$等于边数$|E|$加一：$|V|=|E|+1$。
- 具有两个或多个顶点的树至少有两个叶节点。

**有向树**是一个连通的有向图，使得每对节点之间只有一个有向轨迹（也称为单连通有向图）。

有两种类型的有向树：

- （i）根树（或简单的树），

- （ii）多根树。

**根树**有一个度数为零的节点（根节点），其余的节点度数为一。一个**多根树**可能有一个以上的零度节点（根），而某些节点（零或更多）的零度大于一（称为多父节点）。如果我们去掉多根树中边的方向，它就会转化为无向树。我们可以把树看作是多根树的特例。图3.8显示了根树和多根树的示例。

<u>**有向树**</u>的一些相关术语如下。

- **根（Root）**：度数等于零的节点。
- **叶（Leaf）**：外度数等于零的节点。
- **内部节点（Internal node）**：外度数大于零的节点。
- **父级/子级（Parent/Child）**：如果存在从a到B的有向弧，则a是B的父级，B是a的子级。
- **兄弟级（Brothers）**：如果两个或多个节点具有相同的父级则为兄弟级。
- **上升/下降（Ascendants /Descendants）**：如果有一条从a到B的有向轨迹，a是B的上升，B是a的后代。
- **根为A的子树（Subtree with root A）**：以A为根的子树。
- **A的子树（Subtree of A）**：以A的子树为根的子树。
- **K元树（K-ary Tree）**：每个内部节点最多有K个子节点的树。如果每个内部节点都有K个子节点，则它是一个规则树。
- **二叉树（Binary Tree）**：每个内部节点最多有两个子节点的树。

```html
<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="图片路径放这里" 
         alt="无法显示图片时显示的文字" 
         style="zoom:这里写图片的缩放百分比"/>
    <br> <!--换行-->
    这里是图片的标题 <!--标题-->
    </center>
</div>
```

<div>
    <center>
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231124315351.png" alt="image-20221231124315351"  />
    <br>
    图3.8 a.有根树（A rooted tree.）。
     <br>   b.多根树（A polytree）
    </center>
</div>

<div>
    <center>
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221231124738338.png" alt="image-20221231124738338" style="zoom:100%;" />
    <br>
    图3.9说明术语的有向树示例
    </center>
</div>

例如，在图3.9中的树中；

- （i） A是根节点，

- （ii）C、D、F、G是叶节点，

- （iii）B、E是内部节点，

- （iv）A是B的父节点，B是A的子节点，

- （v）B和C是兄弟节点，

- （vi）A是F的祖先，F是A的后代，

- （vii）子树B、D、E、F、G是根B的子树，

- （viii）子树E、F和G是B的子树。图3.9中的树是非正则（nonregular）二叉树。

#### 3.6 Cliques（团）

完整图是一个图，$G_c$，其中每对节点是相邻的；也就是说，每对节点之间都有一条边。图3.3d是完整图表的示例。一个完备集，$W_c$是G的一个子集，它诱导了$G$的一个完备子图。它是$G$的顶点的子集，因此该子图中的每对节点都是相邻的。例如，在图3.3d中，图中三个节点的每个子集都是一个完整的集合。

**团（Cliques）**C是图G的子集，使得它是一个最大的完备集；也就是说，G中没有其他包含C的完备集。图3.3d中三个节点的子集不是集团，因为它们不是最大的；它们包含在完整的图表中。

图3.10中的图表有五个分支，一个分支有四个节点，一个有三个节点，三个分支有两个节点。注意，图中的每个节点都是至少一个集团的一部分；因此，图的一组群总是覆盖V。

以下各节介绍了图论的一些更高级的概念，因为概率图形模型的一些推理算法使用了这些概念。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101181620922.png" alt="image-20230101181620922" style="zoom:100%;" />
    <br> <!--换行-->
    图3.10 团：图中突出显示了五个团 <!--标题-->
    </center>
</div>

#### 3.7完美顺序

图中节点的排序包括为每个顶点分配一个整数。给定具有$n$个顶点的图$G=(V，E)$，则$\alpha=[V_1,V_2,...,V_n]$ 是图的排序；根据此排序，如果$i＜j$，则$V_i$在$V_j$之前。

如果根据此排序在$V_i$之前的每个顶点$V_i$的所有相邻顶点完全连接，则图$G＝(V，E)$的排序$α$是完美排序。也就是说，对于每个$i$，$Adj(V_i)\bigcap\{V_1，V_2，…，V_{i−1}\}$是$G$的完整子图。图3.11描述了一个完美排序的示例。

考虑一组派系$C_1,C_2,...,C_p$。以与节点排序类似的方式，我们可以定义群的排序，$\beta=[C_1,C_2,...,C_p]$。如果每个集团$C_i$的所有公共节点都包含在一个集团C j中，则集团的一个序$β$具有运行交集性质；$C_j$是$C_i$的父级。换句话说，对于每个团$i>1$都存在一个团$j<i$，使得$C_i\bigcap\{C_1,C_2,...,C_{i−1}\}⊆C_j$。一个团(clique)可能有不止一个家长(parent)。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101182339243.png" alt="image-20230101182339243" style="zoom:100%;" />
    <br> <!--换行-->
    图3.11 图中节点和群的排序示例。
    <br>在这种情况下，节点具有完美的排序，并且群的排序满足运行交集属性 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101182455895.png" alt="image-20230101182455895" style="zoom:100%;" />
    <br> <!--换行-->
    图3.12 未三角化的图形示例。
    <br>电路1、2、5、4、1没有和弦 <!--标题-->
    </center>
</div>

图3.11中的团 $C_1$、$C_2$ 和 $C_3$ 具有完美的顺序。在本例中，$C_1$ 是 $C_2$ 的父级， $C_1$ 和 $C_2$ 是 $C_3$ 的父级。

如果 $G$ 中长度大于3的每个简单回路都有一个和弦（chord），则图G被三角化。**弦（chord）**是连接回路中两个顶点的边，不是回路的一部分。例如，在图3.11中，顶点1、2、4、3、1形成的电路具有连接节点2和3的弦。图3.11中的图形是三角形的。图3.12描述了未三角化的图形示例。虽然从视觉上看，该图可能是三角形的，但有一个回路1、2、5、4、1没有任何和弦。

要实现顶点的完美排序，并具有满足运行交集属性的簇的排序，一个条件是对图进行三角化。

在下一节中，我们将介绍以下算法：

- （i）对图的节点进行排序，以实现完美排序；

- （ii）对团进行计数，以确保给定完美排序的运行交集属性；

- 以及（iii）如果没有，则对图进行三角化。

#### 3.8排序和三角剖分算法（Ordering and Triangulation Algorithms）

##### 3.8.1最大基数搜索（ Maximum Cardinality Search）

假设一个图是三角形的，下面的算法（称为最大基数搜索）保证了一个完美的排序。给定具有$n$个顶点的无向图 $G=(V,E)$：

![image-20230101214846043](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101214846043.png)

---

**算法3.1 最大基数搜索算法**

---

从 $V$ 中选择任意顶点并将其赋值为1。

**while** 不是G中的所有顶点都已编号。**do:**

- 从所有未标记的顶点中，选择相邻标记顶点数较高的顶点，并为其指定下一个编号。任意断开连接。

**end while**

----

给定顶点的完美顺序，可以很容易地对群进行编号，以使顺序满足运行交集属性。为此，各团按相反顺序编号。给定 $p$ 个团，具有最高数量节点的团被分配为 $p$；包括下一个最高编号节点的团被分配 $p−1$ ；这种方法可以用图3.11中的示例来说明。编号最高的节点是5，因此包含它的团是$C_3$。下一个最高节点是4，因此包含它的团是$C_2$。剩下的团是$C_1$。现在，我们将看到如何“填充”图形以使其三角化。

##### 3.8.2图形填充

图的填充包括向原始图G添加弧，以获得新的图$G_t$，从而$G_t$被三角化。给定具有n个节点的无向图$G=(V，E)$，以下算法将该图三角化：

![image-20230101215528247](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101215528247.png)

---

**算法3.2 图形填充算法**

---

使用最大基数搜索对顶点$V$排序：$V_1,V_2,V_n$。

**for** $i=n$ **to** $i=1$ **do**

- 对于节点$V_i$，选择其所有相邻节点$V_j$，使$j>i$。调用这组节点$A_i$。
- 如果$k＞i$且$V_k\notin A_i$，则添加从$V_i$到$V_k$的弧。

**end for**

---

例如，考虑图3.12中未三角化的图形。如果我们应用前面的算法，我们首先对节点进行排序，生成图中所示的标记。接下来，我们以相反的顺序处理节点，并获得每个节点的集合A：

- A5：∅

- A4：5 
- A3：4，5 
- A2：3，5。圆弧从2添加到4。
- A1：2、3、4。圆弧从1添加到5。

生成的图形有两个额外的弧2−4和1−5，我们可以验证它是三角形的。

填充算法保证生成的图形是三角形的，但一般来说，在添加最小数量的附加弧方面，它不是最优的。

#### 3.9 附加阅读

有几本图论书籍更详细地涵盖了本章中介绍的大多数概念，包括[2-4]。那不勒斯( Neapolitan)[5]，第3章，涵盖了贝叶斯网络所需的主要图论背景，包括更先进的概念。[1]中描述了一些从算法角度出发的图论技术，包括图同构。

#### 3.10 Exercises

1. 十八世纪，柯尼斯堡市（位于普鲁士，目前是俄罗斯的一部分）被七座桥连接成四部分。据说，居民们试图在整个城市找到一条线路，以便他们只穿过一次每座桥。Euler将问题转化为一个图（在本章开头进行了说明），并建立了一个连通图中恰好通过每条边一次的电路的条件：图中的所有顶点都必须具有偶数次。确定Könisberg的居民是否能够找到欧拉回路。

2. 证明了欧拉建立的条件：图G具有欧拉回路当且仅当G中的所有顶点都具有偶次。

3. 图具有欧拉轨迹的条件是什么？

4. 给定图3.10中的图形，确定其是否具有（a）欧拉回路（b）欧拉轨迹（c）哈密顿回路（d）哈密顿轨迹。

5. 给定图3.10中的图表，是否进行三角测量？如果未进行三角化，请通过应用图形填充算法进行三角化。

6. 证明图中奇数阶顶点的数目是偶数。

7. 给定图3.13中的图，将其转换为无向图，并应用最大基数搜索算法对节点进行排序。

   <div> <!--块级封装-->
       <center> <!--将图片和文字局中-->
       <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101193114967.png" alt="image-20230101193114967" style="zoom:100%;" />
       <br> <!--换行-->
       图3.13 几个练习中使用的图表 <!--标题-->
       </center>
   </div>

8. 对上一个问题的图形进行三角剖分。

9. 给定在前一个问题中获得的三角化图：（a）找到其簇，（b）根据节点排序对簇进行排序，并验证它们是否满足运行交集属性，（c）显示生成的簇树。

10. \*\*\*开发一个程序，用于在给定无向图的情况下生成一个群树。为此，考虑（a）根据最大基数搜索对节点进行排序，（b）使用图填充算法对图进行三角化，（c）找到生成的图的簇并对其进行编号。考虑到前面算法的实现，考虑一个足够的数据结构来表示图形。

11. \*\*\*将一些启发式方法结合到先前练习的程序中，以选择节点排序，从而使图中最大团的大小最小化（这对于贝叶斯网络的连接树推断算法很重要）。

#### References

1. Aho, A.V., Hopcroft, J.E., Ullman, J.D.: The Design and Analysis of Computer Algorithms【计算机算法的设计与分析】.
   Addison-Wesley, Boston (1974)
2. Golumbic, M.C.: Algorithmic Graph Theory and Perfect Graphs【算法图论和完美图】. Elsevier, The Netherlands
   (1994)
3. Gould, R.: Graph Theory【图论】. Benjamin/Cummings, Menlo Park (1988)
4. Gross, J.L., Yellen, J.: Graph Theory and its Applications【图论及其应用】. CRC Press, Boca Raton (2005)
5. Neapolitan, R.: Probabilistic Reasoning in Expert Systems: Theory and Algorithms【专家系统中的概率推理：理论和算法】. Wiley, New
   York (1990)



### 第五节 隐马尔可夫模型（Hidden Markov Models）

```html
<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="图片路径放这里" 
         alt="无法显示图片时显示的文字" 
         style="zoom:这里写图片的缩放百分比"/>
    <br> <!--换行-->
    这里是图片的标题 <!--标题-->
    </center>
</div>
```

#### 5.1 引言

马尔可夫链是表示动态过程的另一类PGM，特别是过程状态如何随时间变化。例如，假设我们正在模拟特定地点的天气如何随时间变化。在一个非常简化的模型中，我们假设天气在一天中是恒定的，并且可以有三种可能的状态：晴朗、多云、下雨。此外，我们假设某一天的天气只取决于前一天。因此，我们可以把这个简单的天气模型看作一个马尔可夫链，其中每天都有一个状态变量，有三个可能的值；这些变量以一条链的形式连接在一起，从一天到下一天有一条有向弧（见图5.1）。这意味着所谓的马尔可夫性质，即第二天的天气状态St+1，与给定当前天气St的所有前几天无关，即P（St+1|St，St−1，…）=P（St+1 |St）。因此，在马尔可夫链中，所需的主要参数是给定前一状态的概率。

前一个模型假设我们可以每天精确地测量天气，即状态是可观测的。然而，这并不一定是真的。在许多应用程序中，我们无法直接观察过程的状态，因此我们有一个所谓的隐马尔可夫模型，其中状态是隐藏的。在这种情况下，除了给定当前状态的下一个状态的概率之外，还有另一个参数对状态的不确定性进行建模，表示为给定状态的观测概率P（O t|St）。这种类型的模型比简单的马尔可夫链更强大，有许多应用，例如在语音和手势识别中。

在简要介绍了马尔可夫链之后，在接下来的章节中，我们将详细讨论隐马尔可夫模型，包括如何解决这类模型的计算问题。然后，我们讨论了标准HMM的几个扩展，并以两个应用示例结束本章。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101220744434.png" alt="image-20230101220744434" style="zoom:100%;" />
    <br> <!--换行-->
    图5.1 该图说明了一个马尔可夫链，其中每个节点表示某个时间点的状态 <!--标题-->
    </center>
</div>

#### 5.2 马尔可夫链（Markov Chains）

马尔可夫链（MC）是具有离散数量的状态q1、q2、…的状态机，q n，并且状态之间的转换是不确定的，即存在从状态qi转换到另一状态q j:P（St=q j|St−1=qi）的概率。时间也是离散的，使得链对于每个时间步t可以处于某个状态qi。它满足马尔可夫性质，即下一个状态的概率仅取决于当前状态。

形式上，马尔可夫链定义为

**状态集（Set of states）**：$Q = \{q_1, q_2, . . . , q_n \}$

**先验概率向量（Vector of prior probabilities）**：$Π = \{π_1, π_2, . . . , π_n \}$， 当$π_i = P(S_0 = q_i )$

**转移概率矩阵（Matrix of transition probabilities）**：

$A = \{a_{ij} \}, i = [1..n], j = [1..n]$，当 $a_{ij} =
P(S_t = q_j | S_{t−1} = q_i )$

$a_{ij}$  是给定 $S_{t−1} = q_i$ 时，$S_t = q_j$的概率

其中 $n$ 是状态数，$S_0$ 是初始状态。以紧凑的方式，MC表示为$λ = \{A, Π \}$.

（一阶）马尔可夫链满足以下性质：

1. 概率公理：$∑_{i}π_{i}=1$和$∑_{j}a_{i j}=1$

2. 马尔可夫性质：$P(S_t=q_j | S_{t−1}=q_i，S_{t−2}=q_k，…)=P(S_t=q_j \ | \ S_{t−1}=q_i)$

例如，考虑前面的简单天气模型，它有三种状态：$q_1$=晴朗，$q_2$=多云，$q_3$=下雨。在这种情况下，为了指定MC，我们需要一个具有三个先验概率的向量（见表5.1）和一个3×3的转移概率矩阵（见表5.2）。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101222759151.png" 
         alt="image-20230101222759151" 
         style="zoom:80%;" />
    <br> <!--换行-->
    表5.1 天气示例的先验概率 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101223012717.png" 
         alt="image-20230101223012717" 
         style="zoom:80%;" />
    <br> <!--换行-->
    表5.2 天气示例的转移概率 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101223153128.png" 
         alt="image-20230101223153128" 
         style="zoom:80%;" />
    <br> <!--换行-->
    图5.2 该图说明了天气示例的状态转换图 <!--标题-->
    </center>
</div>

其中每个节点是一个状态，并且弧表示状态之间的可能转变。如果图中没有出现状态$q_i$和$q_j$之间的弧，则意味着相应的跃迁概率为零。天气示例的状态图示例如图5.2所示。1【不要将状态图与图形模型图混淆，其中节点表示每个状态一个随机变量的特定值，弧在状态之间转换，其中节点代表随机变量，弧表示概率依赖关系。】

给定马尔可夫链模型，我们可以**问三个基本问题：**

- 某一状态序列的概率是多少？
- 链条在一段时间内保持某种状态的概率是多少？
- 链条保持在某一状态的预期时间是多久？

接下来，我们将看到如何回答这些问题，并将使用天气示例来说明这些问题。

给定模型的状态序列的概率基本上是状态序列的转移概率的乘积：
$$
P(q_i,q_j,q_k,...)=a_{0i} a_{ij} a_{jk} \tag{5.1}
$$
其中$a_{0i}$是序列中到初始状态的转变，它可以是它的先验概率（$π_i$）或从先前状态的转移（如果这是已知的）。

例如，在天气模型中，我们可能想知道以下状态序列的概率：Q=晴、晴、雨、雨、晴、多云、晴。假设晴天是MC中的初始状态，则：
$$
P(Q)=π_1a_{11}a_{13}a_{33}a_{31}a_{12}a_{21}=(0.2)(0.8)(0.1)(0.4)(0.3)(0.1)(0.2)=3.84×10^{-5}
$$
将d个时间步长保持在某一状态的概率$q_i$相当于序列在该状态下$d−1$个时间步长，然后转变到不同状态的概率。即，
$$
P(d_i ) = a^{d−1}_{ii}(1 − a_{ii} ) \tag{5.2}
$$
考虑到天气模型，三天多云的概率是多少？其计算方法如下：
$$
P(d_2=3)=a^2_{22}(1-a_{22})=0.6^2(1-0.6)=0.144 
$$
某一状态下状态序列的平均持续时间是该状态下级数的期望值，即$E(D)=∑_i d_i P(d_i)$。代入方程（5.2），我们得到：
$$
E(d_i)=∑_i d_i a^{d−1}_{ii}(1 − a_{ii} ) \tag{5.3}
$$
其可以紧凑形式写成：
$$
E(d_i ) = 1/(1 − a_{ii} ) \tag{5.4}
$$
例如，天气持续多云的预计天数是多少？使用前面的公式：
$$
E(d_2 ) = 1/(1 − a_{22} )=1/(1−0.6)=2.5
$$

##### 5.2.1 参数估算

另一个重要问题是如何确定模型的参数，这就是所谓的参数估计。对于MC，可以简单地通过计算**序列处于特定状态 $i$ 的次数**以及**从状态 $i$ 到状态 $j$ 的转换次数**来估计参数。假设有$N$个观测序列。$γ_{0i}$ 是状态 $i$ 是序列中初始状态的次数， $γ_i$ 是我们观察状态 $i$ 的次数，而$γ_{ij}$是我们观察到从状态 $i$ 到状态 $j$ 的转变的次数。参数可以用以下方程估计。

初始概率：
$$
π_i = γ_{0i} /N \tag{5.5}
$$
转换概率：
$$
a_{ij} = γ_{ij} /γ_{i} \tag{5.6}
$$
注意，对于序列中最后一个观察到的状态，我们不会观察到下一个状态，因此计数中不考虑所有序列的最后一个状态。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101230622955.png" 
         alt="image-20230101230622955" 
         style="zoom:100%;" />
    <br> <!--换行-->
    表5.3 天气示例的计算先验概率 <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101230649311.png" 
         alt="image-20230101230649311" 
         style="zoom:100%;" />
    <br> <!--换行-->
    表5.4 天气示例的计算转换概率 <!--标题-->
    </center>
</div>

例如，对于天气示例，我们有以下四个观测序列（$q_1$=晴朗，$q_2$=多云，$q_3$=下雨）：
$$
q_2, q_2, q_3, q_3, q_3, q_3, q_1\\
q_1, q_3, q_2, q_3, q_3, q_3, q_3\\
q_3, q_3, q_2, q_2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \\
q_2, q_1, q_2, q_2, q_1, q_3, q_1\\
$$
给定这四个序列，可以如表5.3和5.4所示估计相应的参数。

##### 5.2.2收敛(Convergence)

另外一个有趣的问题是：如果一个序列从一个状态转换到另一个状态的次数很多，M，极限中的概率是多少（$M→ ∞$）每一个状态，$q_i$？

给定初始概率向量 $∏$ 和转移矩阵 $A$，$M$ 次迭代后每个状态的概率$P=\{p_1,p_2,…,p_n\}$为：
$$
P = π A^M \tag{5.7}
$$
当 $M→ ∞$ ? 该解由Perron Frobenius定理给出，该定理表示当满足以下两个条件时：

1. 不可约性（Irreducibility）：从每一个状态 $i$ ，都有一个概率$a_{ij}＞0$过渡到任何状态 $j$ 。

2. 非周期性（Aperiodicity）：链不形成循环（链在过渡到这些状态之一后仍保持的状态子集）。

然后作为 $M→ ∞$ , 链收敛到不变分布$P$，使得$P×A＝P$，其中$A$是转移概率矩阵。收敛速度由矩阵$A$的第二特征值决定。

例如，考虑具有三个状态的MC和以下转移概率矩阵：
$$
A= \left[
\begin{matrix}
0.9 & 0.075 & 0.025 \\
0.15 & 0.8 & 0.05 \\
0.25 & 0.25 & 0.5\\
\end{matrix}
\right]
$$
可以看出，在这种情况下，稳态概率收敛到$P＝\{0.625，0.3125，0.0625\}$。

应用程序部分介绍了马尔可夫链的这种收敛特性在网页排名中的有趣应用。接下来我们讨论隐马尔可夫模型。

#### 5.3隐马尔可夫模型( Hidden Markov Models)

隐马尔可夫模型（HMM）是状态不可直接观察的马尔可夫链。例如，如果我们考虑天气示例，则无法直接测量天气；实际上，天气是基于一系列传感器（温度、压力、风速等）来估计的。因此，在这种情况下，与许多其他现象一样，状态是无法直接观察到的，HMM提供了一种更合适、更强大的建模工具。关于HMM的另一种思考方式是，它是一个双重随机过程：（i）一个我们无法直接观察到的隐藏随机过程，（ii）第二个随机过程，在给定第一个过程的情况下产生观察序列。

例如，假设我们有两个不公平或“有偏见”的硬币，M1和M2。M1具有较高的头部概率，而M2具有较高的尾部概率。有人依次翻转这两个硬币，然而，我们不知道是哪一个。我们只能观察结果：
$$
H, T, T, H, T, H, H, H, T, H, T, H, T, T, T, H, T, H, H, . . .
$$
假设掷硬币的人选择序列中的第一个硬币（先前概率），而给定先前的一个硬币（转移概率），下一个硬币将以相等的概率被掷。除了状态的先验概率和转移概率（如MC），在HMM中，我们需要指定观察概率，在这种情况下，给定每个硬币（状态）的H或T的概率。让我们假设M1的头部概率为80%，M2的尾部概率为80%。然后，我们为这个简单的示例指定了所有必需的参数，汇总在表5.5中。

硬币示例的状态图如图5.3所示，具有两个状态变量和两个可能的观察值，这取决于状态。

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101233054038.png" 
         alt="image-20230101233054038" 
         style="zoom:100%;" />
    <br> <!--换行-->
    表5.5不公平硬币示例的先验概率（∏）、转移概率（A）和观测概率（B） <!--标题-->
    </center>
</div>

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101233314510.png" 
         alt="image-20230101233314510" 
         style="zoom:100%;" />
    <br> <!--换行-->
    图5.3 HMM硬币示例的状态图。图中显示了两个状态q1和q2以及两个观测值H和T，圆弧表示跃迁和观测概率 <!--标题-->
    </center>
</div>

形式上，隐马尔可夫模型定义为

**状态集（Set of states）**：$Q = \{q_1, q_2, . . . , q_n \}$

**观察集（Set of observations）**：$O = \{o_1, o_2, . . . , o_m \}$

**先验概率向量（Vector of prior probabilities）**：$Π = \{π_1, π_2, . . . , π_n \}$， 当$π_i = P(S_0 = q_i )$

**转移概率矩阵（Matrix of transition probabilities）**：

$A = \{a_{ij} \}, i = [1..n], j = [1..n]$，当 $a_{ij} =
P(S_t = q_j | S_{t−1} = q_i )$

**观测概率矩阵（Matrix of observation probabilities）**：$B = \{b_{ij} \}, i = [1..n], j = [1..m]$，当 $b_{ik} =
P(O_t = o_k | S_{t} = q_i )$

其中$n$是状态数，$m$是观察数；$S_0$是初始状态。紧凑地，HMM表示为$λ=\{A，B，∏\}$。

（一阶）HMM满足以下性质：

马尔可夫性质：$P(S_t=q_j | S_{t−1}=q_i，S_{t−2}=q_k，…)=P(S_t=q_j\  |\  S_{t−1}=q_i)$

平稳过程：$P(S_{t−1}＝q_j \  |\ S_{t−2}＝q_i)＝P(S_{t}=q_j\  |\  S_{t−1}＝q_i)$和$P(O_{t−1}＝o_k \  |\  S_{t−1}=q_j)＝P(O_{t}＝o_k \  |\  S_{t}=q_j), ∀(t)$

观察的独立性：$P(O_{t}＝o_k \  |\  S_{t}=q_i,S_{t-1}=q_j,…)=P(O_t=o_k \  |\  S_t=q_i)$

与MC的情况一样，马尔可夫性质意味着当前状态的概率仅取决于先前状态，并且与历史的其余部分无关。第二个属性表示跃迁和观测概率不会随时间变化，即过程是平稳的。第三个属性指定观察值仅取决于当前状态。对基本HMM的扩展放宽了某些假设；其中一些将在下一节和后续章节中讨论。

根据前面的属性，HMM的图形模型如图5.4所示，其中包括两系列随机变量，即时间$t$，$S_t$时的状态和时间$t$，$O_t$时的观测值

<div> <!--块级封装-->
    <center> <!--将图片和文字局中-->
    <img src="C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230101234300752.png" 
         alt="image-20230101234300752" 
         style="zoom:100%;" />
    <br> <!--换行-->
    图5.4 表示隐藏马尔可夫模型的图形模型 <!--标题-->
    </center>
</div>

给定某个域的HMM表示，大多数应用程序中有三个基本问题感兴趣[7]：

1. 评估：给定一个模型，估计一系列观测的概率。
2. 最优序列：给定模型和特定观测序列，估计产生观测的最可能状态序列。
3. 参数学习：给定一系列观察结果，调整模型的参数。

接下来将描述解决这些问题的算法，假设标准HMM具有有限数量的状态和观测值。

##### 5.3.1评估

评估包括在给定模型λ的情况下，确定观测序列$O = \{o_1, o_2, o_3, . . .  \}$的概率，即估计$P(O\ |\ λ)$。我们提出了两种方法。首先，我们提出了直接方法，这是一种天真的算法，激发了对更高效算法的需求，然后对其进行了描述。

###### 5.3.1.1直接法

观测序列$O = \{o_1, o_2, o_3, . . . ,o_T \}$可以由不同的状态序列$Q_i$生成，因为HMM的状态是未知的。因此，为了计算观测序列的概率，我们可以对某个状态序列进行估计，然后将所有可能状态序列的概率相加：
$$
P(O\  |\  λ) = ∑_i {P(O, Q_i \ |\  λ)} \tag{5.8}
$$
为了获得$P(O, Q_i \ |\  λ)$，我们简单地将初始状态的概率$q_1$乘以状态序列的转移概率$q_1,q_2$。以及观测序列的观测概率$o_1.o_2.…$：

###### 5.3.1.2迭代法

##### 5.3.2 State Estimation

##### 5.3.3 Learning

##### 5.3.4 Extensions

#### 5.5附加阅读

[4]中提供了马尔可夫链的一般介绍。Rabiner[7]介绍了HMM及其在语音识别中的应用；[8] 提供了语音识别的更一般概述。[2]对手势识别环境中HMM的几种扩展进行了综述。参考文献[5]分析了搜索引擎和PageRank算法。[1，9]描述了HMM在手势识别中的应用。[3]中提供了HMM的开放软件。
