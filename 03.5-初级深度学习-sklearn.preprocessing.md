## Exercise: Stochastic Gradient Descent

介绍 

在本练习中，您将在 Fuel Economy 数据集上训练神经网络，然后探索学习率和批量大小对 SGD 的影响。 

```python
# Setup plotting
import matplotlib.pyplot as plt
from learntools.deep_learning_intro.dltools import animate_sgd
plt.style.use('seaborn-whitegrid')
# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
#动画
plt.rc('animation', html='html5')

# Setup feedback system
from learntools.core import binder
binder.bind(globals())
from learntools.deep_learning_intro.ex3 import *
```

在燃油经济性数据集中，您的任务是根据发动机类型或制造年份等特征预测汽车的燃油经济性。 

首先通过运行下面的单元格来加载数据集。

```python
import numpy as np
import pandas as pd
#scikit-learn中的数据预处理
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer, make_column_selector
from sklearn.model_selection import train_test_split

fuel = pd.read_csv('../input/dl-course-data/fuel.csv')

X = fuel.copy()
# Remove target
y = X.pop('FE')

preprocessor = make_column_transformer(
    (StandardScaler(),
    make_column_selector(dtype_include=np.number)),
    (OneHotEncoder(sparse=False),
     make_column_selector(dtype_include=object)),
)

X = preprocessor.fit_transform(X)
y = np.log(y) # log transform target instead of standardizing

input_shape = [X.shape[1]]
print("Input shape: {}".format(input_shape))
```

### 数据预处理

数据预处理的工具有许多，在我看来主要有两种：

- `pandas`数据预处理-[Pandas数据处理与分析](https://blog.csdn.net/qq_40195360/article/details/84570503?spm=1001.2014.3001.5502)

- `scikit-learn`中的`sklearn.preprocessing`数据预处理。-[数据预处理（`sklearn.preprocessing`）](https://blog.csdn.net/qq_40195360/article/details/88378248)

此处，主要介绍`sklearn.preprocessing`。

1. #### 标准化

   1. `StandardScaler`

      将特征数据的分布调整成标准正太分布，也叫高斯分布，也就是使得数据的均值为0，方差为1.

   2. `MinMaxScaler`

      最小-最大规范化，对原始数据进行线性变换，变换到[0,1]区间（也可以是其他固定最小最大值的区间）每个特征中的最小值变成了0，最大值变成了1.

   3. `MaxAbsScaler`

      与上面的很像，只是数据会被规模化到[-1,1]之间。也就是特征中，所有数据都会除以最大值。这个方法对那些已经中心化均值为0，或者稀疏的数据有意义，后者**不会改变矩阵的稀疏性**，是0的还是0，而前者会改变。

   4. `RobustScaler`

      根据四分位数来缩放数据。对于数据**有较多异常值**的情况，使用均值和方差来标准化显然不合适，按中位数，一、四分位数缩放效果要好。

   5. 补充：

      - 大多数机器学习算法中，会选择`StandardScaler`来进行特征缩放，因为<u>**`MinMaxScaler`对异常值非常敏感**</u>。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，`StandardScaler`往往是最好的选择。
      - `MinMaxScaler`在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像处理中**量化像素强度**时，都会使用`MinMaxScaler`将数据压缩于[0,1]区间之中。

   ***

2. #### 非线性变换

   1. `QuantileTransformer`

      使用**百分位数**转换特征，通过缩小边缘异常值和非异常值之间的距离来提供特征的非线性变换。

   2. `PowerTransformer`

      映射到高斯分布。在许多建模场景中，数据集中的特性是正常的。幂变换是一类参数的单调变换，其目的是将数据从任意分布映射到尽可能接近高斯分布，以稳定方差和最小化偏度。

      `PowerTransformer`目前提供了两个这样的幂变换，**Yeo-Johnson变换**和**Box-Cox变换**，利用极大似然估计了稳定方差和最小偏度的最优参数。并且，box-Cox要求输入数据严格为***正数据***，而Yeo-Johnson支持***正负数据***。

      ***`PowerTransformer`只有在0.20.0以后的版本才有***

   ***

3. #### 归一化（ Normalizer）

   归一化的目的：

   - 加快了梯度下降求最优解的速度
   - 有可能提高精度

   公式：对于整数p>1，
   $$
   Lp norm = \sum{(|vector|^p)(\frac{1}{p})}
   $$
   归一化是缩放单个样本以具有单位范数的过程，这里的”范数”，可以使用L1或L2范数。如果你计划使用二次形式(如点积或任何其他核函数)来量化任何样本间的相似度，则此过程将非常有用。

   这个观点基于 ***向量空间模型(Vector Space Model)***，经常在**文本分类**和**内容聚类**中使用。

   ```python
   sklearn.preprocessing.Normalizer(norm='l2',copy = True)
   #norm : ‘l1’, ‘l2’, or ‘max’, optional (‘l2’ by default)
   ```

   **与正则化（Regularization）的区别**：

   目的：防止模型出现过拟合，导致模型“泛化”能力太差。

   正则化-规则化-给需要训练的目标函数加上一些规则限制。

   [【直观详解】什么是正则化](https://blog.csdn.net/kdongyi/article/details/83932945)

   [【深度学习概念区分】Normalization vs. Regularization](https://zhuanlan.zhihu.com/p/477747129)

   **L1正则化(lasso)**：
   $$
   C=C_{0}+\frac{\lambda}{n}\sum{|w|}
   $$
   其中C0是代价函数，$\frac{\lambda}{n}\sum{|w|}$ 是L1正则项，$\lambda$是正则化参数。

   **L2正则化(ridge)：**（待添加：权值衰减引入）
   $$
   C=C_{0}+\frac{\lambda}{2n}\sum{w^2}
   $$
   其中，$\frac{\lambda}{2n}\sum{w^2}$ 是L2正则项，$\lambda$是正则化参数。

   **ElasticNet 正则化:**
   $$
   C=C_{0}+\frac{\lambda}{n}{(p\sum_{i-1}^{n}{|\theta_i|}+(1-p)\sum_{i-1}^{n}{\theta_i^2})}
   $$
   

   

   **范数（norm）**的概念来源于**泛函分析与测度理论**，wiki中的定义相当简单明了：**范数是具有“长度”概念的函数**，用于衡量一个**矢量的大小**（测量矢量的测度）

   ***

4. #### 编码分类特征

   **目的：将文字型数据转换为数值型**

   在机器学习中，大多数算法，譬如逻辑回归，支持向量机SVM，k近邻算法等都只能够处理数值型数据，不能处理文字，在sklearn当中，***除了专用来处理文字的算法，其他算法在fit的时候全部要求输入数组或矩阵***，也不能够导入文字型数据（其实手写决策树和朴素贝叶斯可以处理文字，但是sklearn中规定必须导入数值型）。

   然而在现实中，许多标签和特征在数据收集完毕的时候，都不是以数字来表现的。比如说，学历的取值可以是[“小学”，“初中”，“高中”，“大学”]，付费方式可能包含[“支付宝”，“现金”，“微信”]等等。在这种情况下，为了让数据适应算法和库，我们必须将数据进行编码，即是说，**将文字型数据转换为数值型**。

   1. `OrdinalEncoder`

      特征专用，能够将分类特征转换为分类数值。

      用来处理**有序变量**。

      ```python
      import pandas as pd
      from sklearn.preprocessing import OrdinalEncoder
      X = pd.DataFrame(['male','high'],
                      ['female','high'],
                      ['male','low'],
                      ['female','medium'],
                      ['female','medium'],
                      ['female','low'],
                      columns = ['sex','income'])
      X = OrdinalEncoder().fit_transform(X)
      ```

      

   2. `OneHotEncoder`

      类别`OrdinalEncoder`可以用来处理有序变量，但对于**名义变量**，我们只有使用哑变量的方式来处理，才能够尽量向算法传达最准确的信息。

      ```python
      import pandas as pd
      from sklearn.preprocessing import OneHotEncoder
      X = pd.DataFrame(['male','female','male','female','female','female'],columns=['sex'])
      X = OneHotEncoder().fit_transform(X).toarray()
      ```

      

   3. `LabelEncoder`

      **标签专用**，能够将分类转换为分类数值。

      ```python
      import pandas as pd
      from sklearn.preprocessing import labelEncoder
      y = pd.DataFrame(['yes','yes','yes','no','no','no'],columns = ['target'])
      label = labelEncoder().fit_transform(y)
      label
      ```

      

   ***

5. #### 离散化

   离散化(也称为量化或绑定)提供了一种将连续特征划分为离散值的方法。某些具有连续特征的数据集可能受益于离散化，因为离散化可以将连续属性的数据集转换为仅具有名义属性的数据集。

   1. `KBinsDiscretizer`

      k个等宽箱的离散化特征，默认情况下，输出是one-hot编码成稀疏矩阵，并且可以使用encode参数。对于每个特性，在fit再加上分箱的数量，他们会定义间隔。

   2. `Binarizer`

      根据阈值将数据二值化（将特征值设置为0或1），用于处理连续型变量。大于阈值的值映射为1，而小于或等于阈值的值映射为0。默认阈值为0时，特征中所有的正值都映射到1。二值化是对文本计数数据的常见操作，分析人员可以决定仅考虑某种现象的存在与否。

   ***

6. #### 缺失值处理(`Imputer`)

   ***

7. #### 生成多项式特征(`PolynomialFeatures`)

   在机器学习中，通过增加一些输入数据的非线性特征来增加模型的复杂度通常是有效的。一个简单通用的办法是使用多项式特征，这可以获得特征的更高维度和互相间关系的项。多项式特征经常用于使用多项式核函数的核方法（比如SVC和KernelPCA）。

   ***

8. #### 自定义转换器(`FunctionTransformer`)

   我们经常希望将一个Python的函数转变为transformer，用于数据清洗和预处理。可以使用`FunctionTransformer`方法将任意函数转化为一个Transformer。

对于新的知识个人建议去scikit-learn官网中进行查找阅读。
