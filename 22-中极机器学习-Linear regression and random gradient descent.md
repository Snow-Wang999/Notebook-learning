# 21-çº¿æ€§å›å½’å’Œéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆLinear regression and random gradient descentï¼‰

**åœ¨ Python 3.6 ä¸­æµ‹è¯•æ­£ç¡®æ€§:**

- numpy 1.15.4
- pandas 0.23.4
- ä¸ä¸€å®šè¦ä¸€è‡´ï¼Œèƒ½è¿è¡Œå³å¯

æ‚¨å°†æ ¹æ®å…¬å¸åœ¨ç”µè§†ã€æŠ¥çº¸å’Œå¹¿æ’­å¹¿å‘Šæ–¹é¢çš„æŠ•èµ„æ¥é¢„æµ‹å…¬å¸çš„æ”¶å…¥ã€‚

ä½ å°†å­¦ä¹ ï¼š

- è§£å†³çº¿æ€§å›å½’æ¢å¤é—®é¢˜
- å®ç°éšæœºæ¢¯åº¦ä¸‹é™æ¥è°ƒæ•´å®ƒ
- è§£æåœ°è§£å†³çº¿æ€§å›å½’é—®é¢˜

## ä»‹ç»

### çº¿æ€§å›å½’ï¼ˆLinear regressionï¼‰

çº¿æ€§å›å½’æ˜¯ç ”ç©¶æœ€å……åˆ†çš„æœºå™¨å­¦ä¹ æ–¹æ³•ä¹‹ä¸€ï¼Œå®ƒå…è®¸æ‚¨å°†**å®šé‡ç‰¹å¾çš„å€¼é¢„æµ‹ä¸ºå…¶ä»–ç‰¹å¾**ä¸å‚æ•°ï¼ˆæ¨¡å‹æƒé‡ï¼‰çš„çº¿æ€§ç»„åˆã€‚æœ€ä½³ï¼ˆåœ¨æŸäº›è¯¯å·®å‡½æ•°çš„æœ€å°æ„ä¹‰ä¸Šï¼‰çº¿æ€§å›å½’å‚æ•°å¯ä»¥ä½¿ç”¨**æ­£è§„æ–¹ç¨‹**è¿›è¡Œåˆ†ææˆ–ä½¿ç”¨**ä¼˜åŒ–æ–¹æ³•**åœ¨æ•°å€¼ä¸Šæ‰¾åˆ°ã€‚

çº¿æ€§å›å½’ä½¿ç”¨ä¸€ä¸ªç®€å•çš„è´¨é‡å‡½æ•°â€”â€”**æ ‡å‡†è¯¯å·®**ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŒ…å« 3 ä¸ªç‰¹å¾çš„æ ·æœ¬ã€‚è°ƒæ•´æ¨¡å‹çš„å‚æ•°ï¼ˆæƒé‡ï¼‰ï¼Œè§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
$$
\Large \frac{1}{\ell}\sum_{i=1}^\ell{{((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}^2} \rightarrow \min_{w_0, w_1, w_2, w_3},
$$

$$
å…¶ä¸­x_{i1}, x_{i2}, x_{i3}æ˜¯ç¬¬ i-Ğ³Ğ¾ä¸ªå¯¹è±¡çš„ç‰¹å¾å€¼ï¼Œy_iæ˜¯ç¬¬ i-Ğ³Ğ¾ä¸ªå¯¹è±¡çš„ç›®æ ‡ç‰¹å¾å€¼ï¼Œ\ellæ˜¯è®­ç»ƒé›†ä¸­çš„å¯¹è±¡ä¸ªæ•°ã€‚
$$

### æ¢¯åº¦ä¸‹é™ï¼ˆgradient descentï¼‰

$$
å‚æ•° w_0, w_1, w_2, w_3 å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™åœ¨æ•°å€¼ä¸Šæ‰¾åˆ°å‡æ–¹æ ¹è¯¯å·®æœ€å°åŒ–çš„å‚æ•°ã€‚
$$

æƒé‡çš„æ¢¯åº¦æ­¥éª¤å¦‚ä¸‹æ‰€ç¤ºï¼š
$$
\Large w_0 \leftarrow w_0 - \frac{2\eta}{\ell} \sum_{i=1}^\ell{{((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}}
$$

$$
\Large w_j \leftarrow w_j - \frac{2\eta}{\ell} \sum_{i=1}^\ell{{x_{ij}((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}},\ j \in \{1,2,3\}
$$

è¿™é‡Œğœ‚æ˜¯ä¸€ä¸ªå‚æ•°ï¼Œæ˜¯æ¢¯åº¦ä¸‹é™æ­¥æ•°ã€‚

### éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆrandom gradient descentï¼‰

å¦‚ä¸Šæ‰€è¿°ï¼Œæ¢¯åº¦ä¸‹é™çš„é—®é¢˜åœ¨äºï¼Œåœ¨å¤§æ ·æœ¬ä¸Šï¼Œåœ¨æ¯ä¸€æ­¥è®¡ç®—æ‰€æœ‰å¯ç”¨æ•°æ®çš„æ¢¯åº¦å¯èƒ½åœ¨è®¡ç®—ä¸Šéå¸¸å›°éš¾ã€‚

åœ¨æ¢¯åº¦ä¸‹é™çš„éšæœºå˜ä½“ä¸­ï¼Œä»…è€ƒè™‘è®­ç»ƒæ ·æœ¬çš„ä¸€ä¸ªéšæœºå¯¹è±¡æ¥è®¡ç®—æƒé‡çš„æ ¡æ­£ï¼š
$$
\Large w_0 \leftarrow w_0 - \frac{2\eta}{\ell} {((w_0 + w_1x_{k1} + w_2x_{k2} +  w_3x_{k3}) - y_k)}
$$

$$
\Large w_j \leftarrow w_j - \frac{2\eta}{\ell} {x_{kj}((w_0 + w_1x_{k1} + w_2x_{k2} +  w_3x_{k3}) - y_k)},\ j \in \{1,2,3\},
$$

$$
å…¶ä¸­ k- éšæœºç´¢å¼•, k \in \{1, \ldots, \ell\}.
$$

### æ­£è§„æ–¹ç¨‹ï¼ˆnormal_equationï¼‰

æ‰¾åˆ°æœ€ä½³æƒé‡å‘é‡ ğ‘¤ ä¹Ÿå¯ä»¥é€šè¿‡è§£ææ¥å®Œæˆã€‚æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°è¿™æ ·ä¸€ä¸ªæƒé‡å‘é‡ ğ‘¤ ï¼Œä»¥ä¾¿é€šè¿‡å°†çŸ©é˜µ ğ‘‹ï¼ˆç”±è®­ç»ƒæ ·æœ¬å¯¹è±¡çš„é™¤ç›®æ ‡å¯¹è±¡ä¹‹å¤–çš„æ‰€æœ‰ç‰¹å¾ç»„æˆï¼‰ä¹˜ä»¥æƒé‡å‘é‡ ğ‘¤ æ¥è·å¾—é€¼è¿‘ç›®æ ‡ç‰¹å¾çš„å‘é‡ ğ‘¦ã€‚å³æ»¡è¶³çŸ©é˜µæ–¹ç¨‹ï¼š
$$
\Large y = Xw
$$
åœ¨å·¦è¾¹ä¹˜ä»¥ $X^T$ ï¼Œ
$$
\Large X^Ty = X^TXw
$$
è¿™å¾ˆå¥½ï¼Œå› ä¸ºç°åœ¨çŸ©é˜µğ‘‹ğ‘‡ğ‘‹ æ˜¯æ­£æ–¹å½¢çš„ï¼Œå¹¶ä¸”å¯ä»¥æ‰¾åˆ°è§£ï¼ˆå‘é‡ğ‘¤ï¼‰ï¼š
$$
\Large w = {(X^TX)}^{-1}X^Ty
$$
$ (ğ‘‹ğ‘‡ğ‘‹)âˆ’1ğ‘‹ğ‘‡ $ - çŸ©é˜µXçš„ä¼ªé€†. åœ¨ NumPy ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å‡½æ•°è®¡ç®—è¿™æ ·çš„çŸ©é˜µ[numpy.linalg.pinv](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.pinv.html).

ç„¶è€Œï¼Œåœ¨çŸ©é˜µğ‘‹ï¼ˆå¤šé‡å…±çº¿æ€§é—®é¢˜ï¼‰çš„è¡Œåˆ—å¼å¾ˆå°çš„æƒ…å†µä¸‹ï¼Œæ±‚ä¼ªé€†çŸ©é˜µæ˜¯ä¸€ä¸ªè®¡ç®—å¤æ‚ä¸”ä¸ç¨³å®šçš„æ“ä½œã€‚åœ¨å®è·µä¸­ï¼Œæœ€å¥½é€šè¿‡æ±‚è§£çŸ©é˜µæ–¹ç¨‹æ¥æ‰¾åˆ°æƒå‘é‡ğ‘¤
$$
\Large X^TXw = X^Ty
$$
è¿™å¯ä»¥é€šè¿‡ [numpy.linalg.solve](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.solve.html) å‡½æ•°æ¥å®Œæˆã€‚

ä½†å®é™…ä¸Šï¼Œå¯¹äºå¤§å‹çŸ©é˜µğ‘‹ï¼Œæ¢¯åº¦ä¸‹é™çš„å·¥ä½œé€Ÿåº¦æ›´å¿«ï¼Œå°¤å…¶æ˜¯å®ƒçš„éšæœºç‰ˆæœ¬ã€‚

## æ‰§è¡Œè¯´æ˜

[**task_1**](https://github.com/RBVV23/Coursera/blob/21fa00d145e3ec0e26c1a617091000b7e9548e00/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BD%D0%B0%20%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%87%D0%B5%D0%BD%D0%BD%D1%8B%D1%85%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85/Week_1/Project_1/task_1.py)

[ç¬¬ 2 è¯¾ã€‚ç‰¹å¾ç¼©æ”¾ã€‚æ­£åˆ™åŒ–ã€‚éšæœºæ¢¯åº¦ä¸‹é™ã€‚](https://github.com/mahhets/Data_analysis_algs/blob/21d58a6884af1c7de7f900ca164fd5a5de897686/2_Scalers_L1_L2_StochasticGD/Lesson_2.ipynb)

### 1. åŠ è½½æ•°æ®

å°† ads.csv æ–‡ä»¶ä¸­çš„æ•°æ®åŠ è½½åˆ° pandas DataFrame å¯¹è±¡ä¸­ã€‚[æ•°æ®æº](http://www-bcf.usc.edu/~gareth/ISL/data.html)ã€‚

```python
import pandas as pd
adver_data = pd.read_csv('../input/advertising/advertising.csv')
adver_data.head(5)
```

![image-20221101125653822](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101125653822.png)

```python
adver_data.describe()
```

![image-20221101125707388](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101125707388.png)

#### åˆ›å»ºnumpyå’Œpandasæ•°æ®

ä» TVã€Radio å’Œ Newspaper åˆ—åˆ›å»º NumPy æ•°ç»„ Xï¼Œä» Sales åˆ—åˆ›å»º yã€‚ä½¿ç”¨ pandas DataFrame å¯¹è±¡çš„ values å±æ€§ã€‚

```python
#X = adver_data[['TV','Radio','Newspaper']].values
#y = adver_data[['Sales']].values
#print (X[0])
#print (y[0])
```

```python
#X = adver_data.drop(columns=['Sales']).values
#y = adver_data["Sales"].values
# X choose first three lists and y choose last list
X_origin,y = adver_data.iloc[:,0:3].values, adver_data.iloc[:,3:].values
print("scale of features:",X_origin.shape)
print("scale of labels:",y.shape)
```

```
scale of features: (200, 3)
scale of labels: (200, 1)
```

#### è®¡ç®—meanå’Œstd

é€šè¿‡ä»æ¯ä¸ªå€¼ä¸­å‡å»ç›¸åº”åˆ—çš„å¹³å‡å€¼å¹¶å°†ç»“æœé™¤ä»¥æ ‡å‡†åå·®æ¥ç¼©æ”¾ X çŸ©é˜µçš„åˆ—ã€‚

ä¸ºäº†å…·ä½“èµ·è§ï¼Œè¯·ä½¿ç”¨ NumPy å‘é‡çš„å‡å€¼meanå’Œæ ‡å‡†å·®stdæ–¹æ³•ï¼ˆæ ‡å‡†çš„ç†ŠçŒ«å®ç°å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼‰ã€‚

è¯·æ³¨æ„ï¼Œåœ¨ numpy ä¸­ï¼Œè°ƒç”¨ä¸å¸¦å‚æ•°çš„ .mean() å‡½æ•°ä¼šè¿”å›æ•°ç»„æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼ï¼Œè€Œä¸æ˜¯ pandas ä¸­åˆ—çš„å¹³å‡å€¼ã€‚è¦æŒ‰åˆ—è®¡ç®—ï¼Œæ‚¨å¿…é¡»æŒ‡å®šè½´å‚æ•°ã€‚

```python
#means, stds = np.mean(X,axis=0),np.std(X,axis=0)
#X =  (X - means)/stds
```

```python
# calculate  means and stds
data_nolabel = adver_data.drop(columns=['Sales'])
means, stds = data_nolabel.mean(axis = 0, skipna = True),data_nolabel.std(axis = 0, skipna = True)
print("means:",means)
print("stds:",stds)
```

```
means: TV           147.0425
Radio         23.2640
Newspaper     30.5540
dtype: float64
stds: TV           85.854236
Radio        14.846809
Newspaper    21.778621
dtype: float64
```

```python
X_origin = (X_origin - means.values)/stds.values
```

another way

```python
# è·å–å„åˆ—çš„å‡å€¼å’Œæ ‡å‡†å·®
means = np.mean(X_origin, axis=0)
stds = np.std(X_origin, axis=0)
print("means:",means)
print("stds:",stds)
# axiså‚æ•°æŒ‡å®šæŒ‰åˆ—è®¡ç®—å€¼ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°ç»„
#ï¼ˆå‚è§æºä»£ç éƒ¨åˆ†çš„æ–‡æ¡£ï¼‰
# ä»å‡å€¼ä¸­å‡å»æ¯ä¸ªç‰¹å¾å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®
for i in range(X_origin.shape[0]):
    for j in range(X_origin.shape[1]):
        X_origin[i][j] = (X_origin[i][j] - means[j])/stds[j]

```

```
means: [147.0425  23.264   30.554 ]
stds: [85.63933176 14.80964564 21.72410606]
```



#### æ·»åŠ å•ä½å‘é‡åˆ—

ä½¿ç”¨ hstackã€ones å’Œ reshape NumPy æ–¹æ³•å‘ X çŸ©é˜µæ·»åŠ ä¸€åˆ—ã€‚ä¸ºäº†ä¸å•ç‹¬å¤„ç†çº¿æ€§å›å½’çš„ç³»æ•°ğ‘¤0ï¼Œéœ€è¦ä¸€ä¸ªå•ä½å‘é‡ã€‚

```python
import numpy as np
n,m = X_origin.shape
X_1 = np.ones((n,1))
print(X_1)
X = np.hstack((X_1,X_origin))
print(X)
print("scale of features:",X.shape)
```

![image-20221101130130392](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101130130392.png)

![image-20221101150937478](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101150937478.png)

```
scale of features: (200, 4)
```

### 2.å®ç°å‡½æ•°mserror

[ä¸€ç§æ›´ç®€å•çš„æ±‚æœ€å°å¹³æ–¹å‡å€¼å‡½æ•°ï¼ˆMSE)çš„æ–¹æ³• -- æ¢¯åº¦ä¸‹é™æ³•ã€‚](https://blog.csdn.net/weixin_42342803/article/details/81366699)

é¢„æµ‹çš„å‡æ–¹æ ¹è¯¯å·®ã€‚å®ƒæœ‰ä¸¤ä¸ªå‚æ•° - ç³»åˆ—å¯¹è±¡ yï¼ˆç›®æ ‡ç‰¹å¾å€¼ï¼‰å’Œ y_predï¼ˆé¢„æµ‹å€¼ï¼‰ã€‚ä¸è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­ä½¿ç”¨å¾ªç¯â€”â€”é‚£ä¹ˆå®ƒçš„è®¡ç®—æ•ˆç‡ä¼šå¾ˆä½ã€‚

```python
'''
#ä¹Ÿå¯è¡Œ
def mserror(y, y_pred):
   #return np.sqrt(((y_pred - y) ** 2).mean())
   #return(sum((y - y_pred)**2)[0])/float(y.shape[0])
   return sum((y - y_pred)**2,0)/y.shape[0]
'''
# y.shape[0] è¡Œæ•°
```

```python
#æ­¤å¤„ä¸ç”¨
def mserror_1(X, w, y_pred):
    y = X.dot(w)
    return (sum((y - y_pred)**2)) / len(y)
```

```python
def mserror_2(y, y_pred):
    y = np.array(y)
    y_pred = np.array(y_pred)
    return np.mean((y - y_pred)**2)
```

å¦‚æœæ€»æ˜¯é¢„æµ‹åŸå§‹æ ·æœ¬çš„ Sales ä¸­å€¼ï¼Œé‚£ä¹ˆé¢„æµ‹ Sales å€¼çš„æ ‡å‡†è¯¯æ˜¯å¤šå°‘ï¼Ÿç»“æœï¼Œå››èˆäº”å…¥åˆ°å°æ•°ç‚¹å 3 ä½ï¼Œæ˜¯â€œ1 ä¸ªä»»åŠ¡â€çš„ç­”æ¡ˆã€‚

median as y_pred

```python
'''ä¹Ÿå¯è¡Œ
eye = np.array([np.median(y)]*y.shape[0]).reshape((y.shape[0], 1))# the median of the y multiply the number of the y
answer1 = mserror(y, eye)
print(np.round(answer1, 3))
#write_answer_to_file(answer1, '1.txt')
'''
```

```python
N = X.shape[0]
med = np.median(np.array(adver_data['Sales']))
y_pred = np.ones((N))*med
y = np.array(adver_data['Sales'])
# print(y_pred, y)
answer1 =  mserror_2(y, y_pred)
print('\tanswer 1 = ', round(answer1, 3))
```

```
answer 1 =  28.346
```

### 3. å®ç° normal_equation å‡½æ•°

ç»™å®šçŸ©é˜µï¼ˆNumPy æ•°ç»„ï¼‰X å’Œ yï¼Œæ ¹æ®æ­£æ€çº¿æ€§å›å½’æ–¹ç¨‹è®¡ç®—æƒé‡å‘é‡ ğ‘¤ã€‚
$$
\large XÎ¸=Y (3.1)
$$
![image-20221101113822367](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101113822367.png)

![image-20221101113838920](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101113838920.png)

![image-20221101113849556](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101113849556.png)

![image-20221101113858982](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101113858982.png)

```python
#Least Squares
def normal_equation_1(X, y):
    X_t = X.transpose()
    X_obr = np.dot(X_t,X)
    X_obr = np.linalg.inv(X_obr)
    Sol = np.dot(X_obr, X_t)
    return np.dot(Sol, y)
```



```python
'''
#ä¹Ÿå¯è¡Œ
#Least Squares
def normal_equation(X, y):
    return np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y) 
	#np.dot(X.T, X)= Xçš„è½¬ç½®ä¸Xçš„ç‚¹ç§¯ = X^T*X
    #np.linalg.pinv(np.dot(X.T, X)) = Xçš„è½¬ç½®ä¸Xçš„ç‚¹ç§¯çš„ä¼ªé€†çŸ©é˜µ = (X^T*X)^(-1)
    #np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T) = (X^T*X)^(-1)*X^T
    #np.dot(np.dot(np.linalg.pinv(np.dot(X.T, X)), X.T), y) = ((X^T*X)^(-1)*X^T )*y=weight
'''
```

```python
norm_eq_weights_1= normal_equation_1(X, y)
print(norm_eq_weights_1)
```

```
[[14.0225    ]
 [ 3.91925365]
 [ 2.79206274]
 [-0.02253861]]
```

åœ¨ç”µè§†ã€å¹¿æ’­å’ŒæŠ¥çº¸å¹¿å‘Šçš„å¹³å‡æŠ•èµ„æƒ…å†µä¸‹ï¼Œä½¿ç”¨æ­£æ€æ–¹ç¨‹æ‰¾åˆ°æƒé‡çš„çº¿æ€§æ¨¡å‹é¢„æµ‹çš„é”€å”®é¢æ˜¯å¤šå°‘ï¼Ÿ ï¼ˆå³ï¼Œç¼©æ”¾çš„ TVã€Radio å’Œ Newspaper ç‰¹å¾çš„å€¼ä¸ºé›¶ï¼‰ã€‚å¾—åˆ°çš„ç»“æœï¼Œå››èˆäº”å…¥åˆ°å°æ•°ç‚¹å 3 ä½ï¼Œæ˜¯â€œ2 ä¸ªä»»åŠ¡â€çš„ç­”æ¡ˆã€‚

```python
X_0 = np.array([1, 0, 0, 0])
answer2 = np.dot(X_0,norm_eq_weights_1)
print('\tanswer 2 = ', np.round(answer2, 3))
```

```
answer 2 =  [14.022]
```

```python
'''ä¹Ÿå¯è¡Œ
answer2 = np.dot(np.mean(X, axis=0), norm_eq_weights)[0]#Xçš„æ¯è¡Œå¹³å‡å€¼ä¸æƒé‡è¿›è¡Œç‚¹ç§¯
print(np.round(answer2, 3))
'''
```

```
14.022
```

### 4.linear_prediction å‡½æ•°

ç¼–å†™ä¸€ä¸ª linear_prediction å‡½æ•°ï¼Œå®ƒä»¥çŸ©é˜µ X å’Œçº¿æ€§æ¨¡å‹çš„æƒé‡å‘é‡ w ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªé¢„æµ‹å‘é‡ä½œä¸ºçŸ©é˜µ X çš„åˆ—ä¸æƒé‡ w çš„çº¿æ€§ç»„åˆã€‚

```python
def linear_prediction(X, w):
    return np.dot(X, w)
```

ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ‰¾åˆ°æƒé‡çš„çº¿æ€§æ¨¡å‹é¢„æµ‹é”€å”®é¢çš„æ ‡å‡†è¯¯å·®æ˜¯å¤šå°‘ï¼Ÿç»“æœï¼Œå››èˆäº”å…¥åˆ°å°æ•°ç‚¹å 3 ä½ï¼Œæ˜¯â€œé—®é¢˜ 3â€çš„ç­”æ¡ˆã€‚

```python
y_pred = linear_prediction(X, norm_eq_weights_1)
```

```
answer3 = mserror_2(y,y_pred)
print('\tanswer 3 = ', np.round(answer3, 3))
```

```
answer 3 =  2.784
```

### 5.stochastic_gradient_step å‡½æ•°

ç¼–å†™ä¸€ä¸ª stochastic_gradient_step å‡½æ•°ï¼Œå®ç°çº¿æ€§å›å½’çš„éšæœºæ¢¯åº¦ä¸‹é™æ­¥éª¤ã€‚è¯¥å‡½æ•°å¿…é¡»æ¥å—ä¸€ä¸ªçŸ©é˜µ Xï¼Œå‘é‡ y å’Œ wï¼Œæ•°å­— train_ind æ˜¯è®­ç»ƒæ ·æœ¬å¯¹è±¡çš„ç´¢å¼•ï¼ˆçŸ©é˜µ X çš„è¡Œï¼‰ï¼Œé€šè¿‡å®ƒè®¡ç®—æƒé‡çš„å˜åŒ–ï¼Œæ•°å­— ğœ‚ (eta) æ˜¯æ¢¯åº¦ä¸‹é™æ­¥éª¤ï¼ˆé»˜è®¤ eta=0.01ï¼‰ã€‚ç»“æœå°†æ˜¯ä¸€ä¸ªæ›´æ–°æƒé‡çš„å‘é‡ã€‚æˆ‘ä»¬çš„å‡½æ•°å®ç°å°†é’ˆå¯¹å…·æœ‰ 3 ä¸ªç‰¹å¾çš„æ•°æ®æ˜¾å¼ç¼–å†™ï¼Œä½†æ˜¯å¯¹äºä»»æ„æ•°é‡çš„ç‰¹å¾å¾ˆå®¹æ˜“ä¿®æ”¹ï¼Œä½ å¯ä»¥åšåˆ°ã€‚

```python
def stochastic_gradient_step(X, y, w, train_ind, eta=0.01):
    res = w[0]*X[train_ind,0]+w[1]*X[train_ind,1]+w[2]*X[train_ind,2]+w[3]*X[train_ind,3]
    grad0 = X[train_ind,0]*(res-y[train_ind])
    grad1 = X[train_ind,1]*(res-y[train_ind])
    grad2 = X[train_ind,2]*(res-y[train_ind])
    grad3 = X[train_ind,3]*(res-y[train_ind])
    return  w - 2*eta * np.array([grad0, grad1, grad2, grad3])
```

æˆ–è€…

```python
def stochastic_gradient_step(X, y, w, train_ind, eta=0.01):
    res = X[train_ind][0]*w[0] + X[train_ind][1]*w[1] + X[train_ind][2]*w[2] + X[train_ind][3]*w[3]
    grad0 =  ( res - y[train_ind] ) * X[train_ind][0]
    grad1 =  ( res - y[train_ind] ) * X[train_ind][1]
    grad2 =  ( res - y[train_ind] ) * X[train_ind][2]
    grad3 =  ( res - y[train_ind] ) * X[train_ind][3]
    return  w - 2*eta * np.array([grad0, grad1, grad2, grad3])
```

### 6.æœ‰å‚æ•°çš„çº¿æ€§å›å½’çš„éšæœºæ¢¯åº¦ä¸‹é™

ç¼–å†™ä¸€ä¸ª stochastic_gradient_descent å‡½æ•°ï¼Œå®ç°çº¿æ€§å›å½’çš„éšæœºæ¢¯åº¦ä¸‹é™ã€‚è¯¥å‡½æ•°å°†ä»¥ä¸‹å‚æ•°ä½œä¸ºè¾“å…¥ï¼š

- X - å¯¹åº”äºè®­ç»ƒæ ·æœ¬çš„çŸ©é˜µ 
- y - ç›®æ ‡ç‰¹å¾çš„å€¼å‘é‡ 
- w_init - æ¨¡å‹åˆå§‹æƒé‡çš„å‘é‡ 
- eta - æ¢¯åº¦ä¸‹é™æ­¥éª¤ï¼ˆé»˜è®¤ 0.01ï¼‰ 
- max_iter - æ¢¯åº¦ä¸‹é™è¿­ä»£çš„æœ€å¤§æ¬¡æ•°ï¼ˆé»˜è®¤ 10000ï¼‰ 
- min_weight_dist - ç®—æ³•åœæ­¢è¿è¡Œçš„ç›¸é‚»æ¢¯åº¦ä¸‹é™è¿­ä»£ä¸­æƒé‡å‘é‡ä¹‹é—´çš„æœ€å¤§æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆé»˜è®¤ 1e-8ï¼‰ 
- seed - ç”¨äºç”Ÿæˆä¼ªéšæœºæ•°çš„å¯é‡å¤æ€§æ•°å­—ï¼ˆé»˜è®¤ 42ï¼‰ 
- verbose - ç”¨äºæ‰“å°ä¿¡æ¯çš„æ ‡å¿—ï¼ˆä¾‹å¦‚ï¼Œç”¨äºè°ƒè¯•ï¼Œé»˜è®¤ä¸º Falseï¼‰

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå‡æ–¹æ ¹è¯¯å·®çš„å½“å‰å€¼å¿…é¡»å†™å…¥å‘é‡ï¼ˆåˆ—è¡¨ï¼‰ã€‚è¯¥å‡½æ•°å¿…é¡»è¿”å›æƒé‡å‘é‡ ğ‘¤ ä»¥åŠé”™è¯¯å‘é‡ï¼ˆåˆ—è¡¨ï¼‰ã€‚

1e4=10000

```python
def stochastic_gradient_descent(X, y, w_init, eta=1e-2, max_iter=1e4,
                                min_weight_dist=1e-8, seed=42, verbose=False): 
    # åˆå§‹åŒ–ç›¸é‚»æƒé‡å‘é‡ä¹‹é—´çš„è·ç¦»
	# å¤§é‡è¿­ä»£ã€‚
    weight_dist = np.inf
    # åˆå§‹åŒ–æƒé‡å‘é‡
    w = w_init
    # åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®°å½•æ¯ä¸ªè¿­ä»£çš„é”™è¯¯
    errors = []
    # è¿­ä»£è®¡æ•°å™¨
    iter_num = 0
    # ç”Ÿæˆä¼ªéšæœºæ•°
	# ï¼ˆè¦æ›´æ”¹æƒé‡çš„å¯¹è±¡ç¼–å·ï¼‰
	# seedä½¿ç”¨æ­¤ä¼ªéšæœºæ•°åºåˆ—ã€‚
    np.random.seed(seed)
        
    # ä¸»å¾ªç¯
    while weight_dist > min_weight_dist and iter_num < max_iter:
        #åˆ¶é€ ä¼ªéšæœº
		#å­¦ä¹ æ ·æœ¬å¯¹è±¡ç´¢å¼•
        random_ind = np.random.randint(X.shape[0])
        
        # Ğ’Ğ°Ñˆ ĞºĞ¾Ğ´ Ğ·Ğ´ĞµÑÑŒ
        iter_num += 1
        # æ›´æ–°æƒé‡
        w_new = stochastic_gradient_step(X=X, y=y, w=w, train_ind=random_ind, eta=eta)
        #calculate distance between old weight and new weight
        weight_dist = (sum((w - w_new) ** 2)) ** 0.5
        # é¢„æµ‹å€¼ç­‰äºxä¹˜ä»¥w_new
        y_pred = linear_prediction(X, w_new)
        # è®¡ç®—å‡æ–¹å·®ï¼Œä¸è®°å½•
        error = mserror(y, y_pred)
        errors.append(error)
        w = w_new
        if (iter_num % 100) == 0 and verbose == True:
            print('iter_num = ', iter_num)
            print('\tweight_dist = ', weight_dist)
            print('\terror = ', error)
            print('\trandom_ind = ', random_ind)
    if verbose == True:
        print('w = ', w)
        print('errors[0] = ', errors[0])
        print('errors[-1] = ', errors[-1])
        print('mean.errors[-1] = ', np.mean(errors))

    return w, errors
```

è¿è¡Œ 10^5 æ¬¡éšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£ã€‚æŒ‡å®šç”±é›¶ç»„æˆçš„åˆå§‹ w_init æƒé‡å‘é‡ã€‚å°† eta å’Œ seed å‚æ•°ä¿ç•™ä¸ºé»˜è®¤å€¼ï¼ˆeta=0.01ï¼Œseed=42 - è¿™å¯¹äºæ£€æŸ¥ç­”æ¡ˆå¾ˆé‡è¦ï¼‰ã€‚

```python
w_init = np.array([0, 0, 0, 0])
%%time
stoch_grad_desc_weights, stoch_errors_by_iter = stochastic_gradient_descent(
    X, y, w_init, eta=0.01,
    max_iter=1e5, 
    min_weight_dist=1e-8,
    seed=42, 
    verbose=False)
```

```
CPU times: user 7.75 s, sys: 30.9 ms, total: 7.78 s
Wall time: 7.78 s
```

è®©æˆ‘ä»¬çœ‹çœ‹éšæœºæ¢¯åº¦ä¸‹é™çš„å‰ 50 æ¬¡è¿­ä»£çš„è¯¯å·®æ˜¯å¤šå°‘ã€‚æˆ‘ä»¬çœ‹åˆ°è¯¯å·®ä¸ä¸€å®šä¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­å‡å°‘ã€‚

```python
%pylab inline
plot(range(50), stoch_errors_by_iter[:50])
xlabel('Iteration number')
ylabel('MSE')
```

![image-20221101153146409](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101153146409.png)

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹éšæœºæ¢¯åº¦ä¸‹é™çš„ 10^5 æ¬¡è¿­ä»£çš„è¯¯å·®å¯¹è¿­ä»£æ¬¡æ•°çš„ä¾èµ–æ€§ã€‚æˆ‘ä»¬çœ‹åˆ°ç®—æ³•æ”¶æ•›äº†ã€‚

```python
%pylab inline
plot(range(1000), stoch_errors_by_iter[:1000])
xlabel('Iteration number')
ylabel('MSE')
```

![image-20221101153247956](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101153247956.png)

```python
%pylab inline
plot(range(len(stoch_errors_by_iter)), stoch_errors_by_iter)
xlabel('Iteration number')
ylabel('MSE')
```

![image-20221101153219463](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221101153219463.png)

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯¥æ–¹æ³•æ”¶æ•›åˆ°çš„æƒé‡å‘é‡ã€‚

```python
stoch_grad_desc_weights
```

```
array([[13.97836994, 13.97836994, 13.97836994, 13.97836994],
       [ 3.87934503,  3.87934503,  3.87934503,  3.87934503],
       [ 3.14134212,  3.14134212,  3.14134212,  3.14134212],
       [ 0.18323907,  0.18323907,  0.18323907,  0.18323907]])
```

è®©æˆ‘ä»¬çœ‹çœ‹æœ€åä¸€æ¬¡è¿­ä»£çš„å‡æ–¹è¯¯å·®ã€‚

```python
stoch_errors_by_iter[-1]
```

```
array([3.00045025, 3.00045025, 3.00045025, 3.00045025])
```

```python
print('stoch_grad_desc_weights = ', stoch_grad_desc_weights)
print('stoch_errors_by_iter[-1] = ', stoch_errors_by_iter[-1])
```

```
stoch_grad_desc_weights =  [[13.97836994 13.97836994 13.97836994 13.97836994]
 [ 3.87934503  3.87934503  3.87934503  3.87934503]
 [ 3.14134212  3.14134212  3.14134212  3.14134212]
 [ 0.18323907  0.18323907  0.18323907  0.18323907]]
stoch_errors_by_iter[-1] =  [3.00045025 3.00045025 3.00045025 3.00045025]
```

å°† Sales é¢„æµ‹ä¸ºä½¿ç”¨æ¢¯åº¦ä¸‹é™æ‰¾åˆ°æƒé‡çš„çº¿æ€§æ¨¡å‹çš„æ ‡å‡†è¯¯å·®æ˜¯å¤šå°‘ï¼Ÿå¾—åˆ°çš„ç»“æœï¼Œå››èˆäº”å…¥åˆ°å°æ•°ç‚¹å 3 ä½ï¼Œæ˜¯â€œä»»åŠ¡ 4â€çš„ç­”æ¡ˆã€‚

```python
'''ä¹Ÿå¯ä»¥
answer4 = mserror_2(y, linear_prediction(X, stoch_grad_desc_weights))
print(np.round(answer4, 3))
'''
```

```
y_pred = linear_prediction(X, stoch_grad_desc_weights)
#y = np.array(adver_data['Sales'])
answer4 = mserror_2(y, y_pred)
print('\tanswer 4 = ', round(answer4, 3))
```

```
answer 4 =  3.0
```