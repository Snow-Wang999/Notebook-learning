# 9-1-智能图像处理-大作业

## 题目

Детектирование объектов на изображении на основе комбинации SIFT+Случайный лес

基于SIFT+随机森林组合的图像中物体检测

## 资料

https://stud.lms.tpu.ru/course/view.php?id=109

## 文献搜索

```
Hello~订单：3164623526370979936
卡号：83942556
密码：634384
★★★★★使用方法（请仔细阅读）★★★★★
★第一步，复制链接地址：www.jieyoutsg.com 到浏览器打开（建议谷歌浏览器），并输入卡号密码登陆★
★第二步，左侧选择英文库，然后选择需要使用的数据库进入使用即可★
★用不了可尝试更换入口★
★★★可联系客服索取下载教程，还有好礼相送★★★
谢谢！再来哦~
```

## 联系方式

jbolotova@tpu.ru

Zoom：
https://us04web.zoom.us/j/6891830005?pwd=dTNQVW9pVWFLckhTZXNBbmE2YTZvdz09#success

## 图像中的物体检测

### 环境库

```python
import os #os模块提供了多数操作系统的功能接口函数。
import numpy as np
import matplotlib.pyplot as plt

import cv2 # opencv2
from xml.etree import ElementTree as et

import torch
import torchvision

import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
#用于图像增强的 Albumentations 

%matplotlib inline

print(torch.__version__)
```

#### os模块-其他操作系统接口

https://docs.python.org/3/library/os.html

https://docs.python.org/zh-cn/3.8/library/os.html

https://www.zhihu.com/question/322177702/answer/2007270459

该模块提供了一种使用操作系统相关功能的便携式方式。如果您只想读取或写入文件，请参阅open（），如果您想操作路径，请参阅os.path模块，如果您想要读取命令行上所有文件中的所有行，请参阅fileinput模块。有关创建临时文件和目录的信息，请参阅tempfile模块，有关高级文件和目录处理的信息，参见shutil模块。

当os模块被导入后，它会自适应于不同的操作系统平台，根据不同的平台进行相应的操作，在python编程时，经常和文件、目录打交道，所以离不了os模块。

1. 获得当前文件路径：os.getcwd() 
2. 新建一个文件夹：os.mkdir() 
3. 跳转当前文件路径：os.chdir(一个已存在的目录) 
4. 获得路径下的所有文件名称：os.listdir(path) 
5. 返回是否是文件夹：os.path.isdir() 
6. 返回是否是文件：os.path.isfile() 
7. 将文件路径进行拆分：os.path.split() 
8. 修改文件名称：os.rename()

#### sys模块

```python
import sys 
print("Python version: {}". format(sys.version))
```

```
Python version: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) 
[GCC 9.4.0]
```

https://docs.python.org/3/library/sys.html

sys-System-specific parameters and functions-系统特定参数和功能

该模块提供对解释器使用或维护的一些变量的访问，以及对与解释器强烈交互的函数的访问。它总是可用的。

#### xml.etree

[`xml.etree.ElementTree`](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree) 模块实现了一个简单高效的API，用于解析和创建XML数据。

### XML 树和元素

XML 是一种继承性的分层数据格式，最自然的表示方法是使用树。 为此， `ET` 有两个类 -- [`ElementTree`](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.ElementTree) 将整个XML文档表示为一个树， [`Element`](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.Element) 表示该树中的单个节点。 与整个文档的交互（读写文件）通常在 [`ElementTree`](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.ElementTree) 级别完成。 与单个 XML 元素及其子元素的交互是在 [`Element`](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.Element) 级别完成的。

我们将使用以下 XML 文档作为本节的示例数据：

```xml
<?xml version="1.0"?>
<data>
    <country name="Liechtenstein">
        <rank>1</rank>
        <year>2008</year>
        <gdppc>141100</gdppc>
        <neighbor name="Austria" direction="E"/>
        <neighbor name="Switzerland" direction="W"/>
    </country>
    <country name="Singapore">
        <rank>4</rank>
        <year>2011</year>
        <gdppc>59900</gdppc>
        <neighbor name="Malaysia" direction="N"/>
    </country>
    <country name="Panama">
        <rank>68</rank>
        <year>2011</year>
        <gdppc>13600</gdppc>
        <neighbor name="Costa Rica" direction="W"/>
        <neighbor name="Colombia" direction="E"/>
    </country>
</data>
```

可以通过从文件中读取来导入此数据：

```python
import xml.etree.ElementTree as ET
tree = ET.parse('country_data.xml')
root = tree.getroot()
```

作为 [`Element`](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html#xml.etree.ElementTree.Element) ， `root` 具有标签和属性字典:

```python
>>> root.tag
'data'
>>> root.attrib
{}
```

还有可以迭代的子节点：

```python
>>> for child in root:
    	print(child.tag, child.attrib)

country {'name': 'Liechtenstein'}
country {'name': 'Singapore'}
country {'name': 'Panama'}
```



### 数据集

#### UIUC car detection dataset

https://www.heywhale.com/mw/dataset/5e69d076ae2d090037790f1b

##### **背景描述**

数据集中包含用于评估目标检测算法的汽车侧视图。这些图像是由Shivani Agarwal，Aatif Awan和Dan Roth在UIUC收集整理的，并用于[1]，[2]中的论文实验中。
汽车图像均为灰度图像，原始PGM格式，总共1328张图片。

##### **数据说明**

数据集包含以下内容：

- 1050张训练图像（550张汽车图像和500张非汽车图像）
- 170张单比例测试图像，其中包含200张与训练集汽车比例大致相同的图像
- 108张多尺度测试图像，包含139辆不同规模的汽车
- 评估文件：提供了用于评估不同算法的标准化方法。

##### **数据来源**

https://cogcomp.seas.upenn.edu/Data/Car/
[1] Shivani Agarwal, Aatif Awan, and Dan Roth, Learning to detect objects in images via a sparse, part-based representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(11):1475-1490, 2004.
[2] Shivani Agarwal and Dan Roth,
Learning a sparse representation for object detection.
In Proceedings of the Seventh European Conference on Computer Vision, Part IV, pages 113-130, Copenhagen, Denmark, 2002.

##### **问题描述**

适用于研究目标检测、车辆识别等问题

---

1. 测试sift算法-3张照片-[SIFT Feature Transformation](https://www.kaggle.com/datasets/muralidhar123/sift-feature-transformation)

2. 目标检测-[Fruit Images for Object Detection](https://www.kaggle.com/datasets/mbkinaci/fruit-images-for-object-detection)

   1. 得到数据集

      ```python
      class FruitImagesDataset(torch.utils.data.Dataset):
          """
          Get image dataset
          """
      
          def __init__(self, files_dir, width, height, transforms=None):
              self.transforms = transforms # 图片增强变换？
              self.files_dir = files_dir# 文件夹名
              self.height = height# 图片的新的大小的高度
              self.width = width # 图片的新的大小的宽度
              
              self.imgs = [image for image in sorted(os.listdir(files_dir))
                              if image[-4:]=='.jpg']
              # 获取文件名
              # os.listdir("文件夹名")——获取文件夹下的文件名列表
              # image[-4:]=='.jpg'——最后四个字符是‘.jpg’，确认是图片
              self.classes = [_, 'apple','banana','orange']# 框的类名
      
          def __getitem__(self, idx):
              img_name = self.imgs[idx]# 列表中index（key）=idx的文件名
              image_path = os.path.join(self.files_dir, img_name)# 文件的路径名
         
              img = cv2.imread(image_path) # opencv2读取图片路径
              img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
              #bgr to rgb，把整数转为浮点数（eg：8 变成 8.）
              img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)
              # 调整图片大小
              
              img_res /= 255.0 # 把图片像素范围从[0,255]转成[0,1]
              
              annot_filename = img_name[:-4] + '.xml'# 把文件名’.jpg‘去掉，加上’.xml‘
              annot_file_path = os.path.join(self.files_dir, annot_filename)# xml文件路径
              
              boxes = []# 框的坐标列表初始化
              labels = []# 框的类名列表初始化
              tree = et.parse(annot_file_path)#解析xml文件
              root = tree.getroot()#获取XML文件的root元素
              
              wt = img.shape[1]# 原始图片的高
              ht = img.shape[0]# 原始图片的宽
              
              for member in root.findall('object'):
                  # Element.findall() 仅查找当前元素的直接子元素中带有指定标签的元素。
                  # ‘object’是需要的框对象
                  labels.append(self.classes.index(member.find('name').text))
                  # labels的元素是框的index和它的label 1 or 0
          		# member.find('name').text是它的类名name eg: apple
                  
                  xmin = int(member.find('bndbox').find('xmin').text)
                  # xmin是‘bndbox’框的xmin的值
                  xmax = int(member.find('bndbox').find('xmax').text)
                  
                  ymin = int(member.find('bndbox').find('ymin').text)
                  ymax = int(member.find('bndbox').find('ymax').text)
                  
                  # 把框的坐标转换到新的大小的图片中
                  xmin_corr = (xmin/wt)*self.width
                  xmax_corr = (xmax/wt)*self.width
                  ymin_corr = (ymin/ht)*self.height
                  ymax_corr = (ymax/ht)*self.height
                  # 添加一个框的对象的新坐标
                  boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])
              # 把每张图的所有框转化为torch的tensor张量
              boxes = torch.as_tensor(boxes, dtype=torch.float32)
              # 计算框的面积
              area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
      		
              # 框有几个，填零
              iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
              
              # 框的标签转为tensor
              labels = torch.as_tensor(labels, dtype=torch.int64)
      		
              # 组建target字典，包含“boxes”，“labels”，“area”，“iscrowd”，“image_id”
              target = {}
              target["boxes"] = boxes# 单张图片所有框的每个框的坐标
              target["labels"] = labels# 单张图片所有框的每个框内对象的类别
              target["area"] = area # 单张图片所有框的每个框的面积
              target["iscrowd"] = iscrowd# 单张图片是不是有一组框对象
              
              image_id = torch.tensor([idx])# 图片的index转为tensor
              target["image_id"] = image_id # 图片的index
      
              if self.transforms:
                  # 转变
              	sample = self.transforms(image = img_res,# 图片像素范围[0,1]
                                           bboxes = target['boxes'],# 图片的框的位置信息
                                           labels = labels)# 图片的框的类名
                  
                  img_res = sample['image']
                  target['boxes'] = torch.Tensor(sample['bboxes'])
                  
              return img_res, target
      
          def __len__(self):
              return len(self.imgs)#多少张图片
      ```

      ```xml
      <annotation><!--: -->
      	<folder>test</folder><!--:所属文件夹 -->
      	<filename>apple_77.jpg</filename><!--: 文件名-->
      	<path>C:\tensorflow1\models\research\object_detection\images\test\apple_77.jpg</path><!--: 路径-->
      	<source><!--: XML源读取XML数据文件，并用数据填充源输出中的列。-->
      		<database>Unknown</database>
      	</source>
          <!--: 图片的大小-->
      	<size>
      		<width>300</width>
      		<height>229</height>
      		<depth>3</depth>
      	</size>
      	<segmented>0</segmented>
          <object>
              <name>apple</name>
              <pose>Unspecified</pose>
              <truncated>0</truncated>
              <difficult>0</difficult>
              <bndbox>
                  <xmin>71</xmin>
                  <ymin>60</ymin>
                  <xmax>175</xmax>
                  <ymax>164</ymax>
              </bndbox>
          </object>
      </annotation>
      ```

      https://learn.microsoft.com/en-us/sql/integration-services/data-flow/xml-source?view=sql-server-ver16

   2. 得到目标检测的框

## 预处理

图片大小重新reshape

### **为什么深度学习图像分类里的图片的输入大小都是224\*224呢？**

做过图像分类项目或者看过文章的小伙伴们应该都知道，在论文中进行各类方法的比较时，要求使用同样的数据集。而为了公平的比较，网络的输入大小通常都是224*224的大小，那为什么呢？有同学思考过这个问题吗？

我们都知道，一个图像分类模型，在图像中经历了下面的流程。

从输入image->卷积和池化->最后一层的feature map->全连接层->损失函数层softmax loss。

![image-20230124120131834](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230124120131834.png)

从输入到最后一个卷积特征feature map，就是进行信息抽象的过程，然后就经过全连接层/全局池化层的变换进行分类了，这个feature map的大小，可以是3*3，5*5，7*7等等。

解答1：在这些尺寸中，如果尺寸太小，那么信息就丢失太严重，如果尺寸太大，信息的抽象层次不够高，计算量也更大，所以7*7的大小是一个最好的平衡。

另一方面，图像从大分辨率降低到小分辨率，降低倍数通常是2的指数次方，所以图像的输入一定是7*2的指数次方。以ImageNet为代表的大多数分类数据集，图像的长宽在300分辨率左右。

解答2：所以要找一个7*2的指数次方，并且在300左右的，其中7*2的4次方=7*16=112，7*2的5次方等于7*32=224，7*2的6次方=448，与300最接近的就是224了。

这就是最重要的原因了，当然了对于实际的项目来说，有的不需要这么大的分辨率，比如手写数字识别MNIST就用32*32，有的要更大，比如细粒度分类。
[【AI-1000问】为什么深度学习图像分类的输入多是224\*224](https://blog.csdn.net/hacker_long/article/details/88197520)

作者csdn： 言有三

文章首发于微信公众号《有三AI》

[【知识星球】为什么图像分类任务要从256*256中裁剪出224*224](http://www.360doc.com/content/20/1127/03/72629698_948146706.shtml)

## 常用函数

```python
# !pip install selective-search
```

```python
import glob
# import selective_search
from PIL import Image
import cv2
import  xml.dom.minidom
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import average_precision_score, precision_recall_curve

COLOR = (55,255,155)
COLOR_SS = (0,0,255)
```



```python
def globSplit(path, splitNumber):
    """
    读取path中的文件，并分为两个列表
    """
    count = 0
    list1 = []
    list2 = []
    for pgm_file in glob.glob(path):
        if count < splitNumber:
            list1.append(pgm_file)
        elif count < splitNumber * 2:
            list2.append(pgm_file)
        else:
            break
        count += 1
    return list1, list2

def predictFromProba(probaList, posProba=0.5, posTAG=1, negTag=-1):
    """
    根据设置的可能性(probability)阈值返回分类列表
    大于posProba则标记为正样本，否则为负样本
    """
    labelList = []
    for proba in probaList:
    #proba[1]为正样本概率
        if (proba[1] >= posProba):
            labelList.append(posTAG)
        else:
            labelList.append(negTag)
    return labelList

def printList(l):
    newStr = ''
    for i in l:
        newStr += str(i) + ' '
    print(newStr)

def pointInArea(point, area):
    x1 = area[0]
    y1 = area[1]
    x2 = area[2]
    y2 = area[3]

    x = point[0]
    y = point[1]

    if x >= x1 and x <= x2 and y >= y1 and y <= y2:
        return True
    return False

def calArea(area):
    x1 = area[0]
    y1 = area[1]
    x2 = area[2]
    y2 = area[3]
    return (x1 - x2) * (y1 - y2)

def CrossLine(left, right, y, top, bottom, x):
    # 判断一根横线和一根竖线是否交叉
    # 横线有三个参数：left, right和y
    # 竖线有三个参数：top, bottom和x
    return (top < y) and (bottom > y) and (left < x) and (right > x)

def IOU(rect1, rect2):
    x11 = rect1[0]
    y11 = rect1[1]
    x12 = rect1[2]
    y12 = rect1[3]

    x21 = rect2[0]
    y21 = rect2[1]
    x22 = rect2[2]
    y22 = rect2[3]

    if pointInArea((x11, y11), rect2) == False and pointInArea((x12, y12), rect2) == False \
        and pointInArea((x11, y12), rect2) == False and pointInArea((x12, y11), rect2) == False \
        and pointInArea((x21, y21), rect1) == False \
        and not CrossLine(x11, x12, y11, y21, y22, x21) and not CrossLine(x21, x22, y21, y11, y12, x11):
        return 0
    
    xList = [x11, x12, x21, x22]
    yList = [y11, y12, y21, y22]
    xList.sort()
    yList.sort()
    areaMiddle = calArea([xList[1], yList[1], xList[2], yList[2]])
    area1 = calArea(rect1)
    area2 = calArea(rect2)
    return float(areaMiddle) / (area1 + area2 - areaMiddle)

def NMS(rectList, threshold=.5):
    rectList = sorted(rectList, key=lambda rectList: rectList[4],
            reverse=True)
    i = 0
    while i < len(rectList):
        j = i + 1
        while j < len(rectList):
            iou = IOU(rectList[i], rectList[j])
            if iou > threshold:
                del rectList[j]
            else:
                j += 1
        i += 1
    return rectList
        
    print(rectList)

def getSelectiveSelectRect(im):
    shape = im.shape
    if shape[0] > shape[1]:
        maxScale = shape[0]
    else:
        maxScale = shape[1]
    img_lbl, regions = selective_search.selective_search(im, scale=maxScale, sigma=0.7, min_size=400)
    rectList = []
    originMaxSize = (shape[0] - 2) * (shape[1] - 2)
    for i in range(len(regions)):
        rect = regions[i]['rect']
        rect = [rect[0], rect[1], rect[0] + rect[2], rect[1] + rect[3], 0]
        size = calArea(rect)
        if size < 400:
            continue
        if size >= originMaxSize:
            continue
        rectList.append(rect)
    return rectList

def showImgWithSS(im, area, ssList):
    img = im.copy()
    cv2.rectangle(img, (area[0], area[1]), (area[2], area[3]),COLOR, 3)
    for ssArea in ssList:
        if len(ssArea) > 4:
            cv2.putText(img, str(round(ssArea[4], 3)), (ssArea[0], ssArea[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_SS, 2)
        cv2.rectangle(img, (ssArea[0], ssArea[1]), (ssArea[2], ssArea[3]),COLOR_SS) 

    #cv2.imwrite('showImgWithSS.jpg', img)
    cv2.imshow('showImgWithSS', img)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()
        
def minInList(li):
    minValue = 999
    for item in li:
        if item < minValue:
            minValue = item
    return minValue

def maxInList(li):
    maxValue = -999
    for item in li:
        if item > maxValue:
            maxValue = item
    return maxValue

def precisionRecallCurve(testlabels, predictResult):
    chartPrecision, chartRecall, _ = precision_recall_curve(testlabels, predictResult)
    average_precision = average_precision_score(np.array(testlabels), predictResult)
    print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision))
    plt.figure()
    plt.step(chartRecall, chartPrecision, color='b', alpha=0.2, where='post')
    plt.fill_between(chartRecall, chartPrecision, step='post', alpha=0.2, color='b')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title(
        'Average precision score, micro-averaged over all classes: AP={0:0.2f}'
        .format(average_precision))
    plt.show()
```



## sift算法

[OpenCV - SIFT 参数及计算返回结果说明](https://www.aiuai.cn/aifarm1639.html)

[图像特征提取-SIFT](https://www.jianshu.com/p/95b5a2a87e39)

[【动手学计算机视觉】第七讲：传统目标检测之SIFT特征](https://zhuanlan.zhihu.com/p/70385018)

[OpenCV - SIFT 参数及计算返回结果说明](https://www.aiuai.cn/aifarm1639.html)

[**Introduction to SIFT (Scale-Invariant Feature Transform)**](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html)

### github

1. [SIFT tutorial](https://github.com/DGarciaMedina/SIFT/blob/master/SIFT%20tutorial.ipynb)

2. [LeeGenD](https://github.com/LeeGenD)/**[svm_detection](https://github.com/LeeGenD/svm_detection)**

### kaggle

1. [Fruit Detection](https://www.kaggle.com/code/arinalhaq/fruit-detection)
2. [Tutorial_3(Faster_RCNN)](https://www.kaggle.com/code/aravindanr22052001/tutorial-3-faster-rcnn)
3. [Copy Move Forgery Detection(DBSCAN Clustering)](https://www.kaggle.com/code/himj26/copy-move-forgery-detection-dbscan-clustering)

sift不仅是一个描述符提取器，也是一个特征查找器。

```python
import matplotlib.pyplot as plt
import numpy as np
import cv2 
image_path ='/kaggle/input/fruit-images-for-object-detection/train_zip/train/apple_1.jpg'
img = cv2.imread(image_path)# read image
img_gray= cv2.cvtColor(img ,cv2.COLOR_BGR2GRAY)#bgr to gray
img_res = cv2.resize(img_gray, (224, 224), cv2.INTER_AREA)# reshape image
# 使用SIFT 或 SURF 检测角点
sift = cv2.SIFT_create() #sift 特征提取器 
# 计算关键点和描述子
# 其中kp为关键点keypoints
# des为描述子descriptors
kp, des = sift.detectAndCompute(img_res,None)
# 绘出关键点
# 其中参数分别是源图像、关键点、输出图像、显示颜色
img_kp = cv2.drawKeypoints(img_res,kp,img,
                           flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
#cv2.imwrite('pandas_keypoints.jpg',img)
plt.imshow(img_kp)
```

**SIFT_create() 参数**

```python
def SIFT_create(nfeatures=None, 
                nOctaveLayers=None, 
                contrastThreshold=None, 
                edgeThreshold=None, 
                sigma=None):
```

其中，

[1] - **nfeatures：**特征点数目（算法对检测出的特征点排名，返回最好的nfeatures个特征点）

[2] - **nOctaveLayers**：金字塔中每组的层数（算法中会自己计算这个值）

[3] - **contrastThreshold**：过滤掉较差的特征点的对阈值. **contrastThreshold越大，返回的特征点越少.**

[4] - **edgeThreshold**：过滤掉边缘效应的阈值. **edgeThreshold越大，特征点越多（被过滤掉的越少）.**

[5] - **sigma**：金字塔第0层图像高斯滤波系数.

注：

[1] - 参数**nfeatures**指定最终返回的特征点数量，并不影响SIFT特征检测的结果.

[2] - 参数**nOctaveLayers**和**sigma**主要影响图像高斯金字塔的构成.

[3] - **contrastThreshold**和**edgeThreshold** 会影响在DOG中寻找极值点的过程与结果.

OpenCV 默认参数：

```protobuf
nfeatures =0
nOctaveLayers =3
contrastThreshold = 0.04
edgeThreshold = 10
sigma =1.6
```

![GaussianPrymid](https://livezingy.com/uploads/201610/opencv/GaussianPyramid.png)

https://livezingy.com/sift-in-opencv3-1/

## 特征匹配Flann

[OpenCV-Python教程:41.特征匹配](https://www.jianshu.com/p/ed57ee1056ab)

[OpenCV—python 角点特征检测之三（FLANN匹配）](https://blog.csdn.net/wsp_1138886114/article/details/90578810)

[【Python+OpenCV】特征点匹配之cv2.FlannBasedMatcher](https://blog.csdn.net/qq_36584673/article/details/121997887)

[以图搜图--基于*FLANN*特征匹配 - 知乎](http://www.baidu.com/link?url=n-T1q8SzT9M38O_O62tIqsvoSCT8HGXqHp62GLkglaYpjhlHVvMyLq_H5urHhYb4)

[opencv图像特征检测及匹配（harris，sift，surf，fast，breif，orb，BFmatch，FlannBasedMatcher）](https://blog.csdn.net/weixin_37565420/article/details/79090644?spm=1001.2101.3001.6650.6&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-6-79090644-blog-121997887.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-6-79090644-blog-121997887.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=8)

[cv2.FlannBasedMatcher](https://www.google.com/search?q=cv2.FlannBasedMatcher&sxsrf=AJOqlzWqZW3ofZJzS9ZjZu1-92BeGcHjNQ%3A1674230575269&source=hp&ei=L7vKY-b_Ddqp1e8PiqWh0Ac&iflsig=AK50M_UAAAAAY8rJPxL38j_XHWc9t-ai5XhONG25YFe4&ved=0ahUKEwim09X1wtb8AhXaVPUHHYpSCHoQ4dUDCAg&uact=5&oq=cv2.FlannBasedMatcher&gs_lcp=Cgdnd3Mtd2l6EAMyBQgAEIAEMgUIABCABDIFCAAQgAQyBQgAEIAEMgUIABCABDIHCAAQHhDxBFAAWABgngdoAHAAeACAAfgBiAH4AZIBAzItMZgBAKABAqABAQ&sclient=gws-wiz)



关键点已经检测出来，最后一步要做的就是绘出匹配效果，本文用到的是利用 ***FlannBasedMatcher\*** 来显示匹配效果， 首先要对 ***FlannBasedMatcher\*** 进行参数设计和实例化，然后用 ***knn** 对前面计算的出的特征描述子进行匹配，最后利用 ***drawMatchesKnn\*** 显示匹配效果，

```python
def get_flann_matcher():
	flann_params = dict(algorithm = 1, trees = 5)
	return cv2.FlannBasedMatcher(flann_params, {})
```

Flann(Fast_Library_for_Approximate_Nearest_Neighbors):快速最近邻搜索库，应该是目前[OpenCV](https://so.csdn.net/so/search?q=OpenCV&spm=1001.2101.3001.7020)中封装的用于特征匹配的最好的匹配器了。

```python
# FLANN parameters
FLANN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks=50)  # or pass empty dictionary
```

```python
# 设置FLANN匹配器参数，定义FLANN匹配器，使用 KNN 算法实现匹配
flann = cv2.FlannBasedMatcher(index_params,search_params)
matches = flann.knnMatch(des1,des2,k=2)

# 根据matches生成相同长度的matchesMask列表，列表元素为[0,0]
# Need to draw only good matches, so create a mask
matchesMask = [[0,0] for i in xrange(len(matches))]

# ratio test as per Lowe's paper
# 去除错误匹配
for i,(m,n) in enumerate(matches):
    if m.distance < 0.7*n.distance:
        matchesMask[i]=[1,0]

# 将图像显示
# matchColor是两图的匹配连接线，连接线与matchesMask相关
# singlePointColor是勾画关键点
draw_params = dict(matchColor = (0,255,0),
                   singlePointColor = (255,0,0),
                   matchesMask = matchesMask,
                   flags = 0)

img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)
# img3 = cv2.drawMatchesKnn(queryImage,kp1,trainingImage,kp2,matches[:50],None,**drawParams)
plt.imshow(img3,),plt.show()
```

## BOW-词袋模型(Bag-Of-Word)

1. [第十九节、基于传统图像处理的目标检测与识别(词袋模型BOW+SVM附代码)](https://www.bbsmax.com/A/D8543y1vJE/)
2. [**How to use BOWImgDescriptorExtractor in python - openCV ?**](https://answers.opencv.org/question/72038/how-to-use-bowimgdescriptorextractor-in-python-opencv/)

## 随机森林 ensemble

[`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble).[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)

[sklearn随机森林分类类RandomForestClassifier](https://blog.csdn.net/w952470866/article/details/78987265/)

[sklearn RandomForestClassifier  clf.predict_proba](https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=2&tn=baiduhome_pg&wd=sklearn%20RandomForestClassifier%20%20clf.predict_proba&rsv_spt=1&oq=sklearn%2520RandomForest%2526lt%253Blassifier&rsv_pq=c36a4275000da273&rsv_t=e9e9pkiEuhiickrGNYGBDkN95FNcWEOEJtwhgxknCfx5KnvnWdiSqeIvpHSMCA32VBJZ&rqlang=cn&rsv_enter=0&rsv_dl=tb&rsv_sug3=5&rsv_n=2&rsv_btype=t&inputT=110801&rsv_sug4=111406)

#### 随机森林的参数

```python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)
```

- **n_estimators**: int，默认值=100 

  森林中的树木数量。

  n_estimators：@Falcon是错误的，通常树越多，算法过度拟合的可能性就越小。因此，尝试增加它。该数字越小，模型越接近具有受限功能集的决策树。

- **criterion**: 标准｛“gini”，“熵”，“log_loss”｝，默认值=“gini"

  衡量分割质量的函数。支持的标准是基尼杂质的“基尼”和香农信息增益的“log_loss”和“熵”，参见数学公式。注意：此参数是特定于树的。

- **max_depth**: int，默认值=无 

  树的最大深度。如果为“无”，则节点将展开，直到所有树叶都是纯的，或者直到所有树叶包含的样本少于min_samples_split。

  max_depth：尝试一下。这将降低学习模型的复杂度，降低拟合风险。尝试从5-10开始，从小开始，增加您可获得最佳结果。

- **min_samples_split**: int或float，默认值=2 

  拆分内部节点所需的最小样本数： 

  - 如果为int，则将min_samples_split视为最小值。 
  - 如果float，则min_samples_split是一个分数，ceil（min_samples-split*n_samples）是每个分割的最小样本数。

- **min_samples_leaf**: int或float，默认值=1 

  叶节点所需的最小样本数。只有在左分支和右分支中的每个分支中至少留下min_samples_leaf训练样本时，才会考虑任何深度的分割点。这可能具有平滑模型的效果，尤其是在回归中。 

  - 如果为int，则将min_samples_leaf视为最小值。 
  - 如果float，则min_samples_leaf是一个分数，ceil（min_samples.leaf*n_samples）是每个节点的最小样本数。

  min_samples_leaf：尝试将其设置为大于1的值。这与max_depth参数具有相似的效果，这意味着一旦叶子每个具有相同数量的样本，分支将停止分裂。

- **min_weight_fraction_leaf**: float，默认值=0.0 

  叶节点所需的（所有输入样本的）权重总和的最小加权分数。当未提供sample_weight时，样本具有相等的权重。 

- **max_features**:｛“sqrt”，“log2”，None｝，int或float，默认值=“sqrt" 

  寻找最佳分割时需要考虑的功能数量： 

  - 如果为int，则考虑每次拆分时的max_features特征。
  - 如果float，则max_features是一个分数，并且在每次拆分时考虑max（1，int（max_features*n_features_in_））个特征。 

  - 如果“auto”，则max_features=sqrt（n_features）。 

  - 如果“sqrt”，则max_features=sqrt（n_features）。 

  - 如果“log2”，则max_features=log2（n_features）。 

  - 如果无，则max_features=n_features。

  max_features：尝试减少此数量(尝试使用30-50％的功能)。这确定了每棵树被随机分配多少个特征。尺寸越小，过度拟合的可能性越小，但过小的尺寸会开始引入拟合不足。

- **max_leaf_nodes**: int，默认值=无 

  以最佳优先方式使用max_leaf_nodes生长树。最佳节点定义为杂质的相对减少。如果“无”，则无限制的叶节点数。 

- **min_impurity_decrease**: float，默认值=0.0 

  如果此拆分导致杂质减少大于或等于此值，则节点将被拆分。 

  加权杂质减少方程如下： 

  ```
  N_t / N * (impurity - N_t_R / N_t * right_impurity
                      - N_t_L / N_t * left_impurity)
  ```

  其中，N是样本总数，N_t是当前节点处的样本数，N_t_L是左侧子节点中的样本数量，N_t_R是右侧子节点中样本的数量。 N、 如果通过了sample_weight，则N_t、N_t_R和N_t_L都是指加权和。

- **bootstrap**: bool，默认值=True 

  构建树时是否使用引导样本。如果为False，则使用整个数据集来构建每个树。 

- **oob_score**: bool，默认值=False 

  是否使用袋外样本来估计概括得分。仅当bootstrap=True时可用。 

- **n_job**: int，默认值=无 

  要并行运行的作业数。fit、predict、decisionpath和apply都在树上并行化。除非在joblib.paralle_backend 上下文中，否则None表示1-1表示使用所有处理器。有关详细信息，请参阅词汇表。 

- **random_state**: int，RandomState实例或None，默认值=None 

  控制构建树时使用的样本自举的随机性（如果bootstrap=True）和在每个节点上查找最佳分割时要考虑的特征采样（如果max_features<n_features）。有关详细信息，请参阅词汇表。 

- **verbose**: int，默认值=0 

  控制拟合和预测时的详细程度。 

- **warm_start**: bool，默认值=False 当设置为True时，重用上一次调用的解决方案以适应并向集成中添加更多的估计器，否则，只适合一个全新的林。有关详细信息，请参阅词汇表和其他弱势学习者。

- **class_weight**:｛“balanced”，“balanced_subsample”｝，dict或dict列表，默认值=无 

  与形式为｛class_label:weight｝的类关联的权重。如果没有给出，所有的课程都应该有一个权重。对于多输出问题，可以按照与y列相同的顺序提供dict列表。 

  请注意，对于多输出（包括多标签），应为其自身字典中每一列的每个类别定义权重。例如，对于四类多标签分类权重应为[｛0:1，1:1｝，｛0:2，1:5｝，{0:1，1:5}，｛0：1｝、｛0∶1，1:1｝]，而不是[｛1:1｝，2∶5｝，3∶1｝和｛4:1｝]。 

  “balanced-平衡”模式使用y的值自动调整与输入数据中的类频率成反比的权重，如n_samples/（n_classes*np.bincount（y）） 

  “balanced_subsample”模式与“balanced”模式相同，只是权重是根据每棵树的引导样本计算的。 

  对于多输出，y的每一列的权重将相乘。 

  请注意，如果指定了sample_weight，则这些权重将与sample_weights（通过拟合方法传递）相乘。 

- **ccp_alpha**: 非负浮动，默认值=0.0 

  用于最小成本复杂性修剪的复杂性参数。将选择具有小于ccp_alpha的最大成本复杂度的子树。默认情况下，不执行修剪。有关详细信息，请参阅最小成本复杂性修剪。

- **max_samples**: int或float，默认值=无 

  如果bootstrap为True，则从X中提取的样本数，以训练每个基本估计器。 如果无（默认值），则绘制X.shape[0]样本。 如果为int，则绘制max_samples样本。 如果为float，则绘制max_samples*X.shape[0]样本。因此，max_samples应在间隔（0.0，1.0]）内。





[最详细的GBDT、随机森林、XGBoost](https://zhuanlan.zhihu.com/p/486898287)

[能简单解释下随机森林和Xgboost吗？](https://www.zhihu.com/question/525656888)

[XGBoost 和随机森林在表格数据上优于深度学习？](https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&mid=2247548734&idx=1&sn=8d05832765e3cb980da64cdce5e4a2dc&chksm=fb3a9a35cc4d13233ca5ae4dca89a46d675af4c9b193e976d8d6da70aad9fa948ee9da0d4f6d&scene=27)

### 过拟合

[关于机器学习：如何解决Python sklearn随机森林中的过拟合问题？](https://www.codenong.com/20463281/)

[随机森林 过拟合](https://www.baidu.com/s?ie=UTF-8&wd=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%20%E8%BF%87%E6%8B%9F%E5%90%88)

我会同意@Falcon w.r.t.数据集大小。主要问题可能是数据集的大小。如果可能的话，您能做的最好的事情就是获取更多的数据，更多的数据(通常)越不可能拟合得越好，因为随着数据集大小的增加，出现预测性的随机模式开始被淹没。

也就是说，我将查看以下参数：

n_estimators：@Falcon是错误的，通常树越多，算法过度拟合的可能性就越小。因此，尝试增加它。该数字越小，模型越接近具有受限功能集的决策树。

max_features：尝试减少此数量(尝试使用30-50％的功能)。这确定了每棵树被随机分配多少个特征。尺寸越小，过度拟合的可能性越小，但过小的尺寸会开始引入拟合不足。

max_depth：尝试一下。这将降低学习模型的复杂度，降低拟合风险。尝试从5-10开始，从小开始，增加您可获得最佳结果。

min_samples_leaf：尝试将其设置为大于1的值。这与max_depth参数具有相似的效果，这意味着一旦叶子每个具有相同数量的样本，分支将停止分裂。

做这项工作时要注意科学。使用3个数据集，一个训练集，一个单独的"开发"数据集来调整您的参数，以及一个使用最佳参数测试最终模型的测试集。一次只更改一个参数并评估结果。或尝试使用sklearn网格搜索算法一次搜索所有这些参数。

## 集成学习

[机器学习——集成学习(Bagging、Boosting、Stacking)](https://www.shuzhiduo.com/A/x9J2X6ved6/)

### 1. 平均型

### 2. 投票型-voting

[sklearn集成学习之VotingClassifier](https://blog.csdn.net/weixin_45508265/article/details/121647750)

[集成学习 Python-随机森林、SVM、KNN](https://www.it1352.com/2423813.html)

#### voting = ‘hard’：使用 Hard Voting 做决策

![image-20230124235850192](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230124235850192.png)

#### voting = ‘soft’：使用 Soft Voting 做决策

![image-20230124235933222](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230124235933222.png)

3. 学习型-stacking

   ​		投票法和平均法相对比较简单，但是可能学习误差较大，于是就有了学习法。对于学习法，代表方法是 Stacking，当使用 Stacking 的结合策略时， 不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。
   　　在这种情况下，将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。

4. 

## 评价

[python-sklearn中RandomForestClassifier函数以及ROC曲线绘制](https://blog.csdn.net/hjxu2016/article/details/78337308)

[终于搞懂了PR曲线](https://blog.csdn.net/j05073094/article/details/119985348)



```
if __name__ =='__main__':
    
```



## References:

### 1. 图像处理的基本操作

1. [opencv图像显示函数](https://blog.csdn.net/weiwei152433/article/details/122491074)

2. [Elementary image processing](https://www.kaggle.com/code/iinjyi/elementary-image-processing)

   1. 导入图片

      ```python
      #RGB image:
      img=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)# bgr to rgb
      plt.imshow(img)
      plt.show()
      ```

      ![image-20230120175609115](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230120175609115.png)

   2. rgb to gray

      ```python
      #gray scale image:
      img_gs=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
      plt.imshow(img_gs,cmap='gray')
      plt.show()
      ```

      ![image-20230120175657947](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230120175657947.png)

   3. Manipulating RGB channels-单独抽取channel-r，b，g

   4. Adding and removing noise-添加或删除高斯噪声

      ```python
      rows, cols,_= image.shape
      noise = np.random.normal(0,15,(rows,cols,3)).astype(np.uint8)
      noisy_image = image + noise
      plot_images(image, noisy_image, title_1="Orignal",title_2="Image Plus Noise")
      ```

      ![image-20230120175815981](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230120175815981.png)

   5. 中值和高斯模糊

      ```python
      med=cv2.medianBlur(image, 9)
      gaus=cv2.GaussianBlur(image, (3, 3), 0, 0)
      
      fig, (ax1, ax2, ax3) = plt.subplots( 1, 3, figsize=(12, 8))
      
      ax1.imshow(cv2.cvtColor(noisy_image, cv2.COLOR_BGR2RGB))
      ax1.set_title('Image', size=15)
      
      ax2.imshow(cv2.cvtColor(med, cv2.COLOR_BGR2RGB))
      ax2.set_title('Median blur', size=15)
      
      ax3.imshow(cv2.cvtColor(gaus, cv2.COLOR_BGR2RGB))
      ax3.set_title('Gaussian blur', size=15)
      plt.show()
      ```

      ![image-20230120175906253](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230120175906253.png)

   6. Edge detection-边缘检测

   7. 多图片输出

      [Creating multiple subplots using `plt.subplots`](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html)

      ```python
      import numpy as np
      import matplotlib.pyplot as plt
      
      w = 10
      h = 10
      fig = plt.figure(figsize=(9, 13))
      columns = 4
      rows = 5
      
      # prep (x,y) for extra plotting
      xs = np.linspace(0, 2*np.pi, 60)  # from 0 to 2pi
      ys = np.abs(np.sin(xs))           # absolute of sine
      
      # ax enables access to manipulate each of subplots
      ax = []
      
      for i in range(columns*rows):
          img = np.random.randint(10, size=(h,w))
          # create subplot and append to ax
          ax.append( fig.add_subplot(rows, columns, i+1) )
          ax[-1].set_title("ax:"+str(i))  # set title
          plt.imshow(img, alpha=0.25)
      
      # do extra plots on selected axes/subplots
      # note: index starts with 0
      ax[2].plot(xs, 3*ys)
      ax[19].plot(ys**2, xs)
      
      plt.show()  # finally, render the plot
      ```

      

   6. 

3. opencv的resize函数的`interpolation`参数用于告诉函数怎么插值计算输出图像的像素值。

   OpenCV自己提供了5种方法：`INTER_NEAREST`、 `INTER_LINEAR`、`INTER_AREA`、`INTER_CUBIC`，和`INTER_LANCZOS4`。

   这5种方法里，有4种从名字上就很好理解，只要自己搜索一下，就能获得对应方法大概上是怎样算的信息：`INTER_NEAREST`就是用离得最近的像素值作为结果；`INTER_LINEAR`是在 x 和 y 方向根据临近的两个像素的位置进行线性插值；`INTER_CUBIC`是用某种3次方函数差值；`INTER_LANCZOS4`是跟傅立叶变换有关的三角函数的方法。唯独`INTER_AREA`显得神神秘秘，因为它在OpenCV的文档里是这么写的：

   > 使用像素面积关系重新采样。这可能是图像抽取的首选方法，因为它可以获得无莫尔条纹的结果。但是当图像被缩放时，它类似于INTER_NEAREST方法。

   1. [OpenCV里的INTER_AREA究竟是在做啥？](https://zhuanlan.zhihu.com/p/38493205)
   2. `INTER_AREA`在缩小和放大图像时，是完全不一样的。首先我们说缩小图像。

4. [在python中用cv2读取pgm图像](https://qa.1r1g.com/sf/ask/2549500621/#)

   ```python
   cv2.imread('a.pgm',-1) 
   ```

5. 灰度图片变绿

   https://blog.csdn.net/Joker00007/article/details/121345877

   ```python
   img = cv2.imread(path)
   img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
   print(img)
   plot.imshow(img) 
   plot.show()
   ```

   ![image-20230121110717590](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230121110717590.png)

   代码改成

   ```python
   img = cv2.imread(path)  # 读取图片
   img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
   print(img)
   plot.imshow(img, cmap='gray')  # 不加cmap则是默认三通道展示
   plot.show()
   cv2.imwrite('img.jpg',img, [cv2.IMWRITE_JPEG_QUALITY, 50]) # 50代表保存图片的品质，越高图片保存后分辨率越大
   
   ```

   ![image-20230121110738703](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230121110738703.png)

### 2. 数据集

1. [MSCOCO物体检测评测系统的分析](https://zhuanlan.zhihu.com/p/110676412)
2. [[**COCO Dataset**](https://cocodataset.org/#home)](https://airctic.github.io/icedata/coco/)
3. [使用UIUC数据集进行汽车检测](https://blog.csdn.net/jc15988821760/article/details/98043368)-算法的理解
4. 此处使用的数据集：
   1. [car_detection_sift](https://www.kaggle.com/datasets/wangsnow/car-detection-sift)
   2. [Fruit Images for Object Detection](https://www.kaggle.com/datasets/mbkinaci/fruit-images-for-object-detection)


### 3. xml

1. [TensorFlow 对数据集标记的xml文件解析记录](https://gds.51cto.com/posts/217)
2. [XML Source](https://learn.microsoft.com/en-us/sql/integration-services/data-flow/xml-source?view=sql-server-ver16)
3. [[`xml.etree.ElementTree`](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree) --- ElementTree XML API](https://docs.python.org/zh-cn/3/library/xml.etree.elementtree.html)
4. [TensorFlow 对数据集标记的xml文件解析记录](https://gds.51cto.com/posts/217)

### 4. 算法

1. [以代码为基础的opencv-python学习 利用词袋和SVM进行汽车检验](https://www.cnblogs.com/August2019/p/12680602.html)
2. [关于python：在scikit-learn中将分类器保存到磁盘](https://www.codenong.com/10592605/)
3. [python+OpenCV笔记（三十六）：自定义物体检测器——创建、训练与保存物体检测器（依赖于支持向量机与BoW技术）](https://blog.csdn.net/qq_45832961/article/details/122782730)
4. [使用 PyTorch Faster RCNN 进行自定义目标检测](https://zhuanlan.zhihu.com/p/439315673)

### 其他

1. [selective-search的Python项目详细描述](https://www.cnpython.com/pypi/selective-search)
2. [google学术-sift object detection based on random forest](https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=sift+object+detection+based+on+random+forest&btnG=)
3. [【快乐Kaggle入门】Kaggle入门之比赛基本流程](https://blog.csdn.net/weixin_42200613/article/details/122331593)
4. 
5. 

23-晚上

## 参考文献：

1. 随机森林

   1. [cv2.RTrees](https://docs.opencv.org/3.4/d0/d65/classcv_1_1ml_1_1RTrees.html)
   2. [PythonOpenCV--Rtrees随机森林](https://blog.csdn.net/wishchin/article/details/38488621)
   3. [Random Trees in OpenCV from python](https://stackoverflow.com/questions/15906543/random-trees-in-opencv-from-python)
   4. [Random Trees](https://docs.opencv.org/2.4/modules/ml/doc/random_trees.html)
   5.  *随机森林*算法和SelectKBest中对特征降维在*归一化*有很大的不同,*随机森林*是决策树不需要进行*归一化*/标准化操作。 概率形的算法需要对数据进行*归一化*,可以加快算法的运行与迭代。
   6. [机器学习——随机森林，RandomForestClassifier参数含义详解](https://www.cnblogs.com/baby-lily/p/10657185.html)
   7. [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble).[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)

2. 集成模型

   1. [**【周末AI课堂】bagging集成和stacking集成（理论篇）| 机器学习你会遇到的“坑”**](https://blog.51cto.com/u_15057819/2570554)
   2. [通俗易懂--模型集成(多模型)讲解(算法+案例)](http://t.zoukankan.com/mantch-p-10203143.html)
   3. [机器学习——集成学习(Bagging、Boosting、Stacking)](https://www.shuzhiduo.com/A/x9J2X6ved6/)

3. 检测标准

   1. [PR曲线 可算是明白了 precision recall曲线](https://blog.csdn.net/u013249853/article/details/96132766)
   2. [怎么看深度学习目标检测的PR（查准率-查全率 precision-recall）曲线？](https://blog.csdn.net/Dontla/article/details/104194694)
   3. [Precision-Recall (PR) 曲线-在信息检索重要性](https://zhuanlan.zhihu.com/p/398218492)
   4. [终于搞懂了PR曲线](https://blog.csdn.net/j05073094/article/details/119985348)

4. sift

   1. [OpenCV半小时掌握基本操作之SIFT算法](https://www.yingsoo.com/news/devops/45551.html)
   2. [SIFT讲解（SIFT的特征点选取以及描述是重点）](https://blog.csdn.net/qq_42604176/article/details/105640230)
   3. [图像处理理论（五）——SIFT](https://blog.csdn.net/antkillerfarm/article/details/81060269)
   4. [SIFT算法详解（这篇对算法讲解的还是相当清楚的）](http://www.360doc.com/content/21/1214/14/71430804_1008658176.shtml)
   5. [详解一种经典的图像匹配算法----SIFT](http://www.360doc.com/content/22/0702/07/71430804_1038282720.shtml)
   6. [SIFT算法步骤梳理](https://blog.csdn.net/qq_41858089/article/details/121151424)

5. sift打开的文献

   1. 基于空间金字塔和特征集成的智能机器人目标检测算法P3： HoG描述子-梯度计算

      P4：RGB-SIFT 描述子及 BOVW 特征 步骤1-4 +BOW模型

   2. 1999-基于局部尺度不变特征的目标识别

      P3：SIFT key stability（SIFT密钥稳定性）

      P4：4.Local image description

      5.Indexing and matching

   3. 2022-智能车辆检测技术综述

      p2：table 1

      ![image-20230202220159529](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220159529.png)

   4. 2021-无人地面车辆检测技术综述

      p18：table 6

      ![image-20230202220243287](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220243287.png)

   5. 2017-基于空间金字塔和特征集成的智能机器人目标检测算法【无法选择文字的版本】

      ![image-20230202220625840](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220625840.png)![image-20230202220640528](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220640528.png)

      ![image-20230202220713801](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220713801.png)

      ![image-20230202220742937](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220742937.png)

      ![image-20230202220759678](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220759678.png)

      ![image-20230202220828852](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20230202220828852.png)

6. nms-非极大值抑制算法

7. 