# 11-2-卷积的原理

## 1、卷积的定义

### 1）公式与初步形象理解

我们称$(f*g)(n)$ 为 $f,g$ 的卷积

其连续的定义为：

$(f*g)(n)=\int_{-\infty}^{+\infty}{f(\tau)g(n-\tau)d\tau}$

其离散的定义为：

$(f*g)(n)=\sum_{\tau=-\infty}^{+\infty}{f(\tau)g(n-\tau)}$

这两个式子有一个共同的特征：

![image-20221107221010002](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221107221010002.png)

这个特征的意义是什么？

我们令$x=\tau,y=n=\tau$，那么$x+y=n$ 就是下面这些直线：

![image-20221107221407190](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221107221407190.png)

如果遍历这些直线，就好比，把毛巾沿着角卷起来：（没有理解翻转之意）

![动图](https://picx1.zhimg.com/50/v2-1d0c819fc7ca6f8da25435da070a2715_720w.webp?source=1940ef5c)

![image-20221107221422459](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221107221422459.png)

此处受到 [荆哲：卷积为什么叫「卷」积？](https://zhihu.com/question/54677157/answer/141245297) 答案的启发。

但是没有解释卷积中翻转的过程，下面章节《重新形象理解卷积》中有介绍。



### 2）图像卷积计算公式

![动图](https://pic1.zhimg.com/50/v2-c658110eafe027eded16864fb6a28f46_720w.webp?source=1940ef5c)

![image-20221107221639709](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221107221639709.png)

![image-20221107221702023](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221107221702023.png)

要求 c4,5 ，一样可以套用上面的卷积公式

这样相当于实现了 g 这个矩阵在原来图像上的划动（准确来说，下面这幅图把 g 矩阵旋转了 180∘ ）：

<img src="https://picx1.zhimg.com/50/v2-15fea61b768f7561648dbea164fcb75f_720w.webp?source=1940ef5c" alt="动图" style="zoom:50%;" />

---

### 3）重新形象理解卷积

对卷积这个名词的理解：**所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。**

在连续情况下，叠加指的是对两个函数的乘积求积分，在离散情况下就是加权求和，为简单起见就统一称为叠加。

整体看来是这么个过程：

​                **翻转——>滑动——>叠加——>滑动——>叠加——>滑动——>叠加.....**

多次滑动得到的一系列叠加值，构成了[卷积函数](https://www.zhihu.com/search?q=卷积函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A637156871})。

卷积的“卷”，指的的函数的翻转，从 *g(t)* 变成 *g(-t)* 的这个过程；同时，“卷”还有滑动的意味在里面（吸取了网友[李文清](https://www.zhihu.com/people/li-wen-qing-25-49)的建议）。

卷积的“积”，指的是积分/加权求和。

对卷积的意义的理解：

1. 从“积”的过程可以看到，我们得到的叠加值，是个全局的概念。以信号分析为例，卷积的结果是不仅跟当前时刻输入信号的响应值有关，也跟过去所有时刻输入信号的响应都有关系，考虑了对过去的所有输入的效果的累积。在图像处理的中，卷积处理的结果，其实就是把每个像素周边的，甚至是整个图像的像素都考虑进来，对当前像素进行某种[加权处理](https://www.zhihu.com/search?q=加权处理&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A637156871})。所以说，“积”是全局概念，或者说是一种“混合”，把两个函数在时间或者空间上进行混合。

2. 那为什么要进行“卷”？直接相乘不好吗？我的理解，进行“卷”（翻转）的目的其实是施加一种约束，它指定了在“积”的时候以什么为参照。在信号分析的场景，它指定了在哪个特定时间点的前后进行“积”，在空间分析的场景，它指定了在哪个位置的周边进行累积处理。

   

### 4）举例

#### 例1：信号分析

如下图所示，输入信号是 f(t) ，是随时间变化的。系统响应函数是 g(t) ，图中的响应函数是随时间指数下降的，它的物理意义是说：如果在 t=0 的时刻有一个输入，那么随着时间的流逝，这个输入将不断衰减。换言之，到了 t=T时刻，原来在 t=0 时刻的输入f(0)的值将衰减为f(0)g(T)。

![image-20221110152843869](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221110152843869.png)

考虑到信号是连续输入的，也就是说，每个时刻都有新的信号进来，所以，最终输出的是所有之前输入信号的累积效果。如下图所示，在T=10时刻，输出结果跟图中带标记的区域整体有关。

其中，f(10)因为是刚输入的，所以其输出结果应该是f(10)g(0)，而时刻t=9的输入f(9)，只经过了1个时间单位的衰减，所以产生的输出应该是 f(9)g(1)，如此类推，即图中虚线所描述的关系。这些对应点相乘然后累加，就是T=10时刻的输出信号值，这个结果也是f和g两个函数在T=10时刻的[卷积值](https://www.zhihu.com/search?q=卷积值&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A637156871})。

我的理解：

- 当 t=10时，我们要计算之前输入信号的累积效果，要假设现在t=10的时刻是t’=0时刻，输入信号是f(10)，系统响应是g(0)。t=10的时刻对t=10的时刻的效应是f(10)g(0)。

- 当 t=9的时刻，对于t=10的时刻是t‘=1时刻，即t=9的时刻距离t=10只有1个间隔时间。输入信号是f(9)，系统响应是g(1)。t=9的时刻对t=10的时刻的效应是f(9)g(1)。

- 当t=8的时刻，对于t=10的时刻是t‘=2时刻，即t=8的时刻距离t=10只有2个间隔时间。输入信号是f(8)，系统响应是g(2)。t=8的时刻对t=10的时刻的效应是f(8)g(2)。
- 以此类推。
- 最终卷积效果是这些周期T=10内各个时刻对现在时刻t'=0时刻的效应之和，即累积。

![image-20221110152933573](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221110152933573.png)

显然，上面的对应关系看上去比较难看，是拧着的，所以，我们把g函数对折一下，变成了g(-t)，这样就好看一些了。看到了吗？这就是为什么卷积要“卷”，要翻转的原因，这是从它的物理意义中给出的。

![image-20221110154200041](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221110154200041.png)

上图虽然没有拧着，已经顺过来了，但看上去还有点错位，所以再进一步平移T个单位，就是下图。它就是本文开始给出的卷积定义的一种图形的表述：

![image-20221110154237480](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221110154237480.png)

所以，在以上计算T时刻的卷积时，要维持的约束就是： t+ (T-t) = T 。这种约束的意义，大家可以自己体会。

#### 例2：图像处理

还是引用知乎问题 如何通俗易懂地解释卷积？中马同学的例子。图像可以表示为矩阵形式（下图摘自马同学的文章）：

![image-20221110154422201](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221110154422201.png)

对图像的处理函数（如平滑，或者边缘提取），也可以用一个g矩阵来表示，如：
$$
g=\left[\begin{array}{ccc} b_{-1,-1}&b_{-1,0}&b_{-1,1} \\ b_{0,-1}&b_{0,0}&b_{0,1} \\ b_{1,-1}&b_{1,0}&b_{1,1} \end{array}\right]
$$
注意，我们在处理平面空间的问题，已经是二维函数了，相当于：
$$
f(x,y)=a_{x,y}
$$

$$
g(x,y)=b_{x,y}
$$

那么函数f和g的在（u，v）处的卷积 $f∗g(u,v)$ 该如何计算呢？

按卷积的定义，二维离散形式的卷积公式应该是：
$$
(f*g)(u,v)=\sum_i{\sum_j{f(i,j)g(u-i,v-j)}}=\sum_i{\sum_j{a_{i,j}b_{u-i,v-j}}}
$$
从卷积定义来看，应该是在x和y两个方向去累加（对应上面离散公式中的i和j两个下标），而且是无界的，从负无穷到正无穷。可是，真实世界都是有界的。例如，上面列举的图像处理函数g实际上是个3x3的矩阵，意味着，在除了原点附近以外，其它所有点的取值都为0。考虑到这个因素，上面的公式其实退化了，它只把坐标（u,v）附近的点选择出来做计算了。所以，真正的计算如下所示：

![image-20221110155840690](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221110155840690.png)

首先我们在原始图像矩阵中取出（u,v）处的矩阵：
$$
f=\left[\begin{array}{ccc} a_{u-1,v-1}&a_{u-1,v}&a_{u-1,v+1} \\ a_{u,v-1}&a_{u,v}&a_{u,v+1} \\ a_{u+1,v-1}&a_{u+1,v}&a_{u+1,v+1} \end{array}\right]
$$


然后将图像处理矩阵翻转（这个翻转有点意思，可以有几种不同的理解，其效果是等效的：（1）先沿x轴翻转，再沿y轴翻转；（2）先沿x轴翻转，再沿y轴翻转；），如下：

原始矩阵：

![image-20221110160221614](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221110160221614.png)

翻转后的矩阵：
$$
g=\left[\begin{array}{ccc} b_{1,1}&b_{1,0}&b_{1,-1} \\ b_{0,1}&b_{0,0}&b_{0,-1} \\ b_{-1,1}&b_{-1,0}&b_{-1,-1} \end{array}\right]
$$


1）先沿x轴翻转，再沿y轴翻转
$$
g=\left[\begin{array}{ccc} b_{-1,-1}&b_{-1,0}&b_{-1,1} \\ b_{0,-1}&b_{0,0}&b_{0,1} \\ b_{1,-1}&b_{1,0}&b_{1,1} \end{array}\right] =>\left[\begin{array}{ccc} b_{1,-1}&b_{1,0}&b_{1,1} \\ b_{0,-1}&b_{0,0}&b_{0,1} \\ b_{-1,-1}&b_{-1,0}&b_{-1,1} \end{array}\right] =>\left[\begin{array}{ccc} b_{1,1}&b_{1,0}&b_{1,-1} \\ b_{0,1}&b_{0,0}&b_{0,-1} \\ b_{-1,1}&b_{-1,0}&b_{-1,-1} \end{array}\right] =g'
$$
2）先沿y轴翻转，再沿x轴翻转
$$
g=\left[\begin{array}{ccc} b_{-1,-1}&b_{-1,0}&b_{-1,1} \\ b_{0,-1}&b_{0,0}&b_{0,1} \\ b_{1,-1}&b_{1,0}&b_{1,1} \end{array}\right] =>\left[\begin{array}{ccc} b_{-1,1}&b_{-1,0}&b_{-1,-1} \\ b_{0,1}&b_{0,0}&b_{0,-1} \\ b_{1,1}&b_{1,0}&b_{1,-1} \end{array}\right] =>\left[\begin{array}{ccc} b_{1,1}&b_{1,0}&b_{1,-1} \\ b_{0,1}&b_{0,0}&b_{0,-1} \\ b_{-1,1}&b_{-1,0}&b_{-1,-1} \end{array}\right] =g'
$$
计算卷积时，就可以用 f 和 g′ 的内积：
$$
f*g(u,v)=
a_{u-1, v-1}\times b_{1,1}+a_{u-1, v}\times b_{1,0}+a_{u-1, v+1}\times b_{1,-1}+
a_{u, v-1}\times b_{0,1}+a_{u, v}\times b_{0,0}+a_{u, v+1}\times b_{0,-1}+
a_{u+1, v-1}\times b_{-1,1}+a_{u+1, v}\times b_{-1,0}+a_{u+1, v+1}\times b_{-1,-1}
$$
请注意，以上公式有一个特点，做乘法的两个对应变量a,b的下标之和都是（u,v），其目的是对这种加权求和进行一种约束。这也是为什么要将矩阵g进行翻转的原因。以上矩阵下标之所以那么写，并且进行了翻转，是为了让大家更清楚地看到跟卷积的关系。这样做的好处是便于推广，也便于理解其物理意义。实际在计算的时候，都是用翻转以后的矩阵，直接求矩阵内积就可以了。

以上计算的是（u,v）处的卷积，延x轴或者y轴滑动，就可以求出图像中各个位置的卷积，其输出结果是处理以后的图像（即经过平滑、边缘提取等各种处理的图像）。

再深入思考一下，在算图像卷积的时候，我们是直接在原始图像矩阵中取了（u,v）处的矩阵，为什么要取这个位置的矩阵，本质上其实是为了满足以上的约束。因为我们要算（u，v）处的卷积，而g矩阵是3x3的矩阵，要满足下标跟这个3x3矩阵的和是（u,v），只能是取原始图像中以（u，v）为中心的这个3x3矩阵，即图中的阴影区域的矩阵。

推而广之，如果如果g矩阵不是3x3，而是7x7，那我们就要在原始图像中取以（u，v）为中心的7x7矩阵进行计算。由此可见，这种卷积就是**把原始图像中的相邻像素都考虑进来**，进行混合。相邻的区域范围取决于g矩阵的维度，维度越大，涉及的周边像素越多。而矩阵的设计，则决定了这种混合输出的图像跟原始图像比，究竟是模糊了，还是更锐利了。

比如说，如下图像处理矩阵将使得图像变得更为平滑，显得更模糊，因为它联合周边像素进行了平均处理：
$$
g=\left[\begin{array}{ccc} \frac{1}{9}&\frac{1}{9}&\frac{1}{9} \\ \frac{1}{9}&\frac{1}{9}&\frac{1}{9} \\ \frac{1}{9}&\frac{1}{9}&\frac{1}{9} \end{array}\right]
$$


而如下图像处理矩阵将使得像素值变化明显的地方更为明显，强化边缘，而变化平缓的地方没有影响，达到提取边缘的目的：
$$
g=\left[\begin{array}{ccc} -1&-1&-1 \\ -1&9&-1\\ -1&-1&-1 \end{array}\right]
$$


----------------------------------------------------------------------------------------------------------------

作者：palet
链接：https://www.zhihu.com/question/22298352/answer/637156871
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

---

### 5）函数使用

#### a)`tf.nn.conv2d`

```python
def conv2d(
	input, #张量输入
	filter,#卷积核参数
    strides,#步长参数
    padding,#卷积方式
    use_cudnn_on_gpu=None,#是否是gpu加速
    data_format=None,#数据格式，与步长参数配合，决定移动方式
    name=None #名字，用于tensorboard图形显示时使用
)
```

1. input: 指需要做卷积的输入图像，它要求是一个Tensor, 具有 **[batch, in_height,in_width, in_channels]** 这样的形状，具体含义是“**训练时一个batch的图片数量， 图片高度，图片宽度，图像通道数**”，注意这是一个四维的Tensor,要求类型为float32和float64其中之一。

2. filter: 相当于<font bg-colorCNN中的卷积核，它要求是一个Tensor, 具有**[filter_height, filter_width,in_channels, out_channels]**这样的形状,具体含义是“**卷积核的高度，卷积核的宽度，图像通道数，卷积核个数**”，要求类型与参数input相同。有一个地方需要注意，第三维in_channels,就是参数input的第四维。

3. strides:卷积时在图像每一维的步长， 这是一个一维的向量，长度为4（分别是[batch方向,height方向,width方向,channels方向）很多同学只认为第一维和最后一维默认必须置1，其实strides参数确定了滑动窗口在各个维度上移动的步数。

   当输入的默认格式为：“NHWC”，则 strides = [batch , in_height , in_width, in_channels]。其中 batch 和 in_channels 要求一定为1，即只能在一个样本的一个通道上的特征图上进行移动，in_height , in_width表示卷积核在特征图的高度和宽度上移动的步长。

4. padding: 定义元素边框与元素内容之间的空间。string类型的量，只能是SAME和VALID其中之一，这个值决定了不同的卷积方式，padding 的值为VALID时，表示边缘不填充，当其为’SAME时，表示用0去填充周围，到滤波器可以到达图像边界。

5. use_ cudnn on_ gpu: bool类型，是否使用cudnn加速，默认为true.

6. data_format就是步长移动方式，参数的取值有两种，NCHW ,NHWC,默认是NHWC。（N是n，是数量，C是channel， H是height， W是weight）

   - NCHW——先w后h后c后n
   - NHWC——先c后w后h后n

   图片（张量以下我们以图片，代替，方便理解，即二维数据）数据有通道数，有长宽，卷积核是先按宽度方向按指定步长移动，还是按高度方向？，还是按通道方向？这个得有个说法。我们帖一个数据结构图

   ![image-20221111123705278](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221111123705278.png)

   移动方式一： 第一个元素是000，第二个元素是沿着w方向的，即001，这样下去002 003，再接着呢就是沿着H方向，即004 005 006 007…这样到09后，沿C方向，轮到了020，之后021 022 …一直到319，然后再沿N方向。————这种方式叫：NCHW (注意顺序，N是n，是数量，C是channel， H是height， W是weight)
   移动方式二：第一个元素是000，第二个沿C方向，即020，040, 060…一直到300，之后沿W方向，001 021 041 061…301…到了303后，沿H方向，即004 024 .。。304.。最后到了319，变成N方向，320,340…————这种方式叫：NHWC

7. 返回值: tf.nn.conv2d函数结果返回一个Tensor, 这个输出就是常说的feature map.

8. 注意: 在卷积函数中, padding参数是最容易引起歧义的，该参数仅仅决定是否要补0,因此一定要清楚padding设为SAME的真正含义。在设为SAME的情况下，只有在步长为1时生成的feature map才会与输入值相等。

#### b)`tf.keras.layers.conv2d`

- ##### 代码

  ```python
  from keras.layers import Conv2d,MaxPooling2D，Flatten,Dropout
  # 输入是 28x28 RGB 图像，带有 `channels_last` 和批处理大小为 4。
  input_shape = (4, 28, 28, 3)
  model=Sequential([
      Conv2D(64,(3,3),activation='relu',input_shape=input_shape)
  ])
  ```

- ##### 参数解释

  - ```python
    tf.keras.layers.Conv2D(
        filters,# 整数，卷积过滤器的数量，对应输出的维数
        kernel_size,# 整数，过滤器的大小，如果为一个整数，则宽和高相同
        strides=(1, 1), # 横向和纵向的步长，如果为一个整数则横向和纵向相同
        padding='valid', # 是否获取边界信息，‘vaild’是无填充，‘same’是周边用0填充
        data_format=None, # 输入中维度的排序,`channels_last`（默认）或 `channels_first` 之一。
        dilation_rate=(1, 1), # 整数，指定用于扩张卷积的扩张率。
        groups=1, # 正整数，指定输入沿通道轴拆分的组数。
        activation=None, # 激活函数，卷积用relu
        use_bias=True, #布尔值，层是否使用偏置向量。
        kernel_initializer='glorot_uniform', # 初始化内核权重矩阵。默认为“glorot_uniform”。
        bias_initializer='zeros',# 初始化偏置向量
        kernel_regularizer=None, # 正则化内核权重矩阵
        bias_regularizer=None, # 正则化偏置向量
        activity_regularizer=None, # 正则化层输出
        kernel_constraint=None, # 内核权重矩阵的约束函数
        bias_constraint=None, # 偏置向量的约束函数
        **kwargs
    )
    ```

    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D#args

  - | tf.keras.layers.Conv2D参数 | 解释                                                         |
    | -------------------------- | ------------------------------------------------------------ |
    | filters                    | 整数，输出空间的维度（即卷积中输出滤波器的数量）。           |
    | kernel_size                | 一个整数或 2 个整数的元组/列表，指定 2D 卷积窗口的高度和宽度。可以是单个整数，为所有空间维度指定相同的值。 |
    | strides                    | 一个整数或 2 个整数的元组/列表，指定卷积沿高度和宽度的步幅。可以是单个整数，为所有空间维度指定相同的值。指定任何 stride value != 1 与指定任何 dilation_rate value != 1 不兼容。 |
    | padding                    | “vaild”或“same”之一（不区分大小写）。 “vaild”意味着没有填充。 “same”导致在输入的左/右或上/下均匀填充零。当 padding="same" 和 strides=1 时，输出的大小与输入的大小相同。 |
    | data_format                | 一个字符串，`channels_last`（默认）或 `channels_first` 之一。输入中维度的排序。 channels_last 对应于具有形状 (batch_size, height,width, channels) 的输入，而 channels_first 对应于具有形状 (batch_size, channels, height, width) 的输入。它默认为在 `~/.keras/keras.json` 的 `Keras` 配置文件中找到的 image_data_format 值。如果您从未设置它，那么它将是 channels_last。 |
    | dilation_rate              | 一个整数或 2 个整数的元组/列表，指定用于扩张卷积的扩张率。可以是单个整数，为所有空间维度指定相同的值。目前，指定任何 dilation_rate value != 1 与指定任何 stride value != 1 是不兼容的。 |
    | group                      | 一个正整数，指定输入沿通道轴拆分的组数。每个组分别与过滤器/组过滤器进行卷积。输出是沿通道轴的所有组结果的串联。输入通道和过滤器都必须可以按组整除。 |
    | use_bias                   | 布尔值，层是否使用偏置向量。                                 |
    | kernel_initializer         | 内核权重矩阵的初始化程序（请参阅 [`keras.initializers`](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)）。默认为“glorot_uniform”。 |
    | bias_initializer           | 偏置向量的初始化程序（请参阅 [`keras.initializers`](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)）。默认为“零”。 |
    | kernel_regularizer         | 应用于内核权重矩阵的正则化函数 (请参阅 [`keras.regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)). |
    | bias_regularizer           | 应用于偏置向量的正则化函数 (请参阅 [`keras.regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)). |
    | activity_regularizer       | 应用于层输出的正则化函数（其“激活”）  (请参阅 [`keras.regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)). |
    | kernel_constraint          | 应用于核矩阵的常值（约束）函数 (请参阅 [`keras.constraints`](https://www.tensorflow.org/api_docs/python/tf/keras/constraints)). |
    | bias_constraint            | c应用于偏置向量的常值（约束）函数 (请参阅 [`keras.constraints`](https://www.tensorflow.org/api_docs/python/tf/keras/constraints)). |

  - 参数解释

    - filter运行

      例如输入 $224\times 224\times 3$（rgb三通道），要得到输出是32位深度，选取卷积核尺寸为 $5\times 5$。那么我们需要32个卷积核**（不同的卷积核的矩阵参数不同）**，每一个的尺寸为 $5\times 5 \times 3$（最后的3就是原图的rgb位深3），每一个卷积核的每一层是 $5\times 5$（共3层）分别与原图的每层 $224\times 224$ 卷积，然后将得到的三张新图叠加（算术求和），变成一张新的feature map。 每一个卷积核都这样操作，就可以得到32张新的feature map了。 也就是说：不管输入图像的深度为多少，经过一个卷积核（filter），最后都通过下面的公式变成一个深度为1的特征图。不同的filter可以卷积得到不同的特征，也就是得到不同的feature map。。。

      ![在这里插入图片描述](https://img-blog.csdnimg.cn/cbbe159f29c3498aab8de7fd7033ee0c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAeGlhaW1pbmcw,size_20,color_FFFFFF,t_70,g_se,x_16)

    - 输入输出矩阵格式：

      - 输入矩阵格式：四维，依次为：样本数、图像高度、图像宽度、图像通道数
      - 输出矩阵格式：与输入矩阵维度顺序和含义相同，但是后三个维度（图像高度、图像宽度、图像通道数）的尺寸发生变化
      - 权重矩阵（卷积核）格式：四维，但维度的含义和上面两者不同，是：卷积核高度、卷积核宽度、输入通道数、输出通道数（卷积核个数）
      - 输入矩阵、权重矩阵、输出矩阵这三者之间的相互决定关系。
        - 卷积核的输入通道数（in depth）由输入矩阵的通道数所决定。
        - 输出矩阵的通道数（out depth）由卷积核的输出通道数所决定
        - 输出矩阵的高度和宽度（height， width）这两个维度的尺寸由输入矩阵、卷积核、扫描方式所共同决定。

    - 参考：[tf.keras.layers.Conv2D，tf.keras.layers.SimpleRNN()主要参数讲解](https://blog.csdn.net/xiaiming0/article/details/124257782)

  - 训练中的参数

    - batch-size中文翻译是批量大小，所谓的批量是指学习样本的数量，因为在训练模型时需要将祥本图像全部读入到内存中，这么做的原因是提升收敛速度。 
    - epochs：迭代的次数。

#### c) 函数的比较

一般推荐使用tf.layers.下面的函数，用起来方便。但是在tf2.0里，tf.layers.下面的API也都被遗弃了，tf2.0推荐使用keras.layers下面的API。

建议：keras.layers下面的函数封装比tf.nn.更完备，没有特别要求一般推荐使用keras.layers.，tf.nn.较为基础，若需自己定义内部，可用tf.nn.。

#### d）filter计算原理

- ##### 输入层：

  就是输入的图像，有可能是三通道的有可能是单通道的。   比如28*28*1或者28*28*3 分别代表的是大小为28*28，通道数分别为单通道和三通道的图片。

- ##### 输出层：

  输出层的就是filter遍历输入层后的的计算结果。输出层的深度等于filter的out_channels个数或者输出神经元的个数！每一个filter遍历输入层会产生一个深度的输出层，那么n个输出层就会产生n个深度的输出层（也就是输出层的深度啦）。

![image-20221111125653287](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221111125653287.png)

- ##### 计算过程

  输出层每一个深度的计算过程。比如，输入层是28\*28\*3的图像，filter为3\*3\*3的滤波器。那么3个通道的filter会相应的每一个通道相互独立计算，然后再将3个通道的值相加在一起。 这就是每一个filter的计提的计算过程。现在就是CNN卷积的处理过程。

  ![image-20221111130304878](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221111130304878.png)

  每个通道都有一个卷积核，结果是所有通道卷积结果的和。

  ![image-20221111131626565](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221111131626565.png)

- **在tensorflow中的conv2d中处理卷积的通道数问题：**  

  - input的四个维度是[batch, in_height, in_width, in_channels]， 
  - filter的四个维度是[filter_height, filter_width, in_channels, out_channels]。 
  - filter的通道数(深度)与输入层的通道数(深度)是一致的，就是in_channels。
  - filter中out_channels数值 (输出通道数)= filter的数量/深度 = featuremap的数量。
  - 就是 out_channels，有多少输出通道，就有多少个filter。

- 参考资料

  - [卷积神经网络中的filter是怎么工作的](https://blog.csdn.net/qq_21033779/article/details/78211091)
  - [tf.nn.conv2d的实验例子](https://blog.csdn.net/xinyuski/article/details/85072630)
  - [【TensorFlow】理解tf.nn.conv2d方法 ( 附代码详解注释 )_padding](https://blog.csdn.net/kakiebu/article/details/122873281)
  - [tf.nn.conv2d方法_padding](https://cloud.tencent.com/developer/article/1501980)

#### e）计算复杂度

- ##### 输出规模大小：

  - 总矩阵：$Y=X \cdot W+B$
  - 输入$X：c_i\times n_h \times n_w$
  - 核$W：c_o\times c_i \times k_h \times k_w$
  - 偏差$B：c_o\times c_i$
  - 输出$Y：c_o\times m_h \times m_w$
  - 解释：
    - o是输出（output），i是输入（input）
    - h是高度（height），w是宽度（width）
    - n是输入batch的单个图像，c是通道，k是卷积核，m是输出的单个图像

- ##### 计算复杂度（浮点计算数FLOP $O(c_{o}c_{i}k_{h}k_{w}m_{h}m_{w})$：

  - $c_{o}=c_{i}=100$ 
  - $k_{h}=k_{w}=5$
  - $m_{h}=m_{w}=64$
  - $c_{o}c_{i}k_{h}k_{w}m_{h}m_{w}=1,024,000,000$ 次浮点运算
  - $O(c_{o}c_{i}k_{h}k_{w}m_{h}m_{w})=1GFLOP$

![image-20221111132231045](C:\Users\Myste\AppData\Roaming\Typora\typora-user-images\image-20221111132231045.png)

- ##### 每秒浮点运算次数FLOPS

  - 出自 https://blog.csdn.net/ayiya_Oese/article/details/114696589

  - FLOPS：每秒浮点运算次数，亦称每秒峰值速度，（英语：Floating-point operations per second；缩写：FLOPS，flops 或者 flop/s），即每秒所运行的浮点运算次数。浮点（floating-point）指的是带有小数的数值，浮点运算即是小数的四则运算，常用来测量电脑运算速度或被用来估算电脑性能，尤其是在使用到大量浮点运算的科学计算领域中。因为FLOPS后缀的那个S代表秒，而不是复数，所以不能够省略。在多数情况下，测算FLOPS比测算每秒指令数（IPS）要准确。

  - 换算
    一个KFLOPS（kiloFLOPS）等于每秒一千（$10^3$）次的浮点运算；
    一个MFLOPS（megaFLOPS）等于每秒一百万（$10^6$）次的浮点运算；
    一个GFLOPS（gigaFLOPS）等于每秒十亿（$10^9$）次的浮点运算；
    一个TFLOPS（teraFLOPS）等于每秒一万亿（$10^{12}$）次的浮点运算；
    一个PFLOPS（petaFLOPS）等于每秒一千万亿（$10^{15}$）次的浮点运算；
    一个EFLOPS（exaFLOPS）等于每秒一百亿亿（$10^{18}$）次的浮点运算。

  - FLOPS在高性能计算机集群（超算）上可以用这个公式计算：
    $$
    FLOPS=racks \times \frac{nodes}{rack} \times \frac{sockets}{node} \times \frac{cores}{sockets} \times \frac{cycles}{second} \times \frac{FLOPS}{cycle}
    $$

  - 简化到计算机只拥有一块CPU的情况时，可以使用以下公式：
    $$
    FLOPS=cores \times \frac{cycles}{second} \times \frac{FLOPS}{cycle}
    $$

  - 更多细节看这篇文章：[NODES, SOCKETS, CORES AND FLOPS, OH, MY](http://techcenter.wikifoundry.com/page/Nodes%2C+Sockets%2C+Cores+and+FLOPS%2C+Oh%2C+My)
    或者[这里](https://mdotfernandez.wordpress.com/2014/02/03/phi-nodes-sockets-cores-and-flops-oh-my/)



